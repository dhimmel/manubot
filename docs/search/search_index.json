{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python utilities for Manubot: Manuscripts, open and automated Manubot is a workflow and set of tools for the next generation of scholarly publishing. This repository contains a Python package with several Manubot-related utilities, as described in the usage section below. The manubot cite command-line interface retrieves and formats bibliographic metadata for user-supplied persistent identifiers like DOIs or PubMed IDs. The manubot process command-line interface prepares scholarly manuscripts for Pandoc consumption. The manubot process command is used by Manubot manuscripts, which are based off the Rootstock template , to automate several aspects of manuscript generation. See Rootstock's manuscript usage guide for more information. Note: If you want to experience Manubot by editing an existing manuscript, see https://github.com/manubot/try-manubot . If you want to create a new manuscript, see https://github.com/manubot/rootstock . To cite the Manubot project or for more information on its design and history, see: Open collaborative writing with Manubot Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter PLOS Computational Biology (2019-06-24) https://doi.org/c7np DOI: 10.1371/journal.pcbi.1007128 \u00b7 PMID: 31233491 \u00b7 PMCID: PMC6611653 The Manubot version of this manuscript is available at https://greenelab.github.io/meta-review/ . Installation If you are using the manubot Python package as part of a manuscript repository, installation of this package is handled though the Rootstock's environment specification . For other use cases, this package can be installed via pip . Install the latest release version from PyPI : pip install --upgrade manubot Or install from the source code on GitHub , using the version specified by a commit hash: COMMIT = d2160151e52750895571079a6e257beb6e0b1278 pip install --upgrade git+https://github.com/manubot/manubot@ $COMMIT The --upgrade argument ensures pip updates an existing manubot installation if present. Usage Installing the python package creates the manubot command line program. Here is the usage information as per manubot --help : usage : manubot [ - h ] [ -- version ] { process , cite , webpage } ... Manubot : the manuscript bot for scholarly writing optional arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit subcommands : All operations are done through subcommands : { process , cite , webpage } process process manuscript content cite citation to CSL command line utility webpage deploy Manubot outputs to a webpage directory tree Note that all operations are done through the following sub-commands. Process The manubot process program is the primary interface to using Manubot. There are two required arguments: --content-directory and --output-directory , which specify the respective paths to the content and output directories. The content directory stores the manuscript source files. Files generated by Manubot are saved to the output directory. One common setup is to create a directory for a manuscript that contains both the content and output directory. Under this setup, you can run the Manubot using: manubot process \\ --content-directory = content \\ --output-directory = output See manubot process --help for documentation of all command line arguments: usage : manubot process [ - h ] -- content - directory CONTENT_DIRECTORY -- output - directory OUTPUT_DIRECTORY [ -- template - variables - path TEMPLATE_VARIABLES_PATH ] [ -- cache - directory CACHE_DIRECTORY ] [ -- clear - requests - cache ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Process manuscript content to create outputs for Pandoc consumption . Performs bibliographic processing and templating . optional arguments : - h , -- help show this help message and exit -- content - directory CONTENT_DIRECTORY Directory where manuscript content files are located . -- output - directory OUTPUT_DIRECTORY Directory to output files generated by this script . -- template - variables - path TEMPLATE_VARIABLES_PATH Path or URL of a JSON file containing template variables for jinja2 . Specify this argument multiple times to read multiple files . Variables can be applied to a namespace ( i . e . stored under a dictionary key ) like ` -- template - variables - path = namespace = path_or_url `. Namespaces must match the regex `[ a - zA - Z_ ][ a - zA - Z0 - 9 _ ] * `. -- cache - directory CACHE_DIRECTORY Custom cache directory . If not specified , caches to output - directory . -- clear - requests - cache -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Manual references Manubot has the ability to rely on user-provided reference metadata rather than generating it. manubot process searches the content directory for files containing manually-provided reference metadata that match the glob manual-references*.* . If a manual reference filename ends with .json or .yaml , it's assumed to contain CSL Data (i.e. Citation Style Language JSON). Otherwise, the format is inferred from the extension and converted to CSL JSON using the pandoc-citeproc --bib2json utility . The standard citation key for manual references is inferred from the CSL JSON id or note field. When no prefix is provided, such as doi: , url: , or raw: , a raw: prefix is automatically added. If multiple manual reference files load metadata for the same standard citation id , precedence is assigned according to descending filename order. Cite manubot cite is a command line utility to create CSL JSON items for one or more citation keys. Citation keys should be in the format source:identifier . For example, the following example generates CSL JSON for four references: manubot cite doi:10.1098/rsif.2017.0387 pmid:29424689 pmcid:PMC5640425 arxiv:1806.05726 The following terminal recording demonstrates the main features of manubot cite : Additional usage information is available from manubot cite --help : usage : manubot cite [ - h ] [ -- render ] [ -- csl CSL ] [ -- format { plain , markdown , docx , html , jats }] [ -- output OUTPUT ] [ -- allow - invalid - csl - data ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] citekeys [ citekeys ...] Retrieve bibliographic metadata for one or more citation keys . positional arguments : citekeys One or more ( space separated ) citation keys to produce CSL for . optional arguments : - h , -- help show this help message and exit -- render Whether to render CSL Data into a formatted reference list using Pandoc . Pandoc version 2 . 0 or higher is required for complete support of available output formats . -- csl CSL When -- render , specify an XML CSL definition to style references ( i . e . Pandoc ' s --csl option). Defaults to Manubot ' s style. -- format { plain , markdown , docx , html , jats } When -- render , format to use for output file . If not specified , attempt to infer this from filename extension . Otherwise , default to plain . -- output OUTPUT Specify a file to write output , otherwise default to stdout . -- allow - invalid - csl - data Allow CSL Items that do not conform to the JSON Schema . Skips CSL pruning . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Webpage The manubot webpage command populates a webpage directory with Manubot output files. usage : manubot webpage [ - h ] [ -- checkout [ CHECKOUT ]] [ -- version VERSION ] [ -- timestamp ] [ -- no - ots - cache | -- ots - cache OTS_CACHE ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Update the webpage directory tree with Manubot output files . This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout , containing ` output ` and ` webpage ` directories . HTML and PDF outputs are copied to the webpage directory , which is structured as static source files for website hosting . optional arguments : - h , -- help show this help message and exit -- checkout [ CHECKOUT ] branch to checkout / v directory contents from . For example , -- checkout = upstream / gh - pages . -- checkout is equivalent to -- checkout = gh - pages . If -- checkout is ommitted , no checkout is performed . -- version VERSION Used to create webpage / v / { version } directory . Generally a commit hash , tag , or ' local ' . When omitted , version defaults to the commit hash on CI builds and ' local ' elsewhere . -- timestamp timestamp versioned manuscripts in webpage / v using OpenTimestamps . Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions . -- no - ots - cache disable the timestamp cache . -- ots - cache OTS_CACHE location for the timestamp cache ( default : ci / cache / ots ) . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging Development Create a development environment using: conda create --name manubot-dev --channel conda-forge \\ python = 3 .6 jinja2 pandas pytest pandoc conda activate manubot-dev # assumes conda >= 4.4 pip install --editable \".[all]\" Inside this environment, use pytest to run the test suite. You can also use the manubot CLI to build manuscripts. For example: manubot process \\ --content-directory = tests/manuscripts/example/content \\ --output-directory = tests/manuscripts/example/output \\ --log-level = DEBUG To automatically regenerate the README --help messages codeblocks, run: python manubot/tests/test_readme.py Release instructions This section is only relevant for project maintainers. Travis CI deployments are used to upload releases to PyPI . To create a new release, bump the __version__ in manubot/__init__.py . Then run the following commands: TAG = v ` python setup.py --version ` # Commit updated __version__ info git add manubot/__init__.py release-notes/ $TAG .md git commit --message = \"Prepare $TAG release\" git push # Create & push tag (assuming upstream is the manubot organization remote) git tag --annotate $TAG --file = release-notes/ $TAG .md git push upstream $TAG Goals & Acknowledgments Our goal is to create scholarly infrastructure that encourages open science and assists reproducibility. Accordingly, we hope for the Manubot software and philosophy to be adopted widely, by both academic and commercial entities. As such, Manubot is free/libre and open source software (see LICENSE.md ). We would like to thank the contributors and funders whose support makes this project possible. Specifically, Manubot development has been financially supported by: the Alfred P. Sloan Foundation in Grant G-2018-11163 to @dhimmel . the Gordon & Betty Moore Foundation ( @DDD-Moore ) in Grant GBMF4552 to @cgreene .","title":"Home"},{"location":"#python-utilities-for-manubot-manuscripts-open-and-automated","text":"Manubot is a workflow and set of tools for the next generation of scholarly publishing. This repository contains a Python package with several Manubot-related utilities, as described in the usage section below. The manubot cite command-line interface retrieves and formats bibliographic metadata for user-supplied persistent identifiers like DOIs or PubMed IDs. The manubot process command-line interface prepares scholarly manuscripts for Pandoc consumption. The manubot process command is used by Manubot manuscripts, which are based off the Rootstock template , to automate several aspects of manuscript generation. See Rootstock's manuscript usage guide for more information. Note: If you want to experience Manubot by editing an existing manuscript, see https://github.com/manubot/try-manubot . If you want to create a new manuscript, see https://github.com/manubot/rootstock . To cite the Manubot project or for more information on its design and history, see: Open collaborative writing with Manubot Daniel S. Himmelstein, Vincent Rubinetti, David R. Slochower, Dongbo Hu, Venkat S. Malladi, Casey S. Greene, Anthony Gitter PLOS Computational Biology (2019-06-24) https://doi.org/c7np DOI: 10.1371/journal.pcbi.1007128 \u00b7 PMID: 31233491 \u00b7 PMCID: PMC6611653 The Manubot version of this manuscript is available at https://greenelab.github.io/meta-review/ .","title":"Python utilities for Manubot: Manuscripts, open and automated"},{"location":"#installation","text":"If you are using the manubot Python package as part of a manuscript repository, installation of this package is handled though the Rootstock's environment specification . For other use cases, this package can be installed via pip . Install the latest release version from PyPI : pip install --upgrade manubot Or install from the source code on GitHub , using the version specified by a commit hash: COMMIT = d2160151e52750895571079a6e257beb6e0b1278 pip install --upgrade git+https://github.com/manubot/manubot@ $COMMIT The --upgrade argument ensures pip updates an existing manubot installation if present.","title":"Installation"},{"location":"#usage","text":"Installing the python package creates the manubot command line program. Here is the usage information as per manubot --help : usage : manubot [ - h ] [ -- version ] { process , cite , webpage } ... Manubot : the manuscript bot for scholarly writing optional arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit subcommands : All operations are done through subcommands : { process , cite , webpage } process process manuscript content cite citation to CSL command line utility webpage deploy Manubot outputs to a webpage directory tree Note that all operations are done through the following sub-commands.","title":"Usage"},{"location":"#process","text":"The manubot process program is the primary interface to using Manubot. There are two required arguments: --content-directory and --output-directory , which specify the respective paths to the content and output directories. The content directory stores the manuscript source files. Files generated by Manubot are saved to the output directory. One common setup is to create a directory for a manuscript that contains both the content and output directory. Under this setup, you can run the Manubot using: manubot process \\ --content-directory = content \\ --output-directory = output See manubot process --help for documentation of all command line arguments: usage : manubot process [ - h ] -- content - directory CONTENT_DIRECTORY -- output - directory OUTPUT_DIRECTORY [ -- template - variables - path TEMPLATE_VARIABLES_PATH ] [ -- cache - directory CACHE_DIRECTORY ] [ -- clear - requests - cache ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Process manuscript content to create outputs for Pandoc consumption . Performs bibliographic processing and templating . optional arguments : - h , -- help show this help message and exit -- content - directory CONTENT_DIRECTORY Directory where manuscript content files are located . -- output - directory OUTPUT_DIRECTORY Directory to output files generated by this script . -- template - variables - path TEMPLATE_VARIABLES_PATH Path or URL of a JSON file containing template variables for jinja2 . Specify this argument multiple times to read multiple files . Variables can be applied to a namespace ( i . e . stored under a dictionary key ) like ` -- template - variables - path = namespace = path_or_url `. Namespaces must match the regex `[ a - zA - Z_ ][ a - zA - Z0 - 9 _ ] * `. -- cache - directory CACHE_DIRECTORY Custom cache directory . If not specified , caches to output - directory . -- clear - requests - cache -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Process"},{"location":"#manual-references","text":"Manubot has the ability to rely on user-provided reference metadata rather than generating it. manubot process searches the content directory for files containing manually-provided reference metadata that match the glob manual-references*.* . If a manual reference filename ends with .json or .yaml , it's assumed to contain CSL Data (i.e. Citation Style Language JSON). Otherwise, the format is inferred from the extension and converted to CSL JSON using the pandoc-citeproc --bib2json utility . The standard citation key for manual references is inferred from the CSL JSON id or note field. When no prefix is provided, such as doi: , url: , or raw: , a raw: prefix is automatically added. If multiple manual reference files load metadata for the same standard citation id , precedence is assigned according to descending filename order.","title":"Manual references"},{"location":"#cite","text":"manubot cite is a command line utility to create CSL JSON items for one or more citation keys. Citation keys should be in the format source:identifier . For example, the following example generates CSL JSON for four references: manubot cite doi:10.1098/rsif.2017.0387 pmid:29424689 pmcid:PMC5640425 arxiv:1806.05726 The following terminal recording demonstrates the main features of manubot cite : Additional usage information is available from manubot cite --help : usage : manubot cite [ - h ] [ -- render ] [ -- csl CSL ] [ -- format { plain , markdown , docx , html , jats }] [ -- output OUTPUT ] [ -- allow - invalid - csl - data ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] citekeys [ citekeys ...] Retrieve bibliographic metadata for one or more citation keys . positional arguments : citekeys One or more ( space separated ) citation keys to produce CSL for . optional arguments : - h , -- help show this help message and exit -- render Whether to render CSL Data into a formatted reference list using Pandoc . Pandoc version 2 . 0 or higher is required for complete support of available output formats . -- csl CSL When -- render , specify an XML CSL definition to style references ( i . e . Pandoc ' s --csl option). Defaults to Manubot ' s style. -- format { plain , markdown , docx , html , jats } When -- render , format to use for output file . If not specified , attempt to infer this from filename extension . Otherwise , default to plain . -- output OUTPUT Specify a file to write output , otherwise default to stdout . -- allow - invalid - csl - data Allow CSL Items that do not conform to the JSON Schema . Skips CSL pruning . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Cite"},{"location":"#webpage","text":"The manubot webpage command populates a webpage directory with Manubot output files. usage : manubot webpage [ - h ] [ -- checkout [ CHECKOUT ]] [ -- version VERSION ] [ -- timestamp ] [ -- no - ots - cache | -- ots - cache OTS_CACHE ] [ -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL }] Update the webpage directory tree with Manubot output files . This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout , containing ` output ` and ` webpage ` directories . HTML and PDF outputs are copied to the webpage directory , which is structured as static source files for website hosting . optional arguments : - h , -- help show this help message and exit -- checkout [ CHECKOUT ] branch to checkout / v directory contents from . For example , -- checkout = upstream / gh - pages . -- checkout is equivalent to -- checkout = gh - pages . If -- checkout is ommitted , no checkout is performed . -- version VERSION Used to create webpage / v / { version } directory . Generally a commit hash , tag , or ' local ' . When omitted , version defaults to the commit hash on CI builds and ' local ' elsewhere . -- timestamp timestamp versioned manuscripts in webpage / v using OpenTimestamps . Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions . -- no - ots - cache disable the timestamp cache . -- ots - cache OTS_CACHE location for the timestamp cache ( default : ci / cache / ots ) . -- log - level { DEBUG , INFO , WARNING , ERROR , CRITICAL } Set the logging level for stderr logging","title":"Webpage"},{"location":"#development","text":"Create a development environment using: conda create --name manubot-dev --channel conda-forge \\ python = 3 .6 jinja2 pandas pytest pandoc conda activate manubot-dev # assumes conda >= 4.4 pip install --editable \".[all]\" Inside this environment, use pytest to run the test suite. You can also use the manubot CLI to build manuscripts. For example: manubot process \\ --content-directory = tests/manuscripts/example/content \\ --output-directory = tests/manuscripts/example/output \\ --log-level = DEBUG To automatically regenerate the README --help messages codeblocks, run: python manubot/tests/test_readme.py","title":"Development"},{"location":"#release-instructions","text":"This section is only relevant for project maintainers. Travis CI deployments are used to upload releases to PyPI . To create a new release, bump the __version__ in manubot/__init__.py . Then run the following commands: TAG = v ` python setup.py --version ` # Commit updated __version__ info git add manubot/__init__.py release-notes/ $TAG .md git commit --message = \"Prepare $TAG release\" git push # Create & push tag (assuming upstream is the manubot organization remote) git tag --annotate $TAG --file = release-notes/ $TAG .md git push upstream $TAG","title":"Release instructions"},{"location":"#goals-acknowledgments","text":"Our goal is to create scholarly infrastructure that encourages open science and assists reproducibility. Accordingly, we hope for the Manubot software and philosophy to be adopted widely, by both academic and commercial entities. As such, Manubot is free/libre and open source software (see LICENSE.md ). We would like to thank the contributors and funders whose support makes this project possible. Specifically, Manubot development has been financially supported by: the Alfred P. Sloan Foundation in Grant G-2018-11163 to @dhimmel . the Gordon & Betty Moore Foundation ( @DDD-Moore ) in Grant GBMF4552 to @cgreene .","title":"Goals &amp; Acknowledgments"},{"location":"LICENSE/","text":"BSD 3-Clause License Copyright \u00a9 2017, The Greene Lab at the University of Pennsylvania All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"LICENSE/#bsd-3-clause-license","text":"Copyright \u00a9 2017, The Greene Lab at the University of Pennsylvania All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3-Clause License"},{"location":"media/terminal-recordings/","text":"Terminal recordings manubot cite Terminal recordings were created using asciinema . The manubot cite recording is online at https://asciinema.org/a/205085?speed=2. GIF and SVG outputs were created using asciicast2gif and svg-term-cli with the following commands: asciicast2gif - s 2 . 0 https : // asciinema . org / a / 205085 . cast manubot - cite - cast . gif svg - term --window --cast=205085 --out=manubot-cite-cast.svg","title":"Terminal recordings"},{"location":"media/terminal-recordings/#terminal-recordings","text":"","title":"Terminal recordings"},{"location":"media/terminal-recordings/#manubot-cite","text":"Terminal recordings were created using asciinema . The manubot cite recording is online at https://asciinema.org/a/205085?speed=2. GIF and SVG outputs were created using asciicast2gif and svg-term-cli with the following commands: asciicast2gif - s 2 . 0 https : // asciinema . org / a / 205085 . cast manubot - cite - cast . gif svg - term --window --cast=205085 --out=manubot-cite-cast.svg","title":"manubot cite"},{"location":"reference/manubot/","text":"Module manubot View Source __version__ = '0.2.4' Sub-modules manubot.cite manubot.command manubot.pandoc manubot.process manubot.tests manubot.util manubot.webpage","title":"Index"},{"location":"reference/manubot/#module-manubot","text":"View Source __version__ = '0.2.4'","title":"Module manubot"},{"location":"reference/manubot/#sub-modules","text":"manubot.cite manubot.command manubot.pandoc manubot.process manubot.tests manubot.util manubot.webpage","title":"Sub-modules"},{"location":"reference/manubot/command/","text":"Module manubot.command Manubot's command line interface View Source \"\"\" Manubot's command line interface \"\"\" import argparse import logging import pathlib import sys import warnings import manubot from manubot.util import import_function def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = 'Manubot: the manuscript bot for scholarly writing' ) parser . add_argument ( '--version' , action = 'version' , version = f 'v{manubot.__version__}' ) subparsers = parser . add_subparsers ( title = 'subcommands' , description = 'All operations are done through subcommands:' , ) # Require specifying a sub-command subparsers . required = True # https://bugs.python.org/issue26510 subparsers . dest = 'subcommand' # https://bugs.python.org/msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( '--log-level' , default = 'WARNING' , choices = [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ], help = 'Set the logging level for stderr logging' , ) args = parser . parse_args () return args def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = 'process' , help = 'process manuscript content' , description = 'Process manuscript content to create outputs for Pandoc consumption. ' 'Performs bibliographic processing and templating.' , ) parser . add_argument ( '--content-directory' , type = pathlib . Path , required = True , help = 'Directory where manuscript content files are located.' , ) parser . add_argument ( '--output-directory' , type = pathlib . Path , required = True , help = 'Directory to output files generated by this script.' , ) parser . add_argument ( '--template-variables-path' , action = 'append' , default = [], help = 'Path or URL of a JSON file containing template variables for jinja2. ' 'Specify this argument multiple times to read multiple files. ' 'Variables can be applied to a namespace (i.e. stored under a dictionary key) ' 'like `--template-variables-path=namespace=path_or_url`. ' 'Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.' , ) parser . add_argument ( '--cache-directory' , type = pathlib . Path , help = 'Custom cache directory. ' 'If not specified, caches to output-directory.' , ) parser . add_argument ( '--clear-requests-cache' , action = 'store_true' , ) parser . set_defaults ( function = 'manubot.process.process_command.cli_process' ) def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = 'cite' , help = 'citation to CSL command line utility' , description = 'Retrieve bibliographic metadata for one or more citation keys.' , ) parser . add_argument ( '--render' , action = 'store_true' , help = 'Whether to render CSL Data into a formatted reference list using Pandoc. ' 'Pandoc version 2.0 or higher is required for complete support of available output formats.' , ) parser . add_argument ( '--csl' , default = 'https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl' , help = \"When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( '--format' , choices = [ 'plain' , 'markdown' , 'docx' , 'html' , 'jats' ], help = \"When --render, format to use for output file. \" \"If not specified, attempt to infer this from filename extension. \" \"Otherwise, default to plain.\" , ) parser . add_argument ( '--output' , type = pathlib . Path , help = 'Specify a file to write output, otherwise default to stdout.' , ) parser . add_argument ( '--allow-invalid-csl-data' , dest = 'prune_csl' , action = 'store_false' , help = 'Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.' , ) parser . add_argument ( 'citekeys' , nargs = '+' , help = 'One or more (space separated) citation keys to produce CSL for.' , ) parser . set_defaults ( function = 'manubot.cite.cite_command.cli_cite' ) def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = 'webpage' , help = 'deploy Manubot outputs to a webpage directory tree' , description = 'Update the webpage directory tree with Manubot output files. ' 'This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. ' 'HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.' , ) parser . add_argument ( '--checkout' , nargs = '?' , const = 'gh-pages' , default = None , help = 'branch to checkout /v directory contents from. ' 'For example, --checkout=upstream/gh-pages. ' '--checkout is equivalent to --checkout=gh-pages. ' 'If --checkout is ommitted, no checkout is performed.' , ) parser . add_argument ( '--version' , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" ) parser . add_argument ( '--timestamp' , action = 'store_true' , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( '--no-ots-cache' , action = 'store_true' , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( '--ots-cache' , default = pathlib . Path ( 'ci/cache/ots' ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" ) parser . set_defaults ( function = 'manubot.webpage.webpage_command.cli_webpage' ) def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 import errorhandler error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( 'always' , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( '## {levelname} \\n {message}' , style = '{' )) logger . addHandler ( stream_handler ) args = parse_arguments () logger . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) if error_handler . fired : logging . critical ( 'Failure: exiting with code 1 due to logged errors' ) raise SystemExit ( 1 ) Functions add_subparser_cite def add_subparser_cite ( subparsers ) View Source def add_subparser_cite ( subparsers ) : parser = subparsers . add_parser ( name = ' cite ' , help = ' citation to CSL command line utility ' , description = ' Retrieve bibliographic metadata for one or more citation keys. ' , ) parser . add_argument ( ' --render ' , action = ' store_true ' , help = ' Whether to render CSL Data into a formatted reference list using Pandoc. ' ' Pandoc version 2.0 or higher is required for complete support of available output formats. ' , ) parser . add_argument ( ' --csl ' , default = ' https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl ' , help = \" When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \" Defaults to Manubot's style. \" , ) parser . add_argument ( ' --format ' , choices = [ ' plain ' , ' markdown ' , ' docx ' , ' html ' , ' jats ' ], help = \" When --render, format to use for output file. \" \" If not specified, attempt to infer this from filename extension. \" \" Otherwise, default to plain. \" , ) parser . add_argument ( ' --output ' , type = pathlib . Path , help = ' Specify a file to write output, otherwise default to stdout. ' , ) parser . add_argument ( ' --allow-invalid-csl-data ' , dest = ' prune_csl ' , action = ' store_false ' , help = ' Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning. ' , ) parser . add_argument ( ' citekeys ' , nargs = ' + ' , help = ' One or more (space separated) citation keys to produce CSL for. ' , ) parser . set_defaults ( function = ' manubot.cite.cite_command.cli_cite ' ) add_subparser_process def add_subparser_process ( subparsers ) View Source def add_subparser_process ( subparsers ) : parser = subparsers . add_parser ( name = ' process ' , help = ' process manuscript content ' , description = ' Process manuscript content to create outputs for Pandoc consumption. ' ' Performs bibliographic processing and templating. ' , ) parser . add_argument ( ' --content-directory ' , type = pathlib . Path , required = True , help = ' Directory where manuscript content files are located. ' , ) parser . add_argument ( ' --output-directory ' , type = pathlib . Path , required = True , help = ' Directory to output files generated by this script. ' , ) parser . add_argument ( ' --template-variables-path ' , action = ' append ' , default = [], help = ' Path or URL of a JSON file containing template variables for jinja2. ' ' Specify this argument multiple times to read multiple files. ' ' Variables can be applied to a namespace (i.e. stored under a dictionary key) ' ' like `--template-variables-path=namespace=path_or_url`. ' ' Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`. ' , ) parser . add_argument ( ' --cache-directory ' , type = pathlib . Path , help = ' Custom cache directory. ' ' If not specified, caches to output-directory. ' , ) parser . add_argument ( ' --clear-requests-cache ' , action = ' store_true ' , ) parser . set_defaults ( function = ' manubot.process.process_command.cli_process ' ) add_subparser_webpage def add_subparser_webpage ( subparsers ) View Source def add_subparser_webpage ( subparsers ) : parser = subparsers . add_parser ( name = ' webpage ' , help = ' deploy Manubot outputs to a webpage directory tree ' , description = ' Update the webpage directory tree with Manubot output files. ' ' This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. ' ' HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting. ' , ) parser . add_argument ( ' --checkout ' , nargs = ' ? ' , const = ' gh-pages ' , default = None , help = ' branch to checkout /v directory contents from. ' ' For example, --checkout=upstream/gh-pages. ' ' --checkout is equivalent to --checkout=gh-pages. ' ' If --checkout is ommitted, no checkout is performed. ' , ) parser . add_argument ( ' --version ' , help = \" Used to create webpage/v/{version} directory. \" \" Generally a commit hash, tag, or 'local'. \" \" When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere. \" ) parser . add_argument ( ' --timestamp ' , action = ' store_true ' , help = \" timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \" Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions. \" ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( ' --no-ots-cache ' , action = ' store_true ' , help = \" disable the timestamp cache. \" ) cache_group . add_argument ( ' --ots-cache ' , default = pathlib . Path ( ' ci/cache/ots ' ) , type = pathlib . Path , help = \" location for the timestamp cache (default: ci/cache/ots). \" ) parser . set_defaults ( function = ' manubot.webpage.webpage_command.cli_webpage ' ) main def main ( ) Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. View Source def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 import errorhandler error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( 'always' , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( '## {levelname} \\n {message}' , style = '{' )) logger . addHandler ( stream_handler ) args = parse_arguments () logger . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) if error_handler . fired : logging . critical ( 'Failure: exiting with code 1 due to logged errors' ) raise SystemExit ( 1 ) parse_arguments def parse_arguments ( ) Read and process command line arguments. View Source def parse_arguments () : \"\"\" Read and process command line arguments . \"\"\" parser = argparse . ArgumentParser ( description = ' Manubot: the manuscript bot for scholarly writing ' ) parser . add_argument ( ' --version ' , action = ' version ' , version = f ' v{manubot.__version__} ' ) subparsers = parser . add_subparsers ( title = ' subcommands ' , description = ' All operations are done through subcommands: ' , ) # Require specifying a sub - command subparsers . required = True # https : // bugs . python . org / issue26510 subparsers . dest = ' subcommand ' # https : // bugs . python . org / msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values () : subparser . add_argument ( ' --log-level ' , default = ' WARNING ' , choices = [ ' DEBUG ' , ' INFO ' , ' WARNING ' , ' ERROR ' , ' CRITICAL ' ], help = ' Set the logging level for stderr logging ' , ) args = parser . parse_args () return args","title":"Command"},{"location":"reference/manubot/command/#module-manubotcommand","text":"Manubot's command line interface View Source \"\"\" Manubot's command line interface \"\"\" import argparse import logging import pathlib import sys import warnings import manubot from manubot.util import import_function def parse_arguments (): \"\"\" Read and process command line arguments. \"\"\" parser = argparse . ArgumentParser ( description = 'Manubot: the manuscript bot for scholarly writing' ) parser . add_argument ( '--version' , action = 'version' , version = f 'v{manubot.__version__}' ) subparsers = parser . add_subparsers ( title = 'subcommands' , description = 'All operations are done through subcommands:' , ) # Require specifying a sub-command subparsers . required = True # https://bugs.python.org/issue26510 subparsers . dest = 'subcommand' # https://bugs.python.org/msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values (): subparser . add_argument ( '--log-level' , default = 'WARNING' , choices = [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ], help = 'Set the logging level for stderr logging' , ) args = parser . parse_args () return args def add_subparser_process ( subparsers ): parser = subparsers . add_parser ( name = 'process' , help = 'process manuscript content' , description = 'Process manuscript content to create outputs for Pandoc consumption. ' 'Performs bibliographic processing and templating.' , ) parser . add_argument ( '--content-directory' , type = pathlib . Path , required = True , help = 'Directory where manuscript content files are located.' , ) parser . add_argument ( '--output-directory' , type = pathlib . Path , required = True , help = 'Directory to output files generated by this script.' , ) parser . add_argument ( '--template-variables-path' , action = 'append' , default = [], help = 'Path or URL of a JSON file containing template variables for jinja2. ' 'Specify this argument multiple times to read multiple files. ' 'Variables can be applied to a namespace (i.e. stored under a dictionary key) ' 'like `--template-variables-path=namespace=path_or_url`. ' 'Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`.' , ) parser . add_argument ( '--cache-directory' , type = pathlib . Path , help = 'Custom cache directory. ' 'If not specified, caches to output-directory.' , ) parser . add_argument ( '--clear-requests-cache' , action = 'store_true' , ) parser . set_defaults ( function = 'manubot.process.process_command.cli_process' ) def add_subparser_cite ( subparsers ): parser = subparsers . add_parser ( name = 'cite' , help = 'citation to CSL command line utility' , description = 'Retrieve bibliographic metadata for one or more citation keys.' , ) parser . add_argument ( '--render' , action = 'store_true' , help = 'Whether to render CSL Data into a formatted reference list using Pandoc. ' 'Pandoc version 2.0 or higher is required for complete support of available output formats.' , ) parser . add_argument ( '--csl' , default = 'https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl' , help = \"When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \"Defaults to Manubot's style.\" , ) parser . add_argument ( '--format' , choices = [ 'plain' , 'markdown' , 'docx' , 'html' , 'jats' ], help = \"When --render, format to use for output file. \" \"If not specified, attempt to infer this from filename extension. \" \"Otherwise, default to plain.\" , ) parser . add_argument ( '--output' , type = pathlib . Path , help = 'Specify a file to write output, otherwise default to stdout.' , ) parser . add_argument ( '--allow-invalid-csl-data' , dest = 'prune_csl' , action = 'store_false' , help = 'Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning.' , ) parser . add_argument ( 'citekeys' , nargs = '+' , help = 'One or more (space separated) citation keys to produce CSL for.' , ) parser . set_defaults ( function = 'manubot.cite.cite_command.cli_cite' ) def add_subparser_webpage ( subparsers ): parser = subparsers . add_parser ( name = 'webpage' , help = 'deploy Manubot outputs to a webpage directory tree' , description = 'Update the webpage directory tree with Manubot output files. ' 'This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. ' 'HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting.' , ) parser . add_argument ( '--checkout' , nargs = '?' , const = 'gh-pages' , default = None , help = 'branch to checkout /v directory contents from. ' 'For example, --checkout=upstream/gh-pages. ' '--checkout is equivalent to --checkout=gh-pages. ' 'If --checkout is ommitted, no checkout is performed.' , ) parser . add_argument ( '--version' , help = \"Used to create webpage/v/{version} directory. \" \"Generally a commit hash, tag, or 'local'. \" \"When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere.\" ) parser . add_argument ( '--timestamp' , action = 'store_true' , help = \"timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \"Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions.\" ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( '--no-ots-cache' , action = 'store_true' , help = \"disable the timestamp cache.\" ) cache_group . add_argument ( '--ots-cache' , default = pathlib . Path ( 'ci/cache/ots' ), type = pathlib . Path , help = \"location for the timestamp cache (default: ci/cache/ots).\" ) parser . set_defaults ( function = 'manubot.webpage.webpage_command.cli_webpage' ) def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 import errorhandler error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( 'always' , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( '## {levelname} \\n {message}' , style = '{' )) logger . addHandler ( stream_handler ) args = parse_arguments () logger . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) if error_handler . fired : logging . critical ( 'Failure: exiting with code 1 due to logged errors' ) raise SystemExit ( 1 )","title":"Module manubot.command"},{"location":"reference/manubot/command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/command/#add_subparser_cite","text":"def add_subparser_cite ( subparsers ) View Source def add_subparser_cite ( subparsers ) : parser = subparsers . add_parser ( name = ' cite ' , help = ' citation to CSL command line utility ' , description = ' Retrieve bibliographic metadata for one or more citation keys. ' , ) parser . add_argument ( ' --render ' , action = ' store_true ' , help = ' Whether to render CSL Data into a formatted reference list using Pandoc. ' ' Pandoc version 2.0 or higher is required for complete support of available output formats. ' , ) parser . add_argument ( ' --csl ' , default = ' https://github.com/greenelab/manubot-rootstock/raw/master/build/assets/style.csl ' , help = \" When --render, specify an XML CSL definition to style references (i.e. Pandoc's --csl option). \" \" Defaults to Manubot's style. \" , ) parser . add_argument ( ' --format ' , choices = [ ' plain ' , ' markdown ' , ' docx ' , ' html ' , ' jats ' ], help = \" When --render, format to use for output file. \" \" If not specified, attempt to infer this from filename extension. \" \" Otherwise, default to plain. \" , ) parser . add_argument ( ' --output ' , type = pathlib . Path , help = ' Specify a file to write output, otherwise default to stdout. ' , ) parser . add_argument ( ' --allow-invalid-csl-data ' , dest = ' prune_csl ' , action = ' store_false ' , help = ' Allow CSL Items that do not conform to the JSON Schema. Skips CSL pruning. ' , ) parser . add_argument ( ' citekeys ' , nargs = ' + ' , help = ' One or more (space separated) citation keys to produce CSL for. ' , ) parser . set_defaults ( function = ' manubot.cite.cite_command.cli_cite ' )","title":"add_subparser_cite"},{"location":"reference/manubot/command/#add_subparser_process","text":"def add_subparser_process ( subparsers ) View Source def add_subparser_process ( subparsers ) : parser = subparsers . add_parser ( name = ' process ' , help = ' process manuscript content ' , description = ' Process manuscript content to create outputs for Pandoc consumption. ' ' Performs bibliographic processing and templating. ' , ) parser . add_argument ( ' --content-directory ' , type = pathlib . Path , required = True , help = ' Directory where manuscript content files are located. ' , ) parser . add_argument ( ' --output-directory ' , type = pathlib . Path , required = True , help = ' Directory to output files generated by this script. ' , ) parser . add_argument ( ' --template-variables-path ' , action = ' append ' , default = [], help = ' Path or URL of a JSON file containing template variables for jinja2. ' ' Specify this argument multiple times to read multiple files. ' ' Variables can be applied to a namespace (i.e. stored under a dictionary key) ' ' like `--template-variables-path=namespace=path_or_url`. ' ' Namespaces must match the regex `[a-zA-Z_][a-zA-Z0-9_]*`. ' , ) parser . add_argument ( ' --cache-directory ' , type = pathlib . Path , help = ' Custom cache directory. ' ' If not specified, caches to output-directory. ' , ) parser . add_argument ( ' --clear-requests-cache ' , action = ' store_true ' , ) parser . set_defaults ( function = ' manubot.process.process_command.cli_process ' )","title":"add_subparser_process"},{"location":"reference/manubot/command/#add_subparser_webpage","text":"def add_subparser_webpage ( subparsers ) View Source def add_subparser_webpage ( subparsers ) : parser = subparsers . add_parser ( name = ' webpage ' , help = ' deploy Manubot outputs to a webpage directory tree ' , description = ' Update the webpage directory tree with Manubot output files. ' ' This command should be run from the root directory of a Manubot manuscript that follows the Rootstock layout, containing `output` and `webpage` directories. ' ' HTML and PDF outputs are copied to the webpage directory, which is structured as static source files for website hosting. ' , ) parser . add_argument ( ' --checkout ' , nargs = ' ? ' , const = ' gh-pages ' , default = None , help = ' branch to checkout /v directory contents from. ' ' For example, --checkout=upstream/gh-pages. ' ' --checkout is equivalent to --checkout=gh-pages. ' ' If --checkout is ommitted, no checkout is performed. ' , ) parser . add_argument ( ' --version ' , help = \" Used to create webpage/v/{version} directory. \" \" Generally a commit hash, tag, or 'local'. \" \" When omitted, version defaults to the commit hash on CI builds and 'local' elsewhere. \" ) parser . add_argument ( ' --timestamp ' , action = ' store_true ' , help = \" timestamp versioned manuscripts in webpage/v using OpenTimestamps. \" \" Specify this flag to create timestamps for the current HTML and PDF outputs and upgrade any timestamps from past manuscript versions. \" ) cache_group = parser . add_mutually_exclusive_group () cache_group . add_argument ( ' --no-ots-cache ' , action = ' store_true ' , help = \" disable the timestamp cache. \" ) cache_group . add_argument ( ' --ots-cache ' , default = pathlib . Path ( ' ci/cache/ots ' ) , type = pathlib . Path , help = \" location for the timestamp cache (default: ci/cache/ots). \" ) parser . set_defaults ( function = ' manubot.webpage.webpage_command.cli_webpage ' )","title":"add_subparser_webpage"},{"location":"reference/manubot/command/#main","text":"def main ( ) Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. View Source def main (): \"\"\" Called as a console_scripts entry point in setup.py. This function defines the manubot command line script. \"\"\" # Track if message gets logged with severity of error or greater # See https://stackoverflow.com/a/45446664/4651668 import errorhandler error_handler = errorhandler . ErrorHandler () # Log DeprecationWarnings warnings . simplefilter ( 'always' , DeprecationWarning ) logging . captureWarnings ( True ) # Log to stderr logger = logging . getLogger () stream_handler = logging . StreamHandler ( stream = sys . stderr ) stream_handler . setFormatter ( logging . Formatter ( '## {levelname} \\n {message}' , style = '{' )) logger . addHandler ( stream_handler ) args = parse_arguments () logger . setLevel ( getattr ( logging , args . log_level )) function = import_function ( args . function ) function ( args ) if error_handler . fired : logging . critical ( 'Failure: exiting with code 1 due to logged errors' ) raise SystemExit ( 1 )","title":"main"},{"location":"reference/manubot/command/#parse_arguments","text":"def parse_arguments ( ) Read and process command line arguments. View Source def parse_arguments () : \"\"\" Read and process command line arguments . \"\"\" parser = argparse . ArgumentParser ( description = ' Manubot: the manuscript bot for scholarly writing ' ) parser . add_argument ( ' --version ' , action = ' version ' , version = f ' v{manubot.__version__} ' ) subparsers = parser . add_subparsers ( title = ' subcommands ' , description = ' All operations are done through subcommands: ' , ) # Require specifying a sub - command subparsers . required = True # https : // bugs . python . org / issue26510 subparsers . dest = ' subcommand ' # https : // bugs . python . org / msg186387 add_subparser_process ( subparsers ) add_subparser_cite ( subparsers ) add_subparser_webpage ( subparsers ) for subparser in subparsers . choices . values () : subparser . add_argument ( ' --log-level ' , default = ' WARNING ' , choices = [ ' DEBUG ' , ' INFO ' , ' WARNING ' , ' ERROR ' , ' CRITICAL ' ], help = ' Set the logging level for stderr logging ' , ) args = parser . parse_args () return args","title":"parse_arguments"},{"location":"reference/manubot/util/","text":"Module manubot.util View Source import importlib import platform import shlex import sys # Email address that forwards to Manubot maintainers contact_email = 'contact@manubot.org' def import_function ( name ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = '' return ( f 'manubot/{manubot_version} ' f '({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) ' f '<{contact_email}>' ) def shlex_join ( split_command ): \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return ' ' . join ( shlex . quote ( str ( arg )) for arg in split_command ) Variables contact_email Functions get_manubot_user_agent def get_manubot_user_agent ( ) Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. View Source def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = '' return ( f 'manubot/{manubot_version} ' f '({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) ' f '<{contact_email}>' ) import_function def import_function ( name ) Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. View Source def import_function ( name ) : \"\"\" Import a function in a module specified by name . For example , if name were ' manubot.cite.cite_command.cli_cite ' , the cli_cite function would be returned as an object . See https : // stackoverflow . com / a / 8790232 / 4651668 . \"\"\" module_name , function_name = name . rsplit ( ' . ' , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) shlex_join def shlex_join ( split_command ) Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 View Source def shlex_join ( split_command ) : \"\"\" Backport shlex . join for Python < 3 . 8 . Also cast all args to str to increase versatility . https : // github . com / python / cpython / pull / 7605 https : // bugs . python . org / issue22454 \"\"\" return ' ' . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"Util"},{"location":"reference/manubot/util/#module-manubotutil","text":"View Source import importlib import platform import shlex import sys # Email address that forwards to Manubot maintainers contact_email = 'contact@manubot.org' def import_function ( name ): \"\"\" Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. \"\"\" module_name , function_name = name . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name ) def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = '' return ( f 'manubot/{manubot_version} ' f '({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) ' f '<{contact_email}>' ) def shlex_join ( split_command ): \"\"\" Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 \"\"\" return ' ' . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"Module manubot.util"},{"location":"reference/manubot/util/#variables","text":"contact_email","title":"Variables"},{"location":"reference/manubot/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/util/#get_manubot_user_agent","text":"def get_manubot_user_agent ( ) Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. View Source def get_manubot_user_agent (): \"\"\" Return a User-Agent string for web request headers to help services identify requests as coming from Manubot. \"\"\" try : from manubot import __version__ as manubot_version except ImportError : manubot_version = '' return ( f 'manubot/{manubot_version} ' f '({platform.system()}; Python/{sys.version_info.major}.{sys.version_info.minor}) ' f '<{contact_email}>' )","title":"get_manubot_user_agent"},{"location":"reference/manubot/util/#import_function","text":"def import_function ( name ) Import a function in a module specified by name. For example, if name were 'manubot.cite.cite_command.cli_cite', the cli_cite function would be returned as an object. See https://stackoverflow.com/a/8790232/4651668. View Source def import_function ( name ) : \"\"\" Import a function in a module specified by name . For example , if name were ' manubot.cite.cite_command.cli_cite ' , the cli_cite function would be returned as an object . See https : // stackoverflow . com / a / 8790232 / 4651668 . \"\"\" module_name , function_name = name . rsplit ( ' . ' , 1 ) module = importlib . import_module ( module_name ) return getattr ( module , function_name )","title":"import_function"},{"location":"reference/manubot/util/#shlex_join","text":"def shlex_join ( split_command ) Backport shlex.join for Python < 3.8. Also cast all args to str to increase versatility. https://github.com/python/cpython/pull/7605 https://bugs.python.org/issue22454 View Source def shlex_join ( split_command ) : \"\"\" Backport shlex . join for Python < 3 . 8 . Also cast all args to str to increase versatility . https : // github . com / python / cpython / pull / 7605 https : // bugs . python . org / issue22454 \"\"\" return ' ' . join ( shlex . quote ( str ( arg )) for arg in split_command )","title":"shlex_join"},{"location":"reference/manubot/cite/","text":"Module manubot.cite View Source __all__ = [ 'citation_to_citeproc' , 'citekey_to_csl_item' , 'standardize_citation' , 'standardize_citekey' , ] from manubot.cite.citekey import ( citekey_to_csl_item , standardize_citekey , ) def citation_to_citeproc ( * args , ** kwargs ): import warnings warnings . warn ( \"'citation_to_citeproc' has been renamed to 'citekey_to_csl_item'\" \" and will be removed in a future release.\" , category = FutureWarning ) return citekey_to_csl_item ( * args , ** kwargs ) def standardize_citation ( * args , ** kwargs ): import warnings warnings . warn ( \"'standardize_citation' has been renamed to 'standardize_citekey'\" \" and will be removed in a future release.\" , category = FutureWarning ) return standardize_citekey ( * args , ** kwargs ) Sub-modules manubot.cite.arxiv manubot.cite.cite_command manubot.cite.citekey manubot.cite.citeproc manubot.cite.csl_item manubot.cite.doi manubot.cite.isbn manubot.cite.pubmed manubot.cite.tests manubot.cite.url manubot.cite.wikidata manubot.cite.zotero Functions citation_to_citeproc def citation_to_citeproc ( * args , ** kwargs ) View Source def citation_to_citeproc ( * args , ** kwargs ): import warnings warnings . warn ( \"'citation_to_citeproc' has been renamed to 'citekey_to_csl_item'\" \" and will be removed in a future release.\" , category = FutureWarning ) return citekey_to_csl_item ( * args , ** kwargs ) citekey_to_csl_item def citekey_to_csl_item ( citekey , prune = True ) Generate a CSL Item (Python dictionary) for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True ): \"\"\" Generate a CSL Item (Python dictionary) for the input citekey. \"\"\" citekey == standardize_citekey ( citekey , warn_if_changed = True ) source , identifier = citekey . split ( ':' , 1 ) if source in citeproc_retrievers : citeproc_retriever = import_function ( citeproc_retrievers [ source ]) csl_item = citeproc_retriever ( identifier ) else : msg = f 'Unsupported citation source {source!r} in {citekey!r}' raise ValueError ( msg ) from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( csl_item_passthrough , append_to_csl_item_note , ) note_text = f 'This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.' note_dict = { 'standard_id' : citekey , } append_to_csl_item_note ( csl_item , note_text , note_dict ) short_citekey = shorten_citekey ( citekey ) csl_item = csl_item_passthrough ( csl_item , set_id = short_citekey , prune = prune ) return csl_item standardize_citation def standardize_citation ( * args , ** kwargs ) View Source def standardize_citation ( * args , ** kwargs ): import warnings warnings . warn ( \"'standardize_citation' has been renamed to 'standardize_citekey'\" \" and will be removed in a future release.\" , category = FutureWarning ) return standardize_citekey ( * args , ** kwargs ) standardize_citekey def standardize_citekey ( citekey , warn_if_changed = False ) Standardize citation keys based on their source View Source @functools.lru_cache ( maxsize = 5 _000 ) def standardize_citekey ( citekey , warn_if_changed = False ): \"\"\" Standardize citation keys based on their source \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'doi' : if identifier . startswith ( '10/' ): from manubot.cite.doi import expand_short_doi try : identifier = expand_short_doi ( identifier ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f 'Error in expand_short_doi for {identifier} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) identifier = identifier . lower () if source == 'isbn' : from isbnlib import to_isbn13 identifier = to_isbn13 ( identifier ) standard_citekey = f '{source}:{identifier}' if warn_if_changed and citekey != standard_citekey : logging . warning ( f 'standardize_citekey expected citekey to already be standardized. \\n ' f 'Instead citekey was changed from {citekey!r} to {standard_citekey!r}' ) return standard_citekey","title":"Index"},{"location":"reference/manubot/cite/#module-manubotcite","text":"View Source __all__ = [ 'citation_to_citeproc' , 'citekey_to_csl_item' , 'standardize_citation' , 'standardize_citekey' , ] from manubot.cite.citekey import ( citekey_to_csl_item , standardize_citekey , ) def citation_to_citeproc ( * args , ** kwargs ): import warnings warnings . warn ( \"'citation_to_citeproc' has been renamed to 'citekey_to_csl_item'\" \" and will be removed in a future release.\" , category = FutureWarning ) return citekey_to_csl_item ( * args , ** kwargs ) def standardize_citation ( * args , ** kwargs ): import warnings warnings . warn ( \"'standardize_citation' has been renamed to 'standardize_citekey'\" \" and will be removed in a future release.\" , category = FutureWarning ) return standardize_citekey ( * args , ** kwargs )","title":"Module manubot.cite"},{"location":"reference/manubot/cite/#sub-modules","text":"manubot.cite.arxiv manubot.cite.cite_command manubot.cite.citekey manubot.cite.citeproc manubot.cite.csl_item manubot.cite.doi manubot.cite.isbn manubot.cite.pubmed manubot.cite.tests manubot.cite.url manubot.cite.wikidata manubot.cite.zotero","title":"Sub-modules"},{"location":"reference/manubot/cite/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/#citation_to_citeproc","text":"def citation_to_citeproc ( * args , ** kwargs ) View Source def citation_to_citeproc ( * args , ** kwargs ): import warnings warnings . warn ( \"'citation_to_citeproc' has been renamed to 'citekey_to_csl_item'\" \" and will be removed in a future release.\" , category = FutureWarning ) return citekey_to_csl_item ( * args , ** kwargs )","title":"citation_to_citeproc"},{"location":"reference/manubot/cite/#citekey_to_csl_item","text":"def citekey_to_csl_item ( citekey , prune = True ) Generate a CSL Item (Python dictionary) for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True ): \"\"\" Generate a CSL Item (Python dictionary) for the input citekey. \"\"\" citekey == standardize_citekey ( citekey , warn_if_changed = True ) source , identifier = citekey . split ( ':' , 1 ) if source in citeproc_retrievers : citeproc_retriever = import_function ( citeproc_retrievers [ source ]) csl_item = citeproc_retriever ( identifier ) else : msg = f 'Unsupported citation source {source!r} in {citekey!r}' raise ValueError ( msg ) from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( csl_item_passthrough , append_to_csl_item_note , ) note_text = f 'This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.' note_dict = { 'standard_id' : citekey , } append_to_csl_item_note ( csl_item , note_text , note_dict ) short_citekey = shorten_citekey ( citekey ) csl_item = csl_item_passthrough ( csl_item , set_id = short_citekey , prune = prune ) return csl_item","title":"citekey_to_csl_item"},{"location":"reference/manubot/cite/#standardize_citation","text":"def standardize_citation ( * args , ** kwargs ) View Source def standardize_citation ( * args , ** kwargs ): import warnings warnings . warn ( \"'standardize_citation' has been renamed to 'standardize_citekey'\" \" and will be removed in a future release.\" , category = FutureWarning ) return standardize_citekey ( * args , ** kwargs )","title":"standardize_citation"},{"location":"reference/manubot/cite/#standardize_citekey","text":"def standardize_citekey ( citekey , warn_if_changed = False ) Standardize citation keys based on their source View Source @functools.lru_cache ( maxsize = 5 _000 ) def standardize_citekey ( citekey , warn_if_changed = False ): \"\"\" Standardize citation keys based on their source \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'doi' : if identifier . startswith ( '10/' ): from manubot.cite.doi import expand_short_doi try : identifier = expand_short_doi ( identifier ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f 'Error in expand_short_doi for {identifier} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) identifier = identifier . lower () if source == 'isbn' : from isbnlib import to_isbn13 identifier = to_isbn13 ( identifier ) standard_citekey = f '{source}:{identifier}' if warn_if_changed and citekey != standard_citekey : logging . warning ( f 'standardize_citekey expected citekey to already be standardized. \\n ' f 'Instead citekey was changed from {citekey!r} to {standard_citekey!r}' ) return standard_citekey","title":"standardize_citekey"},{"location":"reference/manubot/cite/arxiv/","text":"Module manubot.cite.arxiv View Source import collections import logging import re import xml.etree.ElementTree import requests from manubot.util import get_manubot_user_agent def get_arxiv_csl_item ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: https://arxiv.org/help/api/index http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" url = 'https://export.arxiv.org/api/query' params = { 'id_list' : arxiv_id , 'max_results' : 1 , } headers = { 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( url , params , headers = headers ) # XML namespace prefixes prefix = '{http://www.w3.org/2005/Atom}' alt_prefix = '{http://arxiv.org/schemas/atom}' # Parse XML xml_tree = xml . etree . ElementTree . fromstring ( response . text ) entry , = xml_tree . findall ( prefix + 'entry' ) # Create dictionary for CSL Item csl_item = collections . OrderedDict () # Extract versioned arXiv ID url = entry . findtext ( prefix + 'id' ) pattern = re . compile ( r 'arxiv.org/abs/(.+)' ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item [ 'number' ] = versioned_id _ , csl_item [ 'version' ] = versioned_id . rsplit ( 'v' , 1 ) csl_item [ 'URL' ] = 'https://arxiv.org/abs/' + versioned_id # Extrat CSL title field csl_item [ 'title' ] = entry . findtext ( prefix + 'title' ) # Extract CSL date field published = entry . findtext ( prefix + 'published' ) published , _ = published . split ( 'T' , 1 ) csl_item [ 'issued' ] = { 'date-parts' : [[ int ( x ) for x in published . split ( '-' )]]} # Extract authors authors = list () for elem in entry . findall ( prefix + 'author' ): name = elem . findtext ( prefix + 'name' ) author = { 'literal' : name } authors . append ( author ) csl_item [ 'author' ] = authors # Set publisher to arXiv csl_item [ 'container-title' ] = 'arXiv' csl_item [ 'publisher' ] = 'arXiv' # Extract abstract abstract = entry . findtext ( prefix + 'summary' ) . strip () if abstract : csl_item [ 'abstract' ] = abstract # Check if the article has been published with a DOI DOI = entry . findtext ( '{http://arxiv.org/schemas/atom}doi' ) if DOI : csl_item [ 'DOI' ] = DOI journal_ref = entry . findtext ( alt_prefix + 'journal_ref' ) msg = f 'arXiv article {arxiv_id} published at https://doi.org/{DOI}' if journal_ref : msg += f ' \u2014 {journal_ref}' logging . warning ( msg ) # Set CSL type to report for preprint csl_item [ 'type' ] = 'report' return csl_item Functions get_arxiv_csl_item def get_arxiv_csl_item ( arxiv_id ) Return csl_item item for an arXiv record. arxiv_id can be versioned, like 1512.00567v2 , or versionless, like 1512.00567 . If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as cond-mat/0703470v2 , are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: https://arxiv.org/help/api/index http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def get_arxiv_csl_item ( arxiv_id ) : \"\"\" Return csl_item item for an arXiv record . arxiv_id can be versioned , like ` 1512 . 00567 v2 `, or versionless , like ` 1512 . 00567 `. If versionless , the arXiv API will return metadata for the latest version . Legacy IDs , such as ` cond - mat / 0703470 v2 `, are also supported . If arXiv has an associated DOI for the record , a warning is logged to alert the user that an alternative version of record exists . References : https : // arxiv . org / help / api / index http : // citeproc - js . readthedocs . io / en / latest / csl - json / markup . html https : // github . com / citation - style - language / schema / blob / master / csl - data . json \"\"\" url = ' https://export.arxiv.org/api/query ' params = { ' id_list ' : arxiv_id , ' max_results ' : 1 , } headers = { ' User-Agent ' : get_manubot_user_agent () , } response = requests . get ( url , params , headers = headers ) # XML namespace prefixes prefix = ' {http://www.w3.org/2005/Atom} ' alt_prefix = ' {http://arxiv.org/schemas/atom} ' # Parse XML xml_tree = xml . etree . ElementTree . fromstring ( response . text ) entry , = xml_tree . findall ( prefix + ' entry ' ) # Create dictionary for CSL Item csl_item = collections . OrderedDict () # Extract versioned arXiv ID url = entry . findtext ( prefix + ' id ' ) pattern = re . compile ( r ' arxiv.org/abs/(.+) ' ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item [ ' number ' ] = versioned_id _ , csl_item [ ' version ' ] = versioned_id . rsplit ( ' v ' , 1 ) csl_item [ ' URL ' ] = ' https://arxiv.org/abs/ ' + versioned_id # Extrat CSL title field csl_item [ ' title ' ] = entry . findtext ( prefix + ' title ' ) # Extract CSL date field published = entry . findtext ( prefix + ' published ' ) published , _ = published . split ( ' T ' , 1 ) csl_item [ ' issued ' ] = { ' date-parts ' : [[ int ( x ) for x in published . split ( ' - ' ) ]]} # Extract authors authors = list () for elem in entry . findall ( prefix + ' author ' ) : name = elem . findtext ( prefix + ' name ' ) author = { ' literal ' : name } authors . append ( author ) csl_item [ ' author ' ] = authors # Set publisher to arXiv csl_item [ ' container-title ' ] = ' arXiv ' csl_item [ ' publisher ' ] = ' arXiv ' # Extract abstract abstract = entry . findtext ( prefix + ' summary ' ) . strip () if abstract : csl_item [ ' abstract ' ] = abstract # Check if the article has been published with a DOI DOI = entry . findtext ( ' {http://arxiv.org/schemas/atom}doi ' ) if DOI : csl_item [ ' DOI ' ] = DOI journal_ref = entry . findtext ( alt_prefix + ' journal_ref ' ) msg = f ' arXiv article {arxiv_id} published at https://doi.org/{DOI} ' if journal_ref : msg += f ' \u2014 {journal_ref} ' logging . warning ( msg ) # Set CSL type to report for preprint csl_item [ ' type ' ] = ' report ' return csl_item","title":"Arxiv"},{"location":"reference/manubot/cite/arxiv/#module-manubotcitearxiv","text":"View Source import collections import logging import re import xml.etree.ElementTree import requests from manubot.util import get_manubot_user_agent def get_arxiv_csl_item ( arxiv_id ): \"\"\" Return csl_item item for an arXiv record. arxiv_id can be versioned, like `1512.00567v2`, or versionless, like `1512.00567`. If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as `cond-mat/0703470v2`, are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: https://arxiv.org/help/api/index http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" url = 'https://export.arxiv.org/api/query' params = { 'id_list' : arxiv_id , 'max_results' : 1 , } headers = { 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( url , params , headers = headers ) # XML namespace prefixes prefix = '{http://www.w3.org/2005/Atom}' alt_prefix = '{http://arxiv.org/schemas/atom}' # Parse XML xml_tree = xml . etree . ElementTree . fromstring ( response . text ) entry , = xml_tree . findall ( prefix + 'entry' ) # Create dictionary for CSL Item csl_item = collections . OrderedDict () # Extract versioned arXiv ID url = entry . findtext ( prefix + 'id' ) pattern = re . compile ( r 'arxiv.org/abs/(.+)' ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item [ 'number' ] = versioned_id _ , csl_item [ 'version' ] = versioned_id . rsplit ( 'v' , 1 ) csl_item [ 'URL' ] = 'https://arxiv.org/abs/' + versioned_id # Extrat CSL title field csl_item [ 'title' ] = entry . findtext ( prefix + 'title' ) # Extract CSL date field published = entry . findtext ( prefix + 'published' ) published , _ = published . split ( 'T' , 1 ) csl_item [ 'issued' ] = { 'date-parts' : [[ int ( x ) for x in published . split ( '-' )]]} # Extract authors authors = list () for elem in entry . findall ( prefix + 'author' ): name = elem . findtext ( prefix + 'name' ) author = { 'literal' : name } authors . append ( author ) csl_item [ 'author' ] = authors # Set publisher to arXiv csl_item [ 'container-title' ] = 'arXiv' csl_item [ 'publisher' ] = 'arXiv' # Extract abstract abstract = entry . findtext ( prefix + 'summary' ) . strip () if abstract : csl_item [ 'abstract' ] = abstract # Check if the article has been published with a DOI DOI = entry . findtext ( '{http://arxiv.org/schemas/atom}doi' ) if DOI : csl_item [ 'DOI' ] = DOI journal_ref = entry . findtext ( alt_prefix + 'journal_ref' ) msg = f 'arXiv article {arxiv_id} published at https://doi.org/{DOI}' if journal_ref : msg += f ' \u2014 {journal_ref}' logging . warning ( msg ) # Set CSL type to report for preprint csl_item [ 'type' ] = 'report' return csl_item","title":"Module manubot.cite.arxiv"},{"location":"reference/manubot/cite/arxiv/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/arxiv/#get_arxiv_csl_item","text":"def get_arxiv_csl_item ( arxiv_id ) Return csl_item item for an arXiv record. arxiv_id can be versioned, like 1512.00567v2 , or versionless, like 1512.00567 . If versionless, the arXiv API will return metadata for the latest version. Legacy IDs, such as cond-mat/0703470v2 , are also supported. If arXiv has an associated DOI for the record, a warning is logged to alert the user that an alternative version of record exists. References: https://arxiv.org/help/api/index http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def get_arxiv_csl_item ( arxiv_id ) : \"\"\" Return csl_item item for an arXiv record . arxiv_id can be versioned , like ` 1512 . 00567 v2 `, or versionless , like ` 1512 . 00567 `. If versionless , the arXiv API will return metadata for the latest version . Legacy IDs , such as ` cond - mat / 0703470 v2 `, are also supported . If arXiv has an associated DOI for the record , a warning is logged to alert the user that an alternative version of record exists . References : https : // arxiv . org / help / api / index http : // citeproc - js . readthedocs . io / en / latest / csl - json / markup . html https : // github . com / citation - style - language / schema / blob / master / csl - data . json \"\"\" url = ' https://export.arxiv.org/api/query ' params = { ' id_list ' : arxiv_id , ' max_results ' : 1 , } headers = { ' User-Agent ' : get_manubot_user_agent () , } response = requests . get ( url , params , headers = headers ) # XML namespace prefixes prefix = ' {http://www.w3.org/2005/Atom} ' alt_prefix = ' {http://arxiv.org/schemas/atom} ' # Parse XML xml_tree = xml . etree . ElementTree . fromstring ( response . text ) entry , = xml_tree . findall ( prefix + ' entry ' ) # Create dictionary for CSL Item csl_item = collections . OrderedDict () # Extract versioned arXiv ID url = entry . findtext ( prefix + ' id ' ) pattern = re . compile ( r ' arxiv.org/abs/(.+) ' ) match = pattern . search ( url ) versioned_id = match . group ( 1 ) csl_item [ ' number ' ] = versioned_id _ , csl_item [ ' version ' ] = versioned_id . rsplit ( ' v ' , 1 ) csl_item [ ' URL ' ] = ' https://arxiv.org/abs/ ' + versioned_id # Extrat CSL title field csl_item [ ' title ' ] = entry . findtext ( prefix + ' title ' ) # Extract CSL date field published = entry . findtext ( prefix + ' published ' ) published , _ = published . split ( ' T ' , 1 ) csl_item [ ' issued ' ] = { ' date-parts ' : [[ int ( x ) for x in published . split ( ' - ' ) ]]} # Extract authors authors = list () for elem in entry . findall ( prefix + ' author ' ) : name = elem . findtext ( prefix + ' name ' ) author = { ' literal ' : name } authors . append ( author ) csl_item [ ' author ' ] = authors # Set publisher to arXiv csl_item [ ' container-title ' ] = ' arXiv ' csl_item [ ' publisher ' ] = ' arXiv ' # Extract abstract abstract = entry . findtext ( prefix + ' summary ' ) . strip () if abstract : csl_item [ ' abstract ' ] = abstract # Check if the article has been published with a DOI DOI = entry . findtext ( ' {http://arxiv.org/schemas/atom}doi ' ) if DOI : csl_item [ ' DOI ' ] = DOI journal_ref = entry . findtext ( alt_prefix + ' journal_ref ' ) msg = f ' arXiv article {arxiv_id} published at https://doi.org/{DOI} ' if journal_ref : msg += f ' \u2014 {journal_ref} ' logging . warning ( msg ) # Set CSL type to report for preprint csl_item [ ' type ' ] = ' report ' return csl_item","title":"get_arxiv_csl_item"},{"location":"reference/manubot/cite/cite_command/","text":"Module manubot.cite.cite_command View Source import json import logging import pathlib import subprocess import sys from manubot.cite.citekey import ( citekey_to_csl_item , standardize_citekey , is_valid_citekey ) from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join # For manubot cite, infer --format from --output filename extensions extension_to_format = { '.txt' : 'plain' , '.md' : 'markdown' , '.docx' : 'docx' , '.html' : 'html' , '.xml' : 'jats' , } def call_pandoc ( metadata , path , format = 'plain' ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = '--- \\n {yaml} \\n ... \\n ' . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ 'pandoc' , '--filter' , 'pandoc-citeproc' , '--output' , str ( path ) if path else '-' , ] if format == 'markdown' : args . extend ([ '--to' , 'markdown_strict' , '--wrap' , 'none' ]) elif format == 'jats' : args . extend ([ '--to' , 'jats' , '--standalone' ]) elif format == 'docx' : args . extend ([ '--to' , 'docx' ]) elif format == 'html' : args . extend ([ '--to' , 'html' ]) elif format == 'plain' : args . extend ([ '--to' , 'plain' , '--wrap' , 'none' ]) if info [ 'pandoc version' ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https://github.com/jgm/pandoc/issues/4834#issuecomment-412972008 filter_path = pathlib . Path ( __file__ ) . joinpath ( '..' , 'plain-pandoc-filter.lua' ) . resolve () assert filter_path . exists () args . extend ([ '--lua-filter' , str ( filter_path )]) logging . info ( 'call_pandoc subprocess args: \\n ' + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (), stdout = subprocess . PIPE if path else sys . stdout , stderr = sys . stderr , ) process . check_returncode () def cli_cite ( args ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citaiton rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" # generate CSL JSON data csl_list = list () for citekey in args . citekeys : try : if not is_valid_citekey ( citekey ): continue citekey = standardize_citekey ( citekey ) csl_item = citekey_to_csl_item ( citekey , prune = args . prune_csl ) csl_list . append ( csl_item ) except Exception as error : logging . error ( f 'citekey_to_csl_item for {citekey!r} failed ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) # output CSL JSON data, if --render is False if not args . render : write_file = args . output . open ( 'w' , encoding = 'utf-8' ) if args . output else sys . stdout with write_file : json . dump ( csl_list , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) return # use Pandoc to render references if not args . format and args . output : vars ( args )[ 'format' ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args )[ 'format' ] = 'plain' pandoc_metadata = { 'nocite' : '@*' , 'csl' : args . csl , 'references' : csl_list , } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format , ) def _exit_without_pandoc (): \"\"\" Given info from get_pandoc_info, exit Python if Pandoc is not available. \"\"\" info = get_pandoc_info () for command in 'pandoc' , 'pandoc-citeproc' : if not info [ command ]: logging . critical ( f '\"{command}\" not found on system. ' f 'Check that Pandoc is installed.' ) raise SystemExit ( 1 ) def _check_pandoc_version ( info , metadata , format ): \"\"\" Given info from get_pandoc_info, check that Pandoc's version is sufficient to perform the citation rendering command specified by metadata and format. Please add additional minimum version information to this function, as its discovered. \"\"\" issues = list () if format == 'jats' and info [ 'pandoc version' ] < ( 2 ,): issues . append ( '--jats requires pandoc >= v2.0.' ) # --csl=URL did not work in https://travis-ci.org/greenelab/manubot/builds/417314743#L796, but exact version where this fails unknown # if metadata.get('csl', '').startswith('http') and pandoc_version < (2,): # issues.append('--csl=URL requires pandoc >= v2.0.') issues = ' \\n ' . join ( issues ) if issues : logging . critical ( f 'issues with pandoc version detected: \\n {issues}' ) Variables extension_to_format Functions call_pandoc def call_pandoc ( metadata , path , format = 'plain' ) path is the path to write to. View Source def call_pandoc ( metadata , path , format = ' plain ' ) : \"\"\" path is the path to write to . \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = ' --- \\n {yaml} \\n ... \\n ' . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ ' pandoc ' , ' --filter ' , ' pandoc-citeproc ' , ' --output ' , str ( path ) if path else ' - ' , ] if format == ' markdown ' : args . extend ( [ ' --to ' , ' markdown_strict ' , ' --wrap ' , ' none ' ] ) elif format == ' jats ' : args . extend ( [ ' --to ' , ' jats ' , ' --standalone ' ] ) elif format == ' docx ' : args . extend ( [ ' --to ' , ' docx ' ] ) elif format == ' html ' : args . extend ( [ ' --to ' , ' html ' ] ) elif format == ' plain ' : args . extend ( [ ' --to ' , ' plain ' , ' --wrap ' , ' none ' ] ) if info [ ' pandoc version ' ] >= ( 2 , ) : # Do not use ALL_CAPS for bold & underscores for italics # https : // github . com / jgm / pandoc / issues / 4834 # issuecomment - 412972008 filter_path = pathlib . Path ( __file__ ) . joinpath ( ' .. ' , ' plain-pandoc-filter.lua ' ) . resolve () assert filter_path . exists () args . extend ( [ ' --lua-filter ' , str ( filter_path ) ] ) logging . info ( ' call_pandoc subprocess args: \\n ' + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode () , stdout = subprocess . PIPE if path else sys . stdout , stderr = sys . stderr , ) process . check_returncode () cli_cite def cli_cite ( args ) Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citaiton rendering by output format. See https://github.com/jgm/pandoc/issues/4834 View Source def cli_cite ( args ) : \"\"\" Main function for the manubot cite command - line interface . Does not allow user to directly specify Pandoc ' s --to argument, due to inconsistent citaiton rendering by output format . See https : // github . com / jgm / pandoc / issues / 4834 \"\"\" # generate CSL JSON data csl_list = list () for citekey in args . citekeys : try : if not is_valid_citekey ( citekey ) : continue citekey = standardize_citekey ( citekey ) csl_item = citekey_to_csl_item ( citekey , prune = args . prune_csl ) csl_list . append ( csl_item ) except Exception as error : logging . error ( f ' citekey_to_csl_item for {citekey!r} failed ' f ' due to a {error.__class__.__name__}: \\n {error} ' ) logging . info ( error , exc_info = True ) # output CSL JSON data , if -- render is False if not args . render : write_file = args . output . open ( ' w ' , encoding = ' utf-8 ' ) if args . output else sys . stdout with write_file : json . dump ( csl_list , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) return # use Pandoc to render references if not args . format and args . output : vars ( args ) [ ' format ' ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args ) [ ' format ' ] = ' plain ' pandoc_metadata = { ' nocite ' : ' @* ' , ' csl ' : args . csl , ' references ' : csl_list , } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format , )","title":"Cite Command"},{"location":"reference/manubot/cite/cite_command/#module-manubotcitecite_command","text":"View Source import json import logging import pathlib import subprocess import sys from manubot.cite.citekey import ( citekey_to_csl_item , standardize_citekey , is_valid_citekey ) from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join # For manubot cite, infer --format from --output filename extensions extension_to_format = { '.txt' : 'plain' , '.md' : 'markdown' , '.docx' : 'docx' , '.html' : 'html' , '.xml' : 'jats' , } def call_pandoc ( metadata , path , format = 'plain' ): \"\"\" path is the path to write to. \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = '--- \\n {yaml} \\n ... \\n ' . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ 'pandoc' , '--filter' , 'pandoc-citeproc' , '--output' , str ( path ) if path else '-' , ] if format == 'markdown' : args . extend ([ '--to' , 'markdown_strict' , '--wrap' , 'none' ]) elif format == 'jats' : args . extend ([ '--to' , 'jats' , '--standalone' ]) elif format == 'docx' : args . extend ([ '--to' , 'docx' ]) elif format == 'html' : args . extend ([ '--to' , 'html' ]) elif format == 'plain' : args . extend ([ '--to' , 'plain' , '--wrap' , 'none' ]) if info [ 'pandoc version' ] >= ( 2 ,): # Do not use ALL_CAPS for bold & underscores for italics # https://github.com/jgm/pandoc/issues/4834#issuecomment-412972008 filter_path = pathlib . Path ( __file__ ) . joinpath ( '..' , 'plain-pandoc-filter.lua' ) . resolve () assert filter_path . exists () args . extend ([ '--lua-filter' , str ( filter_path )]) logging . info ( 'call_pandoc subprocess args: \\n ' + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode (), stdout = subprocess . PIPE if path else sys . stdout , stderr = sys . stderr , ) process . check_returncode () def cli_cite ( args ): \"\"\" Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citaiton rendering by output format. See https://github.com/jgm/pandoc/issues/4834 \"\"\" # generate CSL JSON data csl_list = list () for citekey in args . citekeys : try : if not is_valid_citekey ( citekey ): continue citekey = standardize_citekey ( citekey ) csl_item = citekey_to_csl_item ( citekey , prune = args . prune_csl ) csl_list . append ( csl_item ) except Exception as error : logging . error ( f 'citekey_to_csl_item for {citekey!r} failed ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) # output CSL JSON data, if --render is False if not args . render : write_file = args . output . open ( 'w' , encoding = 'utf-8' ) if args . output else sys . stdout with write_file : json . dump ( csl_list , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) return # use Pandoc to render references if not args . format and args . output : vars ( args )[ 'format' ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args )[ 'format' ] = 'plain' pandoc_metadata = { 'nocite' : '@*' , 'csl' : args . csl , 'references' : csl_list , } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format , ) def _exit_without_pandoc (): \"\"\" Given info from get_pandoc_info, exit Python if Pandoc is not available. \"\"\" info = get_pandoc_info () for command in 'pandoc' , 'pandoc-citeproc' : if not info [ command ]: logging . critical ( f '\"{command}\" not found on system. ' f 'Check that Pandoc is installed.' ) raise SystemExit ( 1 ) def _check_pandoc_version ( info , metadata , format ): \"\"\" Given info from get_pandoc_info, check that Pandoc's version is sufficient to perform the citation rendering command specified by metadata and format. Please add additional minimum version information to this function, as its discovered. \"\"\" issues = list () if format == 'jats' and info [ 'pandoc version' ] < ( 2 ,): issues . append ( '--jats requires pandoc >= v2.0.' ) # --csl=URL did not work in https://travis-ci.org/greenelab/manubot/builds/417314743#L796, but exact version where this fails unknown # if metadata.get('csl', '').startswith('http') and pandoc_version < (2,): # issues.append('--csl=URL requires pandoc >= v2.0.') issues = ' \\n ' . join ( issues ) if issues : logging . critical ( f 'issues with pandoc version detected: \\n {issues}' )","title":"Module manubot.cite.cite_command"},{"location":"reference/manubot/cite/cite_command/#variables","text":"extension_to_format","title":"Variables"},{"location":"reference/manubot/cite/cite_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/cite_command/#call_pandoc","text":"def call_pandoc ( metadata , path , format = 'plain' ) path is the path to write to. View Source def call_pandoc ( metadata , path , format = ' plain ' ) : \"\"\" path is the path to write to . \"\"\" _exit_without_pandoc () info = get_pandoc_info () _check_pandoc_version ( info , metadata , format ) metadata_block = ' --- \\n {yaml} \\n ... \\n ' . format ( yaml = json . dumps ( metadata , ensure_ascii = False , indent = 2 ) ) args = [ ' pandoc ' , ' --filter ' , ' pandoc-citeproc ' , ' --output ' , str ( path ) if path else ' - ' , ] if format == ' markdown ' : args . extend ( [ ' --to ' , ' markdown_strict ' , ' --wrap ' , ' none ' ] ) elif format == ' jats ' : args . extend ( [ ' --to ' , ' jats ' , ' --standalone ' ] ) elif format == ' docx ' : args . extend ( [ ' --to ' , ' docx ' ] ) elif format == ' html ' : args . extend ( [ ' --to ' , ' html ' ] ) elif format == ' plain ' : args . extend ( [ ' --to ' , ' plain ' , ' --wrap ' , ' none ' ] ) if info [ ' pandoc version ' ] >= ( 2 , ) : # Do not use ALL_CAPS for bold & underscores for italics # https : // github . com / jgm / pandoc / issues / 4834 # issuecomment - 412972008 filter_path = pathlib . Path ( __file__ ) . joinpath ( ' .. ' , ' plain-pandoc-filter.lua ' ) . resolve () assert filter_path . exists () args . extend ( [ ' --lua-filter ' , str ( filter_path ) ] ) logging . info ( ' call_pandoc subprocess args: \\n ' + shlex_join ( args )) process = subprocess . run ( args = args , input = metadata_block . encode () , stdout = subprocess . PIPE if path else sys . stdout , stderr = sys . stderr , ) process . check_returncode ()","title":"call_pandoc"},{"location":"reference/manubot/cite/cite_command/#cli_cite","text":"def cli_cite ( args ) Main function for the manubot cite command-line interface. Does not allow user to directly specify Pandoc's --to argument, due to inconsistent citaiton rendering by output format. See https://github.com/jgm/pandoc/issues/4834 View Source def cli_cite ( args ) : \"\"\" Main function for the manubot cite command - line interface . Does not allow user to directly specify Pandoc ' s --to argument, due to inconsistent citaiton rendering by output format . See https : // github . com / jgm / pandoc / issues / 4834 \"\"\" # generate CSL JSON data csl_list = list () for citekey in args . citekeys : try : if not is_valid_citekey ( citekey ) : continue citekey = standardize_citekey ( citekey ) csl_item = citekey_to_csl_item ( citekey , prune = args . prune_csl ) csl_list . append ( csl_item ) except Exception as error : logging . error ( f ' citekey_to_csl_item for {citekey!r} failed ' f ' due to a {error.__class__.__name__}: \\n {error} ' ) logging . info ( error , exc_info = True ) # output CSL JSON data , if -- render is False if not args . render : write_file = args . output . open ( ' w ' , encoding = ' utf-8 ' ) if args . output else sys . stdout with write_file : json . dump ( csl_list , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) return # use Pandoc to render references if not args . format and args . output : vars ( args ) [ ' format ' ] = extension_to_format . get ( args . output . suffix ) if not args . format : vars ( args ) [ ' format ' ] = ' plain ' pandoc_metadata = { ' nocite ' : ' @* ' , ' csl ' : args . csl , ' references ' : csl_list , } call_pandoc ( metadata = pandoc_metadata , path = args . output , format = args . format , )","title":"cli_cite"},{"location":"reference/manubot/cite/citekey/","text":"Module manubot.cite.citekey Functions importable from manubot.cite submodule (submodule API): standardize_citekey() citekey_to_csl_item() Helpers: inspect_citekey() is_valid_citekey() - also used in manubot.process shorten_citekey() - used solely in manubot.process infer_citekey_prefix() View Source \"\"\"Functions importable from manubot.cite submodule (submodule API): standardize_citekey() citekey_to_csl_item() Helpers: inspect_citekey() is_valid_citekey() - also used in manubot.process shorten_citekey() - used solely in manubot.process infer_citekey_prefix() \"\"\" import functools import logging import re from manubot.util import import_function citeproc_retrievers = { 'doi' : 'manubot.cite.doi.get_doi_csl_item' , 'pmid' : 'manubot.cite.pubmed.get_pubmed_csl_item' , 'pmcid' : 'manubot.cite.pubmed.get_pmc_csl_item' , 'arxiv' : 'manubot.cite.arxiv.get_arxiv_csl_item' , 'isbn' : 'manubot.cite.isbn.get_isbn_csl_item' , 'wikidata' : 'manubot.cite.wikidata.get_wikidata_csl_item' , 'url' : 'manubot.cite.url.get_url_csl_item' , } \"\"\" Regex to extract citation keys. The leading '@' is omitted from the single match group. Same rules as pandoc, except more permissive in the following ways: 1. the final character can be a slash because many URLs end in a slash. 2. underscores are allowed in internal characters because URLs, DOIs, and citation tags often contain underscores. If a citekey does not match this regex, it can be substituted for a tag that does, as defined in citation-tags.tsv. https://github.com/greenelab/manubot-rootstock/issues/2#issuecomment-312153192 Prototyped at https://regex101.com/r/s3Asz3/4 \"\"\" citekey_pattern = re . compile ( r '(?<!\\w)@([a-zA-Z0-9][\\w:.#$%&\\-+?<>~/]*[a-zA-Z0-9/])' ) @functools.lru_cache ( maxsize = 5 _000 ) def standardize_citekey ( citekey , warn_if_changed = False ): \"\"\" Standardize citation keys based on their source \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'doi' : if identifier . startswith ( '10/' ): from manubot.cite.doi import expand_short_doi try : identifier = expand_short_doi ( identifier ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f 'Error in expand_short_doi for {identifier} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) identifier = identifier . lower () if source == 'isbn' : from isbnlib import to_isbn13 identifier = to_isbn13 ( identifier ) standard_citekey = f '{source}:{identifier}' if warn_if_changed and citekey != standard_citekey : logging . warning ( f 'standardize_citekey expected citekey to already be standardized. \\n ' f 'Instead citekey was changed from {citekey!r} to {standard_citekey!r}' ) return standard_citekey regexes = { 'pmid' : re . compile ( r '[1-9][0-9]{0,7}' ), 'pmcid' : re . compile ( r 'PMC[0-9]+' ), 'doi' : re . compile ( r '10\\.[0-9]{4,9}/\\S+' ), 'shortdoi' : re . compile ( r '10/[a-zA-Z0-9]+' ), 'wikidata' : re . compile ( r 'Q[0-9]+' ), } def inspect_citekey ( citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'pmid' : # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( 'PMC' ): return ( 'PubMed Identifiers should start with digits rather than PMC. ' f \"Should {citekey!r} switch the citation source to 'pmcid'?\" ) elif not regexes [ 'pmid' ] . fullmatch ( identifier ): return 'PubMed Identifiers should be 1-8 digits with no leading zeros.' if source == 'pmcid' : # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( 'PMC' ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not regexes [ 'pmcid' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the PMCID regex. ' 'Double check the PMCID.' ) if source == 'doi' : if identifier . startswith ( '10.' ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not regexes [ 'doi' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the DOI regex. ' 'Double check the DOI.' ) elif identifier . startswith ( '10/' ): # shortDOI, see http://shortdoi.org if not regexes [ 'shortdoi' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the shortDOI regex. ' 'Double check the shortDOI.' ) else : return ( \"DOIs must start with '10.' (or '10/' for shortDOIs).\" ) if source == 'isbn' : import isbnlib fail = isbnlib . notisbn ( identifier , level = 'strict' ) if fail : return ( f 'identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}' ) if source == 'wikidata' : # https://www.wikidata.org/wiki/Wikidata:Identifiers if not identifier . startswith ( 'Q' ): return ( \"Wikidata item IDs must start with 'Q'.\" ) elif not regexes [ 'wikidata' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the Wikidata regex. ' 'Double check the entity ID.' ) return None def is_valid_citekey ( citekey , allow_tag = False , allow_raw = False , allow_pandoc_xnos = False ): \"\"\" Return True if citekey is a properly formatted string. Return False if citekey is not a citation or is an invalid citation. In the case citekey is invalid, an error is logged. This function does not catch all invalid citekeys, but instead performs cursory checks, such as ensuring citekeys adhere to the expected formats. No calls to external resources are used by these checks, so they will not detect citekeys to non-existent identifiers unless those identifiers violate their source's syntax. allow_tag=False, allow_raw=False, and allow_pandoc_xnos=False enable allowing citekey sources that are valid for Manubot manuscripts, but likely not elsewhere. allow_tag=True enables citekey tags (e.g. tag:citation-tag). allow_raw=True enables raw citekeys (e.g. raw:manual-reference). allow_pandoc_xnos=True still returns False for pandoc-xnos references (e.g. fig:figure-id), but does not log an error. With the default of False for these arguments, valid sources are restricted to those for which manubot can retrieve metadata based only on the standalone citekey. \"\"\" if not isinstance ( citekey , str ): logging . error ( f \"citekey should be type 'str' not \" f \"{type(citekey).__name__!r}: {citekey!r}\" ) return False if citekey . startswith ( '@' ): logging . error ( f \"invalid citekey: {citekey!r} \\n starts with '@'\" ) return False try : source , identifier = citekey . split ( ':' , 1 ) except ValueError : logging . error ( f 'citekey not splittable via a single colon: {citekey}. ' 'Citekeys must be in the format of `source:identifier`.' ) return False if not source or not identifier : msg = f 'invalid citekey: {citekey!r} \\n blank source or identifier' logging . error ( msg ) return False if allow_pandoc_xnos : # Exempted non-citation sources used for pandoc-fignos, # pandoc-tablenos, and pandoc-eqnos pandoc_xnos_keys = { 'fig' , 'tbl' , 'eq' } if source in pandoc_xnos_keys : return False if source . lower () in pandoc_xnos_keys : logging . error ( f 'pandoc-xnos reference types should be all lowercase. \\n ' f 'Should {citekey!r} use {source.lower()!r} rather than \"{source!r}\"?' ) return False # Check supported source type sources = set ( citeproc_retrievers ) if allow_raw : sources . add ( 'raw' ) if allow_tag : sources . add ( 'tag' ) if source not in sources : if source . lower () in sources : logging . error ( f 'citekey sources should be all lowercase. \\n ' f 'Should {citekey} use \"{source.lower()}\" rather than \"{source}\"?' ) else : logging . error ( f 'invalid citekey: {citekey!r} \\n ' f 'Source {source!r} is not valid. \\n ' f 'Valid citation sources are {{{\", \".join(sorted(sources))}}}' ) return False inspection = inspect_citekey ( citekey ) if inspection : logging . error ( f 'invalid {source} citekey: {citekey} \\n {inspection}' ) return False return True def shorten_citekey ( standard_citekey ): \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( '@' ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey def citekey_to_csl_item ( citekey , prune = True ): \"\"\" Generate a CSL Item (Python dictionary) for the input citekey. \"\"\" citekey == standardize_citekey ( citekey , warn_if_changed = True ) source , identifier = citekey . split ( ':' , 1 ) if source in citeproc_retrievers : citeproc_retriever = import_function ( citeproc_retrievers [ source ]) csl_item = citeproc_retriever ( identifier ) else : msg = f 'Unsupported citation source {source!r} in {citekey!r}' raise ValueError ( msg ) from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( csl_item_passthrough , append_to_csl_item_note , ) note_text = f 'This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.' note_dict = { 'standard_id' : citekey , } append_to_csl_item_note ( csl_item , note_text , note_dict ) short_citekey = shorten_citekey ( citekey ) csl_item = csl_item_passthrough ( csl_item , set_id = short_citekey , prune = prune ) return csl_item def infer_citekey_prefix ( citekey ): \"\"\" Passthrough citekey if it has a valid citation key prefix. Otherwise, if the lowercase citekey prefix is valid, convert the prefix to lowercase. Otherwise, assume citekey is raw and prepend \"raw:\". \"\"\" prefixes = [ f '{x}:' for x in list ( citeproc_retrievers ) + [ 'raw' ]] for prefix in prefixes : if citekey . startswith ( prefix ): return citekey if citekey . lower () . startswith ( prefix ): return prefix + citekey [ len ( prefix ):] return f 'raw:{citekey}' Variables citekey_pattern citeproc_retrievers Regex to extract citation keys. The leading '@' is omitted from the single match group. Same rules as pandoc, except more permissive in the following ways: the final character can be a slash because many URLs end in a slash. underscores are allowed in internal characters because URLs, DOIs, and citation tags often contain underscores. If a citekey does not match this regex, it can be substituted for a tag that does, as defined in citation-tags.tsv. https://github.com/greenelab/manubot-rootstock/issues/2#issuecomment-312153192 Prototyped at https://regex101.com/r/s3Asz3/4 regexes Functions citekey_to_csl_item def citekey_to_csl_item ( citekey , prune = True ) Generate a CSL Item (Python dictionary) for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True ): \"\"\" Generate a CSL Item (Python dictionary) for the input citekey. \"\"\" citekey == standardize_citekey ( citekey , warn_if_changed = True ) source , identifier = citekey . split ( ':' , 1 ) if source in citeproc_retrievers : citeproc_retriever = import_function ( citeproc_retrievers [ source ]) csl_item = citeproc_retriever ( identifier ) else : msg = f 'Unsupported citation source {source!r} in {citekey!r}' raise ValueError ( msg ) from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( csl_item_passthrough , append_to_csl_item_note , ) note_text = f 'This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.' note_dict = { 'standard_id' : citekey , } append_to_csl_item_note ( csl_item , note_text , note_dict ) short_citekey = shorten_citekey ( citekey ) csl_item = csl_item_passthrough ( csl_item , set_id = short_citekey , prune = prune ) return csl_item infer_citekey_prefix def infer_citekey_prefix ( citekey ) Passthrough citekey if it has a valid citation key prefix. Otherwise, if the lowercase citekey prefix is valid, convert the prefix to lowercase. Otherwise, assume citekey is raw and prepend \"raw:\". View Source def infer_citekey_prefix ( citekey ) : \"\"\" Passthrough citekey if it has a valid citation key prefix . Otherwise , if the lowercase citekey prefix is valid , convert the prefix to lowercase . Otherwise , assume citekey is raw and prepend \" raw: \" . \"\"\" prefixes = [ f ' {x}: ' for x in list ( citeproc_retrievers ) + [ ' raw ' ]] for prefix in prefixes : if citekey . startswith ( prefix ) : return citekey if citekey . lower () . startswith ( prefix ) : return prefix + citekey [ len ( prefix ) :] return f ' raw:{citekey} ' inspect_citekey def inspect_citekey ( citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect_citekey ( citekey ) : \"\"\" Check citekeys adhere to expected formats . If an issue is detected a string describing the issue is returned . Otherwise returns None . \"\"\" source , identifier = citekey . split ( ' : ' , 1 ) if source == ' pmid ' : # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmid if identifier . startswith ( ' PMC ' ) : return ( ' PubMed Identifiers should start with digits rather than PMC. ' f \" Should {citekey!r} switch the citation source to 'pmcid'? \" ) elif not regexes [ ' pmid ' ]. fullmatch ( identifier ) : return ' PubMed Identifiers should be 1-8 digits with no leading zeros. ' if source == ' pmcid ' : # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmc if not identifier . startswith ( ' PMC ' ) : return \" PubMed Central Identifiers must start with 'PMC'. \" elif not regexes [ ' pmcid ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the PMCID regex. ' ' Double check the PMCID. ' ) if source == ' doi ' : if identifier . startswith ( ' 10. ' ) : # https : // www . crossref . org / blog / dois - and - matching - regular - expressions / if not regexes [ ' doi ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the DOI regex. ' ' Double check the DOI. ' ) elif identifier . startswith ( ' 10/ ' ) : # shortDOI , see http : // shortdoi . org if not regexes [ ' shortdoi ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the shortDOI regex. ' ' Double check the shortDOI. ' ) else : return ( \" DOIs must start with '10.' (or '10/' for shortDOIs). \" ) if source == ' isbn ' : import isbnlib fail = isbnlib . notisbn ( identifier , level = ' strict ' ) if fail : return ( f ' identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__} ' ) if source == ' wikidata ' : # https : // www . wikidata . org / wiki / Wikidata : Identifiers if not identifier . startswith ( ' Q ' ) : return ( \" Wikidata item IDs must start with 'Q'. \" ) elif not regexes [ ' wikidata ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the Wikidata regex. ' ' Double check the entity ID. ' ) return None is_valid_citekey def is_valid_citekey ( citekey , allow_tag = False , allow_raw = False , allow_pandoc_xnos = False ) Return True if citekey is a properly formatted string. Return False if citekey is not a citation or is an invalid citation. In the case citekey is invalid, an error is logged. This function does not catch all invalid citekeys, but instead performs cursory checks, such as ensuring citekeys adhere to the expected formats. No calls to external resources are used by these checks, so they will not detect citekeys to non-existent identifiers unless those identifiers violate their source's syntax. allow_tag=False, allow_raw=False, and allow_pandoc_xnos=False enable allowing citekey sources that are valid for Manubot manuscripts, but likely not elsewhere. allow_tag=True enables citekey tags (e.g. tag:citation-tag). allow_raw=True enables raw citekeys (e.g. raw:manual-reference). allow_pandoc_xnos=True still returns False for pandoc-xnos references (e.g. fig:figure-id), but does not log an error. With the default of False for these arguments, valid sources are restricted to those for which manubot can retrieve metadata based only on the standalone citekey. View Source def is_valid_citekey ( citekey , allow_tag = False , allow_raw = False , allow_pandoc_xnos = False ) : \"\"\" Return True if citekey is a properly formatted string . Return False if citekey is not a citation or is an invalid citation . In the case citekey is invalid , an error is logged . This function does not catch all invalid citekeys , but instead performs cursory checks , such as ensuring citekeys adhere to the expected formats . No calls to external resources are used by these checks , so they will not detect citekeys to non - existent identifiers unless those identifiers violate their source ' s syntax. allow_tag = False , allow_raw = False , and allow_pandoc_xnos = False enable allowing citekey sources that are valid for Manubot manuscripts , but likely not elsewhere . allow_tag = True enables citekey tags ( e . g . tag : citation - tag ) . allow_raw = True enables raw citekeys ( e . g . raw : manual - reference ) . allow_pandoc_xnos = True still returns False for pandoc - xnos references ( e . g . fig : figure - id ) , but does not log an error . With the default of False for these arguments , valid sources are restricted to those for which manubot can retrieve metadata based only on the standalone citekey . \"\"\" if not isinstance ( citekey , str ) : logging . error ( f \" citekey should be type 'str' not \" f \" {type(citekey).__name__!r}: {citekey!r} \" ) return False if citekey . startswith ( ' @ ' ) : logging . error ( f \" invalid citekey: {citekey!r} \\n starts with '@' \" ) return False try : source , identifier = citekey . split ( ' : ' , 1 ) except ValueError : logging . error ( f ' citekey not splittable via a single colon: {citekey}. ' ' Citekeys must be in the format of `source:identifier`. ' ) return False if not source or not identifier : msg = f ' invalid citekey: {citekey!r} \\n blank source or identifier ' logging . error ( msg ) return False if allow_pandoc_xnos : # Exempted non - citation sources used for pandoc - fignos , # pandoc - tablenos , and pandoc - eqnos pandoc_xnos_keys = { ' fig ' , ' tbl ' , ' eq ' } if source in pandoc_xnos_keys : return False if source . lower () in pandoc_xnos_keys : logging . error ( f ' pandoc-xnos reference types should be all lowercase. \\n ' f ' Should {citekey!r} use {source.lower()!r} rather than \"{source!r}\"? ' ) return False # Check supported source type sources = set ( citeproc_retrievers ) if allow_raw : sources . add ( ' raw ' ) if allow_tag : sources . add ( ' tag ' ) if source not in sources : if source . lower () in sources : logging . error ( f ' citekey sources should be all lowercase. \\n ' f ' Should {citekey} use \"{source.lower()}\" rather than \"{source}\"? ' ) else : logging . error ( f ' invalid citekey: {citekey!r} \\n ' f ' Source {source!r} is not valid. \\n ' f ' Valid citation sources are {{{\", \".join(sorted(sources))}}} ' ) return False inspection = inspect_citekey ( citekey ) if inspection : logging . error ( f ' invalid {source} citekey: {citekey} \\n {inspection} ' ) return False return True shorten_citekey def shorten_citekey ( standard_citekey ) Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. View Source def shorten_citekey ( standard_citekey ): \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( '@' ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey standardize_citekey def standardize_citekey ( citekey , warn_if_changed = False ) Standardize citation keys based on their source View Source @functools.lru_cache ( maxsize = 5 _000 ) def standardize_citekey ( citekey , warn_if_changed = False ): \"\"\" Standardize citation keys based on their source \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'doi' : if identifier . startswith ( '10/' ): from manubot.cite.doi import expand_short_doi try : identifier = expand_short_doi ( identifier ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f 'Error in expand_short_doi for {identifier} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) identifier = identifier . lower () if source == 'isbn' : from isbnlib import to_isbn13 identifier = to_isbn13 ( identifier ) standard_citekey = f '{source}:{identifier}' if warn_if_changed and citekey != standard_citekey : logging . warning ( f 'standardize_citekey expected citekey to already be standardized. \\n ' f 'Instead citekey was changed from {citekey!r} to {standard_citekey!r}' ) return standard_citekey","title":"Citekey"},{"location":"reference/manubot/cite/citekey/#module-manubotcitecitekey","text":"Functions importable from manubot.cite submodule (submodule API): standardize_citekey() citekey_to_csl_item() Helpers: inspect_citekey() is_valid_citekey() - also used in manubot.process shorten_citekey() - used solely in manubot.process infer_citekey_prefix() View Source \"\"\"Functions importable from manubot.cite submodule (submodule API): standardize_citekey() citekey_to_csl_item() Helpers: inspect_citekey() is_valid_citekey() - also used in manubot.process shorten_citekey() - used solely in manubot.process infer_citekey_prefix() \"\"\" import functools import logging import re from manubot.util import import_function citeproc_retrievers = { 'doi' : 'manubot.cite.doi.get_doi_csl_item' , 'pmid' : 'manubot.cite.pubmed.get_pubmed_csl_item' , 'pmcid' : 'manubot.cite.pubmed.get_pmc_csl_item' , 'arxiv' : 'manubot.cite.arxiv.get_arxiv_csl_item' , 'isbn' : 'manubot.cite.isbn.get_isbn_csl_item' , 'wikidata' : 'manubot.cite.wikidata.get_wikidata_csl_item' , 'url' : 'manubot.cite.url.get_url_csl_item' , } \"\"\" Regex to extract citation keys. The leading '@' is omitted from the single match group. Same rules as pandoc, except more permissive in the following ways: 1. the final character can be a slash because many URLs end in a slash. 2. underscores are allowed in internal characters because URLs, DOIs, and citation tags often contain underscores. If a citekey does not match this regex, it can be substituted for a tag that does, as defined in citation-tags.tsv. https://github.com/greenelab/manubot-rootstock/issues/2#issuecomment-312153192 Prototyped at https://regex101.com/r/s3Asz3/4 \"\"\" citekey_pattern = re . compile ( r '(?<!\\w)@([a-zA-Z0-9][\\w:.#$%&\\-+?<>~/]*[a-zA-Z0-9/])' ) @functools.lru_cache ( maxsize = 5 _000 ) def standardize_citekey ( citekey , warn_if_changed = False ): \"\"\" Standardize citation keys based on their source \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'doi' : if identifier . startswith ( '10/' ): from manubot.cite.doi import expand_short_doi try : identifier = expand_short_doi ( identifier ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f 'Error in expand_short_doi for {identifier} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) identifier = identifier . lower () if source == 'isbn' : from isbnlib import to_isbn13 identifier = to_isbn13 ( identifier ) standard_citekey = f '{source}:{identifier}' if warn_if_changed and citekey != standard_citekey : logging . warning ( f 'standardize_citekey expected citekey to already be standardized. \\n ' f 'Instead citekey was changed from {citekey!r} to {standard_citekey!r}' ) return standard_citekey regexes = { 'pmid' : re . compile ( r '[1-9][0-9]{0,7}' ), 'pmcid' : re . compile ( r 'PMC[0-9]+' ), 'doi' : re . compile ( r '10\\.[0-9]{4,9}/\\S+' ), 'shortdoi' : re . compile ( r '10/[a-zA-Z0-9]+' ), 'wikidata' : re . compile ( r 'Q[0-9]+' ), } def inspect_citekey ( citekey ): \"\"\" Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'pmid' : # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmid if identifier . startswith ( 'PMC' ): return ( 'PubMed Identifiers should start with digits rather than PMC. ' f \"Should {citekey!r} switch the citation source to 'pmcid'?\" ) elif not regexes [ 'pmid' ] . fullmatch ( identifier ): return 'PubMed Identifiers should be 1-8 digits with no leading zeros.' if source == 'pmcid' : # https://www.nlm.nih.gov/bsd/mms/medlineelements.html#pmc if not identifier . startswith ( 'PMC' ): return \"PubMed Central Identifiers must start with 'PMC'.\" elif not regexes [ 'pmcid' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the PMCID regex. ' 'Double check the PMCID.' ) if source == 'doi' : if identifier . startswith ( '10.' ): # https://www.crossref.org/blog/dois-and-matching-regular-expressions/ if not regexes [ 'doi' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the DOI regex. ' 'Double check the DOI.' ) elif identifier . startswith ( '10/' ): # shortDOI, see http://shortdoi.org if not regexes [ 'shortdoi' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the shortDOI regex. ' 'Double check the shortDOI.' ) else : return ( \"DOIs must start with '10.' (or '10/' for shortDOIs).\" ) if source == 'isbn' : import isbnlib fail = isbnlib . notisbn ( identifier , level = 'strict' ) if fail : return ( f 'identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__}' ) if source == 'wikidata' : # https://www.wikidata.org/wiki/Wikidata:Identifiers if not identifier . startswith ( 'Q' ): return ( \"Wikidata item IDs must start with 'Q'.\" ) elif not regexes [ 'wikidata' ] . fullmatch ( identifier ): return ( 'Identifier does not conform to the Wikidata regex. ' 'Double check the entity ID.' ) return None def is_valid_citekey ( citekey , allow_tag = False , allow_raw = False , allow_pandoc_xnos = False ): \"\"\" Return True if citekey is a properly formatted string. Return False if citekey is not a citation or is an invalid citation. In the case citekey is invalid, an error is logged. This function does not catch all invalid citekeys, but instead performs cursory checks, such as ensuring citekeys adhere to the expected formats. No calls to external resources are used by these checks, so they will not detect citekeys to non-existent identifiers unless those identifiers violate their source's syntax. allow_tag=False, allow_raw=False, and allow_pandoc_xnos=False enable allowing citekey sources that are valid for Manubot manuscripts, but likely not elsewhere. allow_tag=True enables citekey tags (e.g. tag:citation-tag). allow_raw=True enables raw citekeys (e.g. raw:manual-reference). allow_pandoc_xnos=True still returns False for pandoc-xnos references (e.g. fig:figure-id), but does not log an error. With the default of False for these arguments, valid sources are restricted to those for which manubot can retrieve metadata based only on the standalone citekey. \"\"\" if not isinstance ( citekey , str ): logging . error ( f \"citekey should be type 'str' not \" f \"{type(citekey).__name__!r}: {citekey!r}\" ) return False if citekey . startswith ( '@' ): logging . error ( f \"invalid citekey: {citekey!r} \\n starts with '@'\" ) return False try : source , identifier = citekey . split ( ':' , 1 ) except ValueError : logging . error ( f 'citekey not splittable via a single colon: {citekey}. ' 'Citekeys must be in the format of `source:identifier`.' ) return False if not source or not identifier : msg = f 'invalid citekey: {citekey!r} \\n blank source or identifier' logging . error ( msg ) return False if allow_pandoc_xnos : # Exempted non-citation sources used for pandoc-fignos, # pandoc-tablenos, and pandoc-eqnos pandoc_xnos_keys = { 'fig' , 'tbl' , 'eq' } if source in pandoc_xnos_keys : return False if source . lower () in pandoc_xnos_keys : logging . error ( f 'pandoc-xnos reference types should be all lowercase. \\n ' f 'Should {citekey!r} use {source.lower()!r} rather than \"{source!r}\"?' ) return False # Check supported source type sources = set ( citeproc_retrievers ) if allow_raw : sources . add ( 'raw' ) if allow_tag : sources . add ( 'tag' ) if source not in sources : if source . lower () in sources : logging . error ( f 'citekey sources should be all lowercase. \\n ' f 'Should {citekey} use \"{source.lower()}\" rather than \"{source}\"?' ) else : logging . error ( f 'invalid citekey: {citekey!r} \\n ' f 'Source {source!r} is not valid. \\n ' f 'Valid citation sources are {{{\", \".join(sorted(sources))}}}' ) return False inspection = inspect_citekey ( citekey ) if inspection : logging . error ( f 'invalid {source} citekey: {citekey} \\n {inspection}' ) return False return True def shorten_citekey ( standard_citekey ): \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( '@' ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey def citekey_to_csl_item ( citekey , prune = True ): \"\"\" Generate a CSL Item (Python dictionary) for the input citekey. \"\"\" citekey == standardize_citekey ( citekey , warn_if_changed = True ) source , identifier = citekey . split ( ':' , 1 ) if source in citeproc_retrievers : citeproc_retriever = import_function ( citeproc_retrievers [ source ]) csl_item = citeproc_retriever ( identifier ) else : msg = f 'Unsupported citation source {source!r} in {citekey!r}' raise ValueError ( msg ) from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( csl_item_passthrough , append_to_csl_item_note , ) note_text = f 'This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.' note_dict = { 'standard_id' : citekey , } append_to_csl_item_note ( csl_item , note_text , note_dict ) short_citekey = shorten_citekey ( citekey ) csl_item = csl_item_passthrough ( csl_item , set_id = short_citekey , prune = prune ) return csl_item def infer_citekey_prefix ( citekey ): \"\"\" Passthrough citekey if it has a valid citation key prefix. Otherwise, if the lowercase citekey prefix is valid, convert the prefix to lowercase. Otherwise, assume citekey is raw and prepend \"raw:\". \"\"\" prefixes = [ f '{x}:' for x in list ( citeproc_retrievers ) + [ 'raw' ]] for prefix in prefixes : if citekey . startswith ( prefix ): return citekey if citekey . lower () . startswith ( prefix ): return prefix + citekey [ len ( prefix ):] return f 'raw:{citekey}'","title":"Module manubot.cite.citekey"},{"location":"reference/manubot/cite/citekey/#variables","text":"citekey_pattern citeproc_retrievers Regex to extract citation keys. The leading '@' is omitted from the single match group. Same rules as pandoc, except more permissive in the following ways: the final character can be a slash because many URLs end in a slash. underscores are allowed in internal characters because URLs, DOIs, and citation tags often contain underscores. If a citekey does not match this regex, it can be substituted for a tag that does, as defined in citation-tags.tsv. https://github.com/greenelab/manubot-rootstock/issues/2#issuecomment-312153192 Prototyped at https://regex101.com/r/s3Asz3/4 regexes","title":"Variables"},{"location":"reference/manubot/cite/citekey/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/citekey/#citekey_to_csl_item","text":"def citekey_to_csl_item ( citekey , prune = True ) Generate a CSL Item (Python dictionary) for the input citekey. View Source def citekey_to_csl_item ( citekey , prune = True ): \"\"\" Generate a CSL Item (Python dictionary) for the input citekey. \"\"\" citekey == standardize_citekey ( citekey , warn_if_changed = True ) source , identifier = citekey . split ( ':' , 1 ) if source in citeproc_retrievers : citeproc_retriever = import_function ( citeproc_retrievers [ source ]) csl_item = citeproc_retriever ( identifier ) else : msg = f 'Unsupported citation source {source!r} in {citekey!r}' raise ValueError ( msg ) from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( csl_item_passthrough , append_to_csl_item_note , ) note_text = f 'This CSL JSON Item was automatically generated by Manubot v{manubot_version} using citation-by-identifier.' note_dict = { 'standard_id' : citekey , } append_to_csl_item_note ( csl_item , note_text , note_dict ) short_citekey = shorten_citekey ( citekey ) csl_item = csl_item_passthrough ( csl_item , set_id = short_citekey , prune = prune ) return csl_item","title":"citekey_to_csl_item"},{"location":"reference/manubot/cite/citekey/#infer_citekey_prefix","text":"def infer_citekey_prefix ( citekey ) Passthrough citekey if it has a valid citation key prefix. Otherwise, if the lowercase citekey prefix is valid, convert the prefix to lowercase. Otherwise, assume citekey is raw and prepend \"raw:\". View Source def infer_citekey_prefix ( citekey ) : \"\"\" Passthrough citekey if it has a valid citation key prefix . Otherwise , if the lowercase citekey prefix is valid , convert the prefix to lowercase . Otherwise , assume citekey is raw and prepend \" raw: \" . \"\"\" prefixes = [ f ' {x}: ' for x in list ( citeproc_retrievers ) + [ ' raw ' ]] for prefix in prefixes : if citekey . startswith ( prefix ) : return citekey if citekey . lower () . startswith ( prefix ) : return prefix + citekey [ len ( prefix ) :] return f ' raw:{citekey} '","title":"infer_citekey_prefix"},{"location":"reference/manubot/cite/citekey/#inspect_citekey","text":"def inspect_citekey ( citekey ) Check citekeys adhere to expected formats. If an issue is detected a string describing the issue is returned. Otherwise returns None. View Source def inspect_citekey ( citekey ) : \"\"\" Check citekeys adhere to expected formats . If an issue is detected a string describing the issue is returned . Otherwise returns None . \"\"\" source , identifier = citekey . split ( ' : ' , 1 ) if source == ' pmid ' : # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmid if identifier . startswith ( ' PMC ' ) : return ( ' PubMed Identifiers should start with digits rather than PMC. ' f \" Should {citekey!r} switch the citation source to 'pmcid'? \" ) elif not regexes [ ' pmid ' ]. fullmatch ( identifier ) : return ' PubMed Identifiers should be 1-8 digits with no leading zeros. ' if source == ' pmcid ' : # https : // www . nlm . nih . gov / bsd / mms / medlineelements . html # pmc if not identifier . startswith ( ' PMC ' ) : return \" PubMed Central Identifiers must start with 'PMC'. \" elif not regexes [ ' pmcid ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the PMCID regex. ' ' Double check the PMCID. ' ) if source == ' doi ' : if identifier . startswith ( ' 10. ' ) : # https : // www . crossref . org / blog / dois - and - matching - regular - expressions / if not regexes [ ' doi ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the DOI regex. ' ' Double check the DOI. ' ) elif identifier . startswith ( ' 10/ ' ) : # shortDOI , see http : // shortdoi . org if not regexes [ ' shortdoi ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the shortDOI regex. ' ' Double check the shortDOI. ' ) else : return ( \" DOIs must start with '10.' (or '10/' for shortDOIs). \" ) if source == ' isbn ' : import isbnlib fail = isbnlib . notisbn ( identifier , level = ' strict ' ) if fail : return ( f ' identifier violates the ISBN syntax according to isbnlib v{isbnlib.__version__} ' ) if source == ' wikidata ' : # https : // www . wikidata . org / wiki / Wikidata : Identifiers if not identifier . startswith ( ' Q ' ) : return ( \" Wikidata item IDs must start with 'Q'. \" ) elif not regexes [ ' wikidata ' ]. fullmatch ( identifier ) : return ( ' Identifier does not conform to the Wikidata regex. ' ' Double check the entity ID. ' ) return None","title":"inspect_citekey"},{"location":"reference/manubot/cite/citekey/#is_valid_citekey","text":"def is_valid_citekey ( citekey , allow_tag = False , allow_raw = False , allow_pandoc_xnos = False ) Return True if citekey is a properly formatted string. Return False if citekey is not a citation or is an invalid citation. In the case citekey is invalid, an error is logged. This function does not catch all invalid citekeys, but instead performs cursory checks, such as ensuring citekeys adhere to the expected formats. No calls to external resources are used by these checks, so they will not detect citekeys to non-existent identifiers unless those identifiers violate their source's syntax. allow_tag=False, allow_raw=False, and allow_pandoc_xnos=False enable allowing citekey sources that are valid for Manubot manuscripts, but likely not elsewhere. allow_tag=True enables citekey tags (e.g. tag:citation-tag). allow_raw=True enables raw citekeys (e.g. raw:manual-reference). allow_pandoc_xnos=True still returns False for pandoc-xnos references (e.g. fig:figure-id), but does not log an error. With the default of False for these arguments, valid sources are restricted to those for which manubot can retrieve metadata based only on the standalone citekey. View Source def is_valid_citekey ( citekey , allow_tag = False , allow_raw = False , allow_pandoc_xnos = False ) : \"\"\" Return True if citekey is a properly formatted string . Return False if citekey is not a citation or is an invalid citation . In the case citekey is invalid , an error is logged . This function does not catch all invalid citekeys , but instead performs cursory checks , such as ensuring citekeys adhere to the expected formats . No calls to external resources are used by these checks , so they will not detect citekeys to non - existent identifiers unless those identifiers violate their source ' s syntax. allow_tag = False , allow_raw = False , and allow_pandoc_xnos = False enable allowing citekey sources that are valid for Manubot manuscripts , but likely not elsewhere . allow_tag = True enables citekey tags ( e . g . tag : citation - tag ) . allow_raw = True enables raw citekeys ( e . g . raw : manual - reference ) . allow_pandoc_xnos = True still returns False for pandoc - xnos references ( e . g . fig : figure - id ) , but does not log an error . With the default of False for these arguments , valid sources are restricted to those for which manubot can retrieve metadata based only on the standalone citekey . \"\"\" if not isinstance ( citekey , str ) : logging . error ( f \" citekey should be type 'str' not \" f \" {type(citekey).__name__!r}: {citekey!r} \" ) return False if citekey . startswith ( ' @ ' ) : logging . error ( f \" invalid citekey: {citekey!r} \\n starts with '@' \" ) return False try : source , identifier = citekey . split ( ' : ' , 1 ) except ValueError : logging . error ( f ' citekey not splittable via a single colon: {citekey}. ' ' Citekeys must be in the format of `source:identifier`. ' ) return False if not source or not identifier : msg = f ' invalid citekey: {citekey!r} \\n blank source or identifier ' logging . error ( msg ) return False if allow_pandoc_xnos : # Exempted non - citation sources used for pandoc - fignos , # pandoc - tablenos , and pandoc - eqnos pandoc_xnos_keys = { ' fig ' , ' tbl ' , ' eq ' } if source in pandoc_xnos_keys : return False if source . lower () in pandoc_xnos_keys : logging . error ( f ' pandoc-xnos reference types should be all lowercase. \\n ' f ' Should {citekey!r} use {source.lower()!r} rather than \"{source!r}\"? ' ) return False # Check supported source type sources = set ( citeproc_retrievers ) if allow_raw : sources . add ( ' raw ' ) if allow_tag : sources . add ( ' tag ' ) if source not in sources : if source . lower () in sources : logging . error ( f ' citekey sources should be all lowercase. \\n ' f ' Should {citekey} use \"{source.lower()}\" rather than \"{source}\"? ' ) else : logging . error ( f ' invalid citekey: {citekey!r} \\n ' f ' Source {source!r} is not valid. \\n ' f ' Valid citation sources are {{{\", \".join(sorted(sources))}}} ' ) return False inspection = inspect_citekey ( citekey ) if inspection : logging . error ( f ' invalid {source} citekey: {citekey} \\n {inspection} ' ) return False return True","title":"is_valid_citekey"},{"location":"reference/manubot/cite/citekey/#shorten_citekey","text":"def shorten_citekey ( standard_citekey ) Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. View Source def shorten_citekey ( standard_citekey ): \"\"\" Return a shortened citekey derived from the input citekey. The input citekey should be standardized prior to this function, since differences in the input citekey will result in different shortened citekeys. Short citekeys are generated by converting the input citekey to a 6 byte hash, and then converting this digest to a base62 ASCII str. Shortened citekeys consist of characters in the following ranges: 0-9, a-z and A-Z. \"\"\" import hashlib import base62 assert not standard_citekey . startswith ( '@' ) as_bytes = standard_citekey . encode () blake_hash = hashlib . blake2b ( as_bytes , digest_size = 6 ) digest = blake_hash . digest () short_citekey = base62 . encodebytes ( digest ) return short_citekey","title":"shorten_citekey"},{"location":"reference/manubot/cite/citekey/#standardize_citekey","text":"def standardize_citekey ( citekey , warn_if_changed = False ) Standardize citation keys based on their source View Source @functools.lru_cache ( maxsize = 5 _000 ) def standardize_citekey ( citekey , warn_if_changed = False ): \"\"\" Standardize citation keys based on their source \"\"\" source , identifier = citekey . split ( ':' , 1 ) if source == 'doi' : if identifier . startswith ( '10/' ): from manubot.cite.doi import expand_short_doi try : identifier = expand_short_doi ( identifier ) except Exception as error : # If DOI shortening fails, return the unshortened DOI. # DOI metadata lookup will eventually fail somewhere with # appropriate error handling, as opposed to here. logging . error ( f 'Error in expand_short_doi for {identifier} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) identifier = identifier . lower () if source == 'isbn' : from isbnlib import to_isbn13 identifier = to_isbn13 ( identifier ) standard_citekey = f '{source}:{identifier}' if warn_if_changed and citekey != standard_citekey : logging . warning ( f 'standardize_citekey expected citekey to already be standardized. \\n ' f 'Instead citekey was changed from {citekey!r} to {standard_citekey!r}' ) return standard_citekey","title":"standardize_citekey"},{"location":"reference/manubot/cite/citeproc/","text":"Module manubot.cite.citeproc View Source import copy import functools import logging import re csl_item_type_fixer = { 'journal-article' : 'article-journal' , 'book-chapter' : 'chapter' , 'posted-content' : 'manuscript' , 'proceedings-article' : 'paper-conference' , 'standard' : 'entry' , 'reference-entry' : 'entry' , } def csl_item_passthrough ( csl_item , set_id = None , prune = True ): \"\"\" Fix errors in a CSL item, according to the CSL JSON schema, and optionally change its id. http://docs.citationstyles.org/en/1.0.1/specification.html http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" if set_id is not None : csl_item [ 'id' ] = set_id logging . debug ( f \"Starting csl_item_passthrough with{'' if prune else 'out'} CSL pruning for id: {csl_item.get('id', 'id not specified')}\" ) # Correct invalid CSL item types # See https://github.com/CrossRef/rest-api-doc/issues/187 if 'type' in csl_item : csl_item [ 'type' ] = csl_item_type_fixer . get ( csl_item [ 'type' ], csl_item [ 'type' ]) if prune : # Remove fields that violate the CSL Item JSON Schema csl_item , = remove_jsonschema_errors ([ csl_item ]) # Default CSL type to entry csl_item [ 'type' ] = csl_item . get ( 'type' , 'entry' ) if prune : # Confirm that corrected CSL validates validator = get_jsonschema_csl_validator () validator . validate ([ csl_item ]) return csl_item def append_to_csl_item_note ( csl_item , text = '' , dictionary = {}): \"\"\" Add information to the note field of a CSL Item. In addition to accepting arbitrary text, the note field can be used to encode additional values not defined by the CSL JSON schema, as per https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Use dictionary to specify variable-value pairs. \"\"\" if not isinstance ( csl_item , dict ): raise ValueError ( f 'append_to_csl_item_note: csl_item must be a dict but was of type {type(csl_item)}' ) if not isinstance ( dictionary , dict ): raise ValueError ( f 'append_to_csl_item_note: dictionary must be a dict but was of type {type(dictionary)}' ) if not isinstance ( text , str ): raise ValueError ( f 'append_to_csl_item_note: text must be a str but was of type {type(text)}' ) note = str ( csl_item . get ( 'note' , '' )) if text : if note and not note . endswith ( ' \\n ' ): note += ' \\n ' note += text for key , value in dictionary . items (): if not re . fullmatch ( r '[A-Z]+|[-_a-z]+' , key ): logging . warning ( f 'append_to_csl_item_note: skipping adding \"{key}\" because it does not conform to the variable_name syntax as per https://git.io/fjTzW.' ) continue if ' \\n ' in value : logging . warning ( f 'append_to_csl_item_note: skipping adding \"{key}\" because the value contains a newline: \"{value}\"' ) continue if note and not note . endswith ( ' \\n ' ): note += ' \\n ' note += f '{key}: {value}' if note : csl_item [ 'note' ] = note return csl_item def parse_csl_item_note ( note ): \"\"\" Return the dictionary of key-value pairs encoded in a CSL JSON note. Extracts both forms (line-entry and braced-entry) of key-value pairs from \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields \"\"\" note = str ( note ) line_matches = re . findall ( r '^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$' , note , re . MULTILINE ) braced_matches = re . findall ( r '{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}' , note ) return dict ( line_matches + braced_matches ) @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonref import jsonschema url = 'https://github.com/dhimmel/schema/raw/manubot/csl-data.json' # Use jsonref to workaround https://github.com/Julian/jsonschema/issues/447 schema = jsonref . load_uri ( url , jsonschema = True ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) def remove_jsonschema_errors ( instance , recurse_depth = 5 ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 ) def _delete_elem ( instance , path , absolute_path = None , message = '' ): \"\"\" Helper function for remove_jsonschema_errors that deletes an element in the JSON-like input instance at the specified path. absolute_path is relative to the original validated instance for logging purposes. Defaults to path, if not specified. message is an optional string with additional error information to log. \"\"\" if absolute_path is None : absolute_path = path logging . debug ( ( f '{message} \\n ' if message else message ) + '_delete_elem deleting CSL element at: ' + '/' . join ( map ( str , absolute_path )) ) * head , tail = path try : del _deep_get ( instance , head )[ tail ] except KeyError : pass def _deep_get ( instance , path ): \"\"\" Descend path to return a deep element in the JSON object instance. \"\"\" for key in path : instance = instance [ key ] return instance def _remove_error ( instance , error ): \"\"\" Remove a jsonschema ValidationError from the JSON-like instance. See ValidationError documentation at http://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError \"\"\" sub_errors = error . context if sub_errors : # already_removed_additional was neccessary to workaround https://github.com/citation-style-language/schema/issues/154 already_removed_additional = False for sub_error in sub_errors : if sub_error . validator == 'additionalProperties' : if already_removed_additional : continue already_removed_additional = True sub_instance = _deep_get ( instance , error . path ) _remove_error ( sub_instance , sub_error ) elif error . validator == 'additionalProperties' : extras = set ( error . instance ) - set ( error . schema [ 'properties' ]) logging . debug ( error . message + f ' \\n Will now remove these {len(extras)} additional properties.' ) for key in extras : _delete_elem ( instance = instance , path = list ( error . path ) + [ key ], absolute_path = list ( error . absolute_path ) + [ key ] ) elif error . validator in { 'enum' , 'type' , 'minItems' , 'maxItems' }: _delete_elem ( instance , error . path , error . absolute_path , error . message ) elif error . validator == 'required' : logging . warning ( ( f '{error.message} \\n ' if error . message else error . message ) + 'requried element missing at: ' + '/' . join ( map ( str , error . absolute_path )) ) else : raise NotImplementedError ( f '{error.validator} is not yet supported' ) Variables csl_item_type_fixer Functions append_to_csl_item_note def append_to_csl_item_note ( csl_item , text = '' , dictionary = {} ) Add information to the note field of a CSL Item. In addition to accepting arbitrary text, the note field can be used to encode additional values not defined by the CSL JSON schema, as per https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Use dictionary to specify variable-value pairs. View Source def append_to_csl_item_note ( csl_item , text = '' , dictionary = {} ) : \"\"\" Add information to the note field of a CSL Item . In addition to accepting arbitrary text , the note field can be used to encode additional values not defined by the CSL JSON schema , as per https : // github . com / Juris - M / citeproc - js - docs / blob / 93 d7991d42b4a96b74b7281f38e168e365847e40 / csl - json / markup . rst # cheater - syntax - for - odd - fields Use dictionary to specify variable - value pairs . \"\"\" if not isinstance ( csl_item , dict ) : raise ValueError ( f ' append_to_csl_item_note: csl_item must be a dict but was of type {type(csl_item)} ' ) if not isinstance ( dictionary , dict ) : raise ValueError ( f ' append_to_csl_item_note: dictionary must be a dict but was of type {type(dictionary)} ' ) if not isinstance ( text , str ) : raise ValueError ( f ' append_to_csl_item_note: text must be a str but was of type {type(text)} ' ) note = str ( csl_item . get ( ' note ' , '' )) if text : if note and not note . endswith ( ' \\n ' ) : note += ' \\n ' note += text for key , value in dictionary . items () : if not re . fullmatch ( r ' [A-Z]+|[-_a-z]+ ' , key ) : logging . warning ( f ' append_to_csl_item_note: skipping adding \"{key}\" because it does not conform to the variable_name syntax as per https://git.io/fjTzW. ' ) continue if ' \\n ' in value : logging . warning ( f ' append_to_csl_item_note: skipping adding \"{key}\" because the value contains a newline: \"{value}\" ' ) continue if note and not note . endswith ( ' \\n ' ) : note += ' \\n ' note += f ' {key}: {value} ' if note : csl_item [ ' note ' ] = note return csl_item csl_item_passthrough def csl_item_passthrough ( csl_item , set_id = None , prune = True ) Fix errors in a CSL item, according to the CSL JSON schema, and optionally change its id. http://docs.citationstyles.org/en/1.0.1/specification.html http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_passthrough ( csl_item , set_id = None , prune = True ) : \"\"\" Fix errors in a CSL item , according to the CSL JSON schema , and optionally change its id . http : // docs . citationstyles . org / en / 1 . 0 . 1 / specification . html http : // citeproc - js . readthedocs . io / en / latest / csl - json / markup . html https : // github . com / citation - style - language / schema / blob / master / csl - data . json \"\"\" if set_id is not None : csl_item [ ' id ' ] = set_id logging . debug ( f \" Starting csl_item_passthrough with{'' if prune else 'out'} CSL pruning for id: {csl_item.get('id', 'id not specified')} \" ) # Correct invalid CSL item types # See https : // github . com / CrossRef / rest - api - doc / issues / 187 if ' type ' in csl_item : csl_item [ ' type ' ] = csl_item_type_fixer . get ( csl_item [ ' type ' ], csl_item [ ' type ' ] ) if prune : # Remove fields that violate the CSL Item JSON Schema csl_item , = remove_jsonschema_errors ( [ csl_item ] ) # Default CSL type to entry csl_item [ ' type ' ] = csl_item . get ( ' type ' , ' entry ' ) if prune : # Confirm that corrected CSL validates validator = get_jsonschema_csl_validator () validator . validate ( [ csl_item ] ) return csl_item get_jsonschema_csl_validator def get_jsonschema_csl_validator ( ) Return a jsonschema validator for the CSL Item JSON Schema View Source @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonref import jsonschema url = 'https://github.com/dhimmel/schema/raw/manubot/csl-data.json' # Use jsonref to workaround https://github.com/Julian/jsonschema/issues/447 schema = jsonref . load_uri ( url , jsonschema = True ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) parse_csl_item_note def parse_csl_item_note ( note ) Return the dictionary of key-value pairs encoded in a CSL JSON note. Extracts both forms (line-entry and braced-entry) of key-value pairs from \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields View Source def parse_csl_item_note ( note ) : \"\"\" Return the dictionary of key - value pairs encoded in a CSL JSON note . Extracts both forms ( line - entry and braced - entry ) of key - value pairs from \" cheater syntax \" https : // github . com / Juris - M / citeproc - js - docs / blob / 93 d7991d42b4a96b74b7281f38e168e365847e40 / csl - json / markup . rst # cheater - syntax - for - odd - fields \"\"\" note = str ( note ) line_matches = re . findall ( r ' ^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$ ' , note , re . MULTILINE ) braced_matches = re . findall ( r ' {:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *} ' , note ) return dict ( line_matches + braced_matches ) remove_jsonschema_errors def remove_jsonschema_errors ( instance , recurse_depth = 5 ) Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 View Source def remove_jsonschema_errors ( instance , recurse_depth = 5 ) : \"\"\" Remove fields in CSL Items that produce JSON Schema errors . Should errors be removed , but the JSON instance still fails to validate , recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached . Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task - specific tests to provide empirical evaluate that it works as intended . See also : https : // github . com / Julian / jsonschema / issues / 448 https : // stackoverflow . com / questions / 44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 )","title":"Citeproc"},{"location":"reference/manubot/cite/citeproc/#module-manubotciteciteproc","text":"View Source import copy import functools import logging import re csl_item_type_fixer = { 'journal-article' : 'article-journal' , 'book-chapter' : 'chapter' , 'posted-content' : 'manuscript' , 'proceedings-article' : 'paper-conference' , 'standard' : 'entry' , 'reference-entry' : 'entry' , } def csl_item_passthrough ( csl_item , set_id = None , prune = True ): \"\"\" Fix errors in a CSL item, according to the CSL JSON schema, and optionally change its id. http://docs.citationstyles.org/en/1.0.1/specification.html http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" if set_id is not None : csl_item [ 'id' ] = set_id logging . debug ( f \"Starting csl_item_passthrough with{'' if prune else 'out'} CSL pruning for id: {csl_item.get('id', 'id not specified')}\" ) # Correct invalid CSL item types # See https://github.com/CrossRef/rest-api-doc/issues/187 if 'type' in csl_item : csl_item [ 'type' ] = csl_item_type_fixer . get ( csl_item [ 'type' ], csl_item [ 'type' ]) if prune : # Remove fields that violate the CSL Item JSON Schema csl_item , = remove_jsonschema_errors ([ csl_item ]) # Default CSL type to entry csl_item [ 'type' ] = csl_item . get ( 'type' , 'entry' ) if prune : # Confirm that corrected CSL validates validator = get_jsonschema_csl_validator () validator . validate ([ csl_item ]) return csl_item def append_to_csl_item_note ( csl_item , text = '' , dictionary = {}): \"\"\" Add information to the note field of a CSL Item. In addition to accepting arbitrary text, the note field can be used to encode additional values not defined by the CSL JSON schema, as per https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Use dictionary to specify variable-value pairs. \"\"\" if not isinstance ( csl_item , dict ): raise ValueError ( f 'append_to_csl_item_note: csl_item must be a dict but was of type {type(csl_item)}' ) if not isinstance ( dictionary , dict ): raise ValueError ( f 'append_to_csl_item_note: dictionary must be a dict but was of type {type(dictionary)}' ) if not isinstance ( text , str ): raise ValueError ( f 'append_to_csl_item_note: text must be a str but was of type {type(text)}' ) note = str ( csl_item . get ( 'note' , '' )) if text : if note and not note . endswith ( ' \\n ' ): note += ' \\n ' note += text for key , value in dictionary . items (): if not re . fullmatch ( r '[A-Z]+|[-_a-z]+' , key ): logging . warning ( f 'append_to_csl_item_note: skipping adding \"{key}\" because it does not conform to the variable_name syntax as per https://git.io/fjTzW.' ) continue if ' \\n ' in value : logging . warning ( f 'append_to_csl_item_note: skipping adding \"{key}\" because the value contains a newline: \"{value}\"' ) continue if note and not note . endswith ( ' \\n ' ): note += ' \\n ' note += f '{key}: {value}' if note : csl_item [ 'note' ] = note return csl_item def parse_csl_item_note ( note ): \"\"\" Return the dictionary of key-value pairs encoded in a CSL JSON note. Extracts both forms (line-entry and braced-entry) of key-value pairs from \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields \"\"\" note = str ( note ) line_matches = re . findall ( r '^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$' , note , re . MULTILINE ) braced_matches = re . findall ( r '{:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *}' , note ) return dict ( line_matches + braced_matches ) @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonref import jsonschema url = 'https://github.com/dhimmel/schema/raw/manubot/csl-data.json' # Use jsonref to workaround https://github.com/Julian/jsonschema/issues/447 schema = jsonref . load_uri ( url , jsonschema = True ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema ) def remove_jsonschema_errors ( instance , recurse_depth = 5 ): \"\"\" Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 ) def _delete_elem ( instance , path , absolute_path = None , message = '' ): \"\"\" Helper function for remove_jsonschema_errors that deletes an element in the JSON-like input instance at the specified path. absolute_path is relative to the original validated instance for logging purposes. Defaults to path, if not specified. message is an optional string with additional error information to log. \"\"\" if absolute_path is None : absolute_path = path logging . debug ( ( f '{message} \\n ' if message else message ) + '_delete_elem deleting CSL element at: ' + '/' . join ( map ( str , absolute_path )) ) * head , tail = path try : del _deep_get ( instance , head )[ tail ] except KeyError : pass def _deep_get ( instance , path ): \"\"\" Descend path to return a deep element in the JSON object instance. \"\"\" for key in path : instance = instance [ key ] return instance def _remove_error ( instance , error ): \"\"\" Remove a jsonschema ValidationError from the JSON-like instance. See ValidationError documentation at http://python-jsonschema.readthedocs.io/en/latest/errors/#jsonschema.exceptions.ValidationError \"\"\" sub_errors = error . context if sub_errors : # already_removed_additional was neccessary to workaround https://github.com/citation-style-language/schema/issues/154 already_removed_additional = False for sub_error in sub_errors : if sub_error . validator == 'additionalProperties' : if already_removed_additional : continue already_removed_additional = True sub_instance = _deep_get ( instance , error . path ) _remove_error ( sub_instance , sub_error ) elif error . validator == 'additionalProperties' : extras = set ( error . instance ) - set ( error . schema [ 'properties' ]) logging . debug ( error . message + f ' \\n Will now remove these {len(extras)} additional properties.' ) for key in extras : _delete_elem ( instance = instance , path = list ( error . path ) + [ key ], absolute_path = list ( error . absolute_path ) + [ key ] ) elif error . validator in { 'enum' , 'type' , 'minItems' , 'maxItems' }: _delete_elem ( instance , error . path , error . absolute_path , error . message ) elif error . validator == 'required' : logging . warning ( ( f '{error.message} \\n ' if error . message else error . message ) + 'requried element missing at: ' + '/' . join ( map ( str , error . absolute_path )) ) else : raise NotImplementedError ( f '{error.validator} is not yet supported' )","title":"Module manubot.cite.citeproc"},{"location":"reference/manubot/cite/citeproc/#variables","text":"csl_item_type_fixer","title":"Variables"},{"location":"reference/manubot/cite/citeproc/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/citeproc/#append_to_csl_item_note","text":"def append_to_csl_item_note ( csl_item , text = '' , dictionary = {} ) Add information to the note field of a CSL Item. In addition to accepting arbitrary text, the note field can be used to encode additional values not defined by the CSL JSON schema, as per https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields Use dictionary to specify variable-value pairs. View Source def append_to_csl_item_note ( csl_item , text = '' , dictionary = {} ) : \"\"\" Add information to the note field of a CSL Item . In addition to accepting arbitrary text , the note field can be used to encode additional values not defined by the CSL JSON schema , as per https : // github . com / Juris - M / citeproc - js - docs / blob / 93 d7991d42b4a96b74b7281f38e168e365847e40 / csl - json / markup . rst # cheater - syntax - for - odd - fields Use dictionary to specify variable - value pairs . \"\"\" if not isinstance ( csl_item , dict ) : raise ValueError ( f ' append_to_csl_item_note: csl_item must be a dict but was of type {type(csl_item)} ' ) if not isinstance ( dictionary , dict ) : raise ValueError ( f ' append_to_csl_item_note: dictionary must be a dict but was of type {type(dictionary)} ' ) if not isinstance ( text , str ) : raise ValueError ( f ' append_to_csl_item_note: text must be a str but was of type {type(text)} ' ) note = str ( csl_item . get ( ' note ' , '' )) if text : if note and not note . endswith ( ' \\n ' ) : note += ' \\n ' note += text for key , value in dictionary . items () : if not re . fullmatch ( r ' [A-Z]+|[-_a-z]+ ' , key ) : logging . warning ( f ' append_to_csl_item_note: skipping adding \"{key}\" because it does not conform to the variable_name syntax as per https://git.io/fjTzW. ' ) continue if ' \\n ' in value : logging . warning ( f ' append_to_csl_item_note: skipping adding \"{key}\" because the value contains a newline: \"{value}\" ' ) continue if note and not note . endswith ( ' \\n ' ) : note += ' \\n ' note += f ' {key}: {value} ' if note : csl_item [ ' note ' ] = note return csl_item","title":"append_to_csl_item_note"},{"location":"reference/manubot/cite/citeproc/#csl_item_passthrough","text":"def csl_item_passthrough ( csl_item , set_id = None , prune = True ) Fix errors in a CSL item, according to the CSL JSON schema, and optionally change its id. http://docs.citationstyles.org/en/1.0.1/specification.html http://citeproc-js.readthedocs.io/en/latest/csl-json/markup.html https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_passthrough ( csl_item , set_id = None , prune = True ) : \"\"\" Fix errors in a CSL item , according to the CSL JSON schema , and optionally change its id . http : // docs . citationstyles . org / en / 1 . 0 . 1 / specification . html http : // citeproc - js . readthedocs . io / en / latest / csl - json / markup . html https : // github . com / citation - style - language / schema / blob / master / csl - data . json \"\"\" if set_id is not None : csl_item [ ' id ' ] = set_id logging . debug ( f \" Starting csl_item_passthrough with{'' if prune else 'out'} CSL pruning for id: {csl_item.get('id', 'id not specified')} \" ) # Correct invalid CSL item types # See https : // github . com / CrossRef / rest - api - doc / issues / 187 if ' type ' in csl_item : csl_item [ ' type ' ] = csl_item_type_fixer . get ( csl_item [ ' type ' ], csl_item [ ' type ' ] ) if prune : # Remove fields that violate the CSL Item JSON Schema csl_item , = remove_jsonschema_errors ( [ csl_item ] ) # Default CSL type to entry csl_item [ ' type ' ] = csl_item . get ( ' type ' , ' entry ' ) if prune : # Confirm that corrected CSL validates validator = get_jsonschema_csl_validator () validator . validate ( [ csl_item ] ) return csl_item","title":"csl_item_passthrough"},{"location":"reference/manubot/cite/citeproc/#get_jsonschema_csl_validator","text":"def get_jsonschema_csl_validator ( ) Return a jsonschema validator for the CSL Item JSON Schema View Source @functools.lru_cache () def get_jsonschema_csl_validator (): \"\"\" Return a jsonschema validator for the CSL Item JSON Schema \"\"\" import jsonref import jsonschema url = 'https://github.com/dhimmel/schema/raw/manubot/csl-data.json' # Use jsonref to workaround https://github.com/Julian/jsonschema/issues/447 schema = jsonref . load_uri ( url , jsonschema = True ) Validator = jsonschema . validators . validator_for ( schema ) Validator . check_schema ( schema ) return Validator ( schema )","title":"get_jsonschema_csl_validator"},{"location":"reference/manubot/cite/citeproc/#parse_csl_item_note","text":"def parse_csl_item_note ( note ) Return the dictionary of key-value pairs encoded in a CSL JSON note. Extracts both forms (line-entry and braced-entry) of key-value pairs from \"cheater syntax\" https://github.com/Juris-M/citeproc-js-docs/blob/93d7991d42b4a96b74b7281f38e168e365847e40/csl-json/markup.rst#cheater-syntax-for-odd-fields View Source def parse_csl_item_note ( note ) : \"\"\" Return the dictionary of key - value pairs encoded in a CSL JSON note . Extracts both forms ( line - entry and braced - entry ) of key - value pairs from \" cheater syntax \" https : // github . com / Juris - M / citeproc - js - docs / blob / 93 d7991d42b4a96b74b7281f38e168e365847e40 / csl - json / markup . rst # cheater - syntax - for - odd - fields \"\"\" note = str ( note ) line_matches = re . findall ( r ' ^(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *$ ' , note , re . MULTILINE ) braced_matches = re . findall ( r ' {:(?P<key>[A-Z]+|[-_a-z]+): *(?P<value>.+?) *} ' , note ) return dict ( line_matches + braced_matches )","title":"parse_csl_item_note"},{"location":"reference/manubot/cite/citeproc/#remove_jsonschema_errors","text":"def remove_jsonschema_errors ( instance , recurse_depth = 5 ) Remove fields in CSL Items that produce JSON Schema errors. Should errors be removed, but the JSON instance still fails to validate, recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached. Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task-specific tests to provide empirical evaluate that it works as intended. See also: https://github.com/Julian/jsonschema/issues/448 https://stackoverflow.com/questions/44694835 View Source def remove_jsonschema_errors ( instance , recurse_depth = 5 ) : \"\"\" Remove fields in CSL Items that produce JSON Schema errors . Should errors be removed , but the JSON instance still fails to validate , recursively call remove_jsonschema_errors until the instance validates or the recursion depth limit is reached . Note that this method may not be work for all types of JSON Schema errors and users looking to adapt it for other applications should write task - specific tests to provide empirical evaluate that it works as intended . See also : https : // github . com / Julian / jsonschema / issues / 448 https : // stackoverflow . com / questions / 44694835 \"\"\" validator = get_jsonschema_csl_validator () errors = list ( validator . iter_errors ( instance )) instance = copy . deepcopy ( instance ) errors = sorted ( errors , key = lambda e : e . path , reverse = True ) for error in errors : _remove_error ( instance , error ) if validator . is_valid ( instance ) or recurse_depth < 1 : return instance return remove_jsonschema_errors ( instance , recurse_depth - 1 )","title":"remove_jsonschema_errors"},{"location":"reference/manubot/cite/csl_item/","text":"Module manubot.cite.csl_item View Source from manubot.cite.citekey import standardize_citekey , infer_citekey_prefix , is_valid_citekey def csl_item_set_standard_id ( csl_item ): \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. If extracting the citation from the \"id\" field, uses the infer_citekey_prefix function to set the prefix. For example, if the extracted standard_id does not begin with a supported prefix (e.g. \"doi:\", \"pmid:\" or \"raw:\"), the citation is assumed to be raw and given a \"raw:\" prefix. The extracted citation (referred to as \"original_standard_id\") is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id, original_standard_id, and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. \"\"\" if not isinstance ( csl_item , dict ): raise ValueError ( \"csl_item must be a CSL Data Item represented as a Python dictionary\" ) from manubot.cite.citeproc import ( append_to_csl_item_note , parse_csl_item_note , ) note_dict = parse_csl_item_note ( csl_item . get ( 'note' , '' )) original_id = None original_standard_id = None if 'id' in csl_item : original_id = csl_item [ 'id' ] original_standard_id = infer_citekey_prefix ( original_id ) if 'standard_id' in note_dict : original_standard_id = note_dict [ 'standard_id' ] if 'standard_citation' in csl_item : original_standard_id = csl_item . pop ( 'standard_citation' ) if original_standard_id is None : raise ValueError ( 'csl_item_set_standard_id could not detect a field with a citation / standard_citation. ' 'Consider setting the CSL Item \"id\" field.' ) assert is_valid_citekey ( original_standard_id , allow_raw = True ) standard_id = standardize_citekey ( original_standard_id , warn_if_changed = False ) add_to_note = {} if original_id and original_id != standard_id : if original_id != note_dict . get ( 'original_id' ): add_to_note [ 'original_id' ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( 'original_standard_id' ): add_to_note [ 'original_standard_id' ] = original_standard_id if standard_id != note_dict . get ( 'standard_id' ): add_to_note [ 'standard_id' ] = standard_id append_to_csl_item_note ( csl_item , dictionary = add_to_note ) csl_item [ 'id' ] = standard_id return csl_item Functions csl_item_set_standard_id def csl_item_set_standard_id ( csl_item ) Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. If extracting the citation from the \"id\" field, uses the infer_citekey_prefix function to set the prefix. For example, if the extracted standard_id does not begin with a supported prefix (e.g. \"doi:\", \"pmid:\" or \"raw:\"), the citation is assumed to be raw and given a \"raw:\" prefix. The extracted citation (referred to as \"original_standard_id\") is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id, original_standard_id, and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def csl_item_set_standard_id ( csl_item ) : \"\"\" Extract the standard_id ( standard citation key ) for a csl_item and modify the csl_item in - place to set its \" id \" field . The standard_id is extracted from a \" standard_citation \" field , the \" note \" field , or the \" id \" field . If extracting the citation from the \" id \" field , uses the infer_citekey_prefix function to set the prefix . For example , if the extracted standard_id does not begin with a supported prefix ( e . g . \" doi: \" , \" pmid: \" or \" raw: \" ) , the citation is assumed to be raw and given a \" raw: \" prefix . The extracted citation ( referred to as \" original_standard_id \" ) is checked for validity and standardized , after which it is the final \" standard_id \" . Regarding csl_item modification , the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key - value pairs for standard_id , original_standard_id , and original_id . Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey . However , in this context , we use \" id \" rather than \" citekey \" for consistency with CSL ' s \"id\" field. \"\"\" if not isinstance ( csl_item , dict ) : raise ValueError ( \" csl_item must be a CSL Data Item represented as a Python dictionary \" ) from manubot . cite . citeproc import ( append_to_csl_item_note , parse_csl_item_note , ) note_dict = parse_csl_item_note ( csl_item . get ( ' note ' , '' )) original_id = None original_standard_id = None if ' id ' in csl_item : original_id = csl_item [ ' id ' ] original_standard_id = infer_citekey_prefix ( original_id ) if ' standard_id ' in note_dict : original_standard_id = note_dict [ ' standard_id ' ] if ' standard_citation ' in csl_item : original_standard_id = csl_item . pop ( ' standard_citation ' ) if original_standard_id is None : raise ValueError ( ' csl_item_set_standard_id could not detect a field with a citation / standard_citation. ' ' Consider setting the CSL Item \"id\" field. ' ) assert is_valid_citekey ( original_standard_id , allow_raw = True ) standard_id = standardize_citekey ( original_standard_id , warn_if_changed = False ) add_to_note = {} if original_id and original_id != standard_id : if original_id != note_dict . get ( ' original_id ' ) : add_to_note [ ' original_id ' ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( ' original_standard_id ' ) : add_to_note [ ' original_standard_id ' ] = original_standard_id if standard_id != note_dict . get ( ' standard_id ' ) : add_to_note [ ' standard_id ' ] = standard_id append_to_csl_item_note ( csl_item , dictionary = add_to_note ) csl_item [ ' id ' ] = standard_id return csl_item","title":"Csl Item"},{"location":"reference/manubot/cite/csl_item/#module-manubotcitecsl_item","text":"View Source from manubot.cite.citekey import standardize_citekey , infer_citekey_prefix , is_valid_citekey def csl_item_set_standard_id ( csl_item ): \"\"\" Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. If extracting the citation from the \"id\" field, uses the infer_citekey_prefix function to set the prefix. For example, if the extracted standard_id does not begin with a supported prefix (e.g. \"doi:\", \"pmid:\" or \"raw:\"), the citation is assumed to be raw and given a \"raw:\" prefix. The extracted citation (referred to as \"original_standard_id\") is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id, original_standard_id, and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. \"\"\" if not isinstance ( csl_item , dict ): raise ValueError ( \"csl_item must be a CSL Data Item represented as a Python dictionary\" ) from manubot.cite.citeproc import ( append_to_csl_item_note , parse_csl_item_note , ) note_dict = parse_csl_item_note ( csl_item . get ( 'note' , '' )) original_id = None original_standard_id = None if 'id' in csl_item : original_id = csl_item [ 'id' ] original_standard_id = infer_citekey_prefix ( original_id ) if 'standard_id' in note_dict : original_standard_id = note_dict [ 'standard_id' ] if 'standard_citation' in csl_item : original_standard_id = csl_item . pop ( 'standard_citation' ) if original_standard_id is None : raise ValueError ( 'csl_item_set_standard_id could not detect a field with a citation / standard_citation. ' 'Consider setting the CSL Item \"id\" field.' ) assert is_valid_citekey ( original_standard_id , allow_raw = True ) standard_id = standardize_citekey ( original_standard_id , warn_if_changed = False ) add_to_note = {} if original_id and original_id != standard_id : if original_id != note_dict . get ( 'original_id' ): add_to_note [ 'original_id' ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( 'original_standard_id' ): add_to_note [ 'original_standard_id' ] = original_standard_id if standard_id != note_dict . get ( 'standard_id' ): add_to_note [ 'standard_id' ] = standard_id append_to_csl_item_note ( csl_item , dictionary = add_to_note ) csl_item [ 'id' ] = standard_id return csl_item","title":"Module manubot.cite.csl_item"},{"location":"reference/manubot/cite/csl_item/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/csl_item/#csl_item_set_standard_id","text":"def csl_item_set_standard_id ( csl_item ) Extract the standard_id (standard citation key) for a csl_item and modify the csl_item in-place to set its \"id\" field. The standard_id is extracted from a \"standard_citation\" field, the \"note\" field, or the \"id\" field. If extracting the citation from the \"id\" field, uses the infer_citekey_prefix function to set the prefix. For example, if the extracted standard_id does not begin with a supported prefix (e.g. \"doi:\", \"pmid:\" or \"raw:\"), the citation is assumed to be raw and given a \"raw:\" prefix. The extracted citation (referred to as \"original_standard_id\") is checked for validity and standardized, after which it is the final \"standard_id\". Regarding csl_item modification, the csl_item \"id\" field is set to the standard_citation and the note field is created or updated with key-value pairs for standard_id, original_standard_id, and original_id. Note that the Manubot software generally refers to the \"id\" of a CSL Item as a citekey. However, in this context, we use \"id\" rather than \"citekey\" for consistency with CSL's \"id\" field. View Source def csl_item_set_standard_id ( csl_item ) : \"\"\" Extract the standard_id ( standard citation key ) for a csl_item and modify the csl_item in - place to set its \" id \" field . The standard_id is extracted from a \" standard_citation \" field , the \" note \" field , or the \" id \" field . If extracting the citation from the \" id \" field , uses the infer_citekey_prefix function to set the prefix . For example , if the extracted standard_id does not begin with a supported prefix ( e . g . \" doi: \" , \" pmid: \" or \" raw: \" ) , the citation is assumed to be raw and given a \" raw: \" prefix . The extracted citation ( referred to as \" original_standard_id \" ) is checked for validity and standardized , after which it is the final \" standard_id \" . Regarding csl_item modification , the csl_item \" id \" field is set to the standard_citation and the note field is created or updated with key - value pairs for standard_id , original_standard_id , and original_id . Note that the Manubot software generally refers to the \" id \" of a CSL Item as a citekey . However , in this context , we use \" id \" rather than \" citekey \" for consistency with CSL ' s \"id\" field. \"\"\" if not isinstance ( csl_item , dict ) : raise ValueError ( \" csl_item must be a CSL Data Item represented as a Python dictionary \" ) from manubot . cite . citeproc import ( append_to_csl_item_note , parse_csl_item_note , ) note_dict = parse_csl_item_note ( csl_item . get ( ' note ' , '' )) original_id = None original_standard_id = None if ' id ' in csl_item : original_id = csl_item [ ' id ' ] original_standard_id = infer_citekey_prefix ( original_id ) if ' standard_id ' in note_dict : original_standard_id = note_dict [ ' standard_id ' ] if ' standard_citation ' in csl_item : original_standard_id = csl_item . pop ( ' standard_citation ' ) if original_standard_id is None : raise ValueError ( ' csl_item_set_standard_id could not detect a field with a citation / standard_citation. ' ' Consider setting the CSL Item \"id\" field. ' ) assert is_valid_citekey ( original_standard_id , allow_raw = True ) standard_id = standardize_citekey ( original_standard_id , warn_if_changed = False ) add_to_note = {} if original_id and original_id != standard_id : if original_id != note_dict . get ( ' original_id ' ) : add_to_note [ ' original_id ' ] = original_id if original_standard_id and original_standard_id != standard_id : if original_standard_id != note_dict . get ( ' original_standard_id ' ) : add_to_note [ ' original_standard_id ' ] = original_standard_id if standard_id != note_dict . get ( ' standard_id ' ) : add_to_note [ ' standard_id ' ] = standard_id append_to_csl_item_note ( csl_item , dictionary = add_to_note ) csl_item [ ' id ' ] = standard_id return csl_item","title":"csl_item_set_standard_id"},{"location":"reference/manubot/cite/doi/","text":"Module manubot.cite.doi View Source import json import logging import urllib.request import requests from manubot.cite.pubmed import get_pubmed_ids_for_doi from manubot.util import get_manubot_user_agent def expand_short_doi ( short_doi ): \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( '10/' ): raise ValueError ( f 'shortDOIs start with `10/`, but expand_short_doi received: {short_doi}' ) url = f 'https://doi.org/api/handles/{short_doi.lower()}' params = { \"type\" : \"HS_ALIAS\" , } response = requests . get ( url , params = params ) # response documentation at https://www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( 'responseCode' ) # Handle protocol response code if response_code == 100 : raise ValueError ( f 'Handle not found. Double check short_doi: {short_doi}' ) if response_code == 200 : raise ValueError ( f 'HS_ALIAS values not found. Double check short_doi: {short_doi}' ) if response_code != 1 : raise ValueError ( f 'Error response code of {response_code} returned by {response.url}' ) values = results . get ( 'values' , []) for value in values : if value . get ( 'type' ) == 'HS_ALIAS' : doi = value [ 'data' ][ 'value' ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\"{short_doi}\") \\n ' f 'The following JSON was retrieved from {response.url}: \\n ' + json . dumps ( results , indent = 2 ) ) def get_short_doi_url ( doi ): \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = 'http://shortdoi.org/{}?format=json' . format ( quoted_doi ) headers = { 'User-Agent' : get_manubot_user_agent (), } try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ 'ShortDOI' ] short_url = 'https://doi.org/' + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f 'shortDOI lookup failed for {doi}' , exc_info = True ) return None def get_doi_csl_item ( doi ): \"\"\" Use Content Negotioation (http://citation.crosscite.org/docs.html) to retrieve the CSL Item metadata for a DOI. \"\"\" url = 'https://doi.org/' + urllib . request . quote ( doi ) header = { 'Accept' : 'application/vnd.citationstyles.csl+json' , 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : csl_item = response . json () except Exception as error : logging . error ( f 'Error fetching metadata for doi:{doi}. \\n ' f 'Invalid response from {response.url}: \\n {response.text}' ) raise error csl_item [ 'URL' ] = f 'https://doi.org/{doi}' short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ 'URL' ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f 'Error calling get_pubmed_ids_for_doi for {doi}' , exc_info = True ) return csl_item Functions expand_short_doi def expand_short_doi ( short_doi ) Convert a shortDOI to a regular DOI. View Source def expand_short_doi ( short_doi ) : \"\"\" Convert a shortDOI to a regular DOI . \"\"\" if not short_doi . startswith ( ' 10/ ' ) : raise ValueError ( f ' shortDOIs start with `10/`, but expand_short_doi received: {short_doi} ' ) url = f ' https://doi.org/api/handles/{short_doi.lower()} ' params = { \" type \" : \" HS_ALIAS \" , } response = requests . get ( url , params = params ) # response documentation at https : // www . handle . net / proxy_servlet . html results = response . json () response_code = results . get ( ' responseCode ' ) # Handle protocol response code if response_code == 100 : raise ValueError ( f ' Handle not found. Double check short_doi: {short_doi} ' ) if response_code == 200 : raise ValueError ( f ' HS_ALIAS values not found. Double check short_doi: {short_doi} ' ) if response_code != 1 : raise ValueError ( f ' Error response code of {response_code} returned by {response.url} ' ) values = results . get ( ' values ' , [] ) for value in values : if value . get ( ' type ' ) == ' HS_ALIAS ' : doi = value [ ' data ' ][ ' value ' ] return doi . lower () raise RuntimeError ( f ' HS_ALIAS value not found by expand_short_doi(\"{short_doi}\") \\n ' f ' The following JSON was retrieved from {response.url}: \\n ' + json . dumps ( results , indent = 2 ) ) get_doi_csl_item def get_doi_csl_item ( doi ) Use Content Negotioation (http://citation.crosscite.org/docs.html) to retrieve the CSL Item metadata for a DOI. View Source def get_doi_csl_item ( doi ) : \"\"\" Use Content Negotioation ( http : // citation . crosscite . org / docs . html ) to retrieve the CSL Item metadata for a DOI . \"\"\" url = ' https://doi.org/ ' + urllib . request . quote ( doi ) header = { ' Accept ' : ' application/vnd.citationstyles.csl+json ' , ' User-Agent ' : get_manubot_user_agent () , } response = requests . get ( url , headers = header ) try : csl_item = response . json () except Exception as error : logging . error ( f ' Error fetching metadata for doi:{doi}. \\n ' f ' Invalid response from {response.url}: \\n {response.text} ' ) raise error csl_item [ ' URL ' ] = f ' https://doi.org/{doi} ' short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ ' URL ' ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f ' Error calling get_pubmed_ids_for_doi for {doi} ' , exc_info = True ) return csl_item get_short_doi_url def get_short_doi_url ( doi ) Get the shortDOI URL for a DOI. View Source def get_short_doi_url ( doi ) : \"\"\" Get the shortDOI URL for a DOI . \"\"\" quoted_doi = urllib . request . quote ( doi ) url = ' http://shortdoi.org/{}?format=json ' . format ( quoted_doi ) headers = { ' User-Agent ' : get_manubot_user_agent () , } try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ ' ShortDOI ' ] short_url = ' https://doi.org/ ' + short_doi [ 3 :] # Remove \" 10/ \" prefix return short_url except Exception : logging . warning ( f ' shortDOI lookup failed for {doi} ' , exc_info = True ) return None","title":"Doi"},{"location":"reference/manubot/cite/doi/#module-manubotcitedoi","text":"View Source import json import logging import urllib.request import requests from manubot.cite.pubmed import get_pubmed_ids_for_doi from manubot.util import get_manubot_user_agent def expand_short_doi ( short_doi ): \"\"\" Convert a shortDOI to a regular DOI. \"\"\" if not short_doi . startswith ( '10/' ): raise ValueError ( f 'shortDOIs start with `10/`, but expand_short_doi received: {short_doi}' ) url = f 'https://doi.org/api/handles/{short_doi.lower()}' params = { \"type\" : \"HS_ALIAS\" , } response = requests . get ( url , params = params ) # response documentation at https://www.handle.net/proxy_servlet.html results = response . json () response_code = results . get ( 'responseCode' ) # Handle protocol response code if response_code == 100 : raise ValueError ( f 'Handle not found. Double check short_doi: {short_doi}' ) if response_code == 200 : raise ValueError ( f 'HS_ALIAS values not found. Double check short_doi: {short_doi}' ) if response_code != 1 : raise ValueError ( f 'Error response code of {response_code} returned by {response.url}' ) values = results . get ( 'values' , []) for value in values : if value . get ( 'type' ) == 'HS_ALIAS' : doi = value [ 'data' ][ 'value' ] return doi . lower () raise RuntimeError ( f 'HS_ALIAS value not found by expand_short_doi(\"{short_doi}\") \\n ' f 'The following JSON was retrieved from {response.url}: \\n ' + json . dumps ( results , indent = 2 ) ) def get_short_doi_url ( doi ): \"\"\" Get the shortDOI URL for a DOI. \"\"\" quoted_doi = urllib . request . quote ( doi ) url = 'http://shortdoi.org/{}?format=json' . format ( quoted_doi ) headers = { 'User-Agent' : get_manubot_user_agent (), } try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ 'ShortDOI' ] short_url = 'https://doi.org/' + short_doi [ 3 :] # Remove \"10/\" prefix return short_url except Exception : logging . warning ( f 'shortDOI lookup failed for {doi}' , exc_info = True ) return None def get_doi_csl_item ( doi ): \"\"\" Use Content Negotioation (http://citation.crosscite.org/docs.html) to retrieve the CSL Item metadata for a DOI. \"\"\" url = 'https://doi.org/' + urllib . request . quote ( doi ) header = { 'Accept' : 'application/vnd.citationstyles.csl+json' , 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( url , headers = header ) try : csl_item = response . json () except Exception as error : logging . error ( f 'Error fetching metadata for doi:{doi}. \\n ' f 'Invalid response from {response.url}: \\n {response.text}' ) raise error csl_item [ 'URL' ] = f 'https://doi.org/{doi}' short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ 'URL' ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f 'Error calling get_pubmed_ids_for_doi for {doi}' , exc_info = True ) return csl_item","title":"Module manubot.cite.doi"},{"location":"reference/manubot/cite/doi/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/doi/#expand_short_doi","text":"def expand_short_doi ( short_doi ) Convert a shortDOI to a regular DOI. View Source def expand_short_doi ( short_doi ) : \"\"\" Convert a shortDOI to a regular DOI . \"\"\" if not short_doi . startswith ( ' 10/ ' ) : raise ValueError ( f ' shortDOIs start with `10/`, but expand_short_doi received: {short_doi} ' ) url = f ' https://doi.org/api/handles/{short_doi.lower()} ' params = { \" type \" : \" HS_ALIAS \" , } response = requests . get ( url , params = params ) # response documentation at https : // www . handle . net / proxy_servlet . html results = response . json () response_code = results . get ( ' responseCode ' ) # Handle protocol response code if response_code == 100 : raise ValueError ( f ' Handle not found. Double check short_doi: {short_doi} ' ) if response_code == 200 : raise ValueError ( f ' HS_ALIAS values not found. Double check short_doi: {short_doi} ' ) if response_code != 1 : raise ValueError ( f ' Error response code of {response_code} returned by {response.url} ' ) values = results . get ( ' values ' , [] ) for value in values : if value . get ( ' type ' ) == ' HS_ALIAS ' : doi = value [ ' data ' ][ ' value ' ] return doi . lower () raise RuntimeError ( f ' HS_ALIAS value not found by expand_short_doi(\"{short_doi}\") \\n ' f ' The following JSON was retrieved from {response.url}: \\n ' + json . dumps ( results , indent = 2 ) )","title":"expand_short_doi"},{"location":"reference/manubot/cite/doi/#get_doi_csl_item","text":"def get_doi_csl_item ( doi ) Use Content Negotioation (http://citation.crosscite.org/docs.html) to retrieve the CSL Item metadata for a DOI. View Source def get_doi_csl_item ( doi ) : \"\"\" Use Content Negotioation ( http : // citation . crosscite . org / docs . html ) to retrieve the CSL Item metadata for a DOI . \"\"\" url = ' https://doi.org/ ' + urllib . request . quote ( doi ) header = { ' Accept ' : ' application/vnd.citationstyles.csl+json ' , ' User-Agent ' : get_manubot_user_agent () , } response = requests . get ( url , headers = header ) try : csl_item = response . json () except Exception as error : logging . error ( f ' Error fetching metadata for doi:{doi}. \\n ' f ' Invalid response from {response.url}: \\n {response.text} ' ) raise error csl_item [ ' URL ' ] = f ' https://doi.org/{doi} ' short_doi_url = get_short_doi_url ( doi ) if short_doi_url : csl_item [ ' URL ' ] = short_doi_url try : csl_item . update ( get_pubmed_ids_for_doi ( doi )) except Exception : logging . warning ( f ' Error calling get_pubmed_ids_for_doi for {doi} ' , exc_info = True ) return csl_item","title":"get_doi_csl_item"},{"location":"reference/manubot/cite/doi/#get_short_doi_url","text":"def get_short_doi_url ( doi ) Get the shortDOI URL for a DOI. View Source def get_short_doi_url ( doi ) : \"\"\" Get the shortDOI URL for a DOI . \"\"\" quoted_doi = urllib . request . quote ( doi ) url = ' http://shortdoi.org/{}?format=json ' . format ( quoted_doi ) headers = { ' User-Agent ' : get_manubot_user_agent () , } try : response = requests . get ( url , headers = headers ) . json () short_doi = response [ ' ShortDOI ' ] short_url = ' https://doi.org/ ' + short_doi [ 3 :] # Remove \" 10/ \" prefix return short_url except Exception : logging . warning ( f ' shortDOI lookup failed for {doi} ' , exc_info = True ) return None","title":"get_short_doi_url"},{"location":"reference/manubot/cite/isbn/","text":"Module manubot.cite.isbn View Source import collections import json import logging import re def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f 'Error in {retriever.__name__} for {isbn} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) raise Exception ( f 'all get_isbn_csl_item methods failed for {isbn}' ) def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import export_as_csl , search_query zotero_data = search_query ( f 'isbn:{isbn}' ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'User-Agent' : get_manubot_user_agent (), } url = f 'https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}' response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ 'title' ] == 'Not found.' : raise KeyError ( f 'Metadata for ISBN {isbn} not found at {url}' ) else : raise Exception ( f 'Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n ' f '{json.dumps(result.text)}' ) mediawiki , = result csl_item = collections . OrderedDict () csl_item [ 'type' ] = mediawiki . get ( 'itemType' , 'book' ) if 'title' in mediawiki : csl_item [ 'title' ] = mediawiki [ 'title' ] if 'author' in mediawiki : csl_author = list () for last , first in mediawiki [ 'author' ]: csl_author . append ({ 'given' : first , 'family' : last , }) if csl_author : csl_item [ 'author' ] = csl_author if 'date' in mediawiki : year_pattern = re . compile ( r '[0-9]{4}' ) match = year_pattern . search ( mediawiki [ 'date' ]) if match : year = int ( match . group ()) csl_item [ 'issued' ] = { 'date-parts' : [[ year ]]} else : logging . debug ( f 'get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n ' f 'metadata retrieved from {url} \\n ' f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if 'publisher' in mediawiki : csl_item [ 'publisher' ] = mediawiki [ 'publisher' ] if 'place' in mediawiki : csl_item [ 'publisher-place' ] = mediawiki [ 'place' ] if 'volume' in mediawiki : csl_item [ 'volume' ] = mediawiki [ 'volume' ] if 'edition' in mediawiki : csl_item [ 'edition' ] = mediawiki [ 'edition' ] if 'abstractNote' in mediawiki : csl_item [ 'abstract' ] = mediawiki [ 'abstractNote' ] csl_item [ 'ISBN' ] = isbn if 'source' in mediawiki : csl_item [ 'source' ] = mediawiki [ 'source' ][ 0 ] if 'url' in mediawiki : csl_item [ 'URL' ] = mediawiki [ 'url' ] return csl_item def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn , cache = None ) csl_json = isbnlib . registry . bibformatters [ 'csl' ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data isbn_retrievers = [ get_isbn_csl_item_zotero , get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , ] Variables isbn_retrievers Functions get_isbn_csl_item def get_isbn_csl_item ( isbn ) Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable isbn_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f 'Error in {retriever.__name__} for {isbn} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) raise Exception ( f 'all get_isbn_csl_item methods failed for {isbn}' ) get_isbn_csl_item_citoid def get_isbn_csl_item_citoid ( isbn ) Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation View Source def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'User-Agent' : get_manubot_user_agent (), } url = f 'https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}' response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ 'title' ] == 'Not found.' : raise KeyError ( f 'Metadata for ISBN {isbn} not found at {url}' ) else : raise Exception ( f 'Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n ' f '{json.dumps(result.text)}' ) mediawiki , = result csl_item = collections . OrderedDict () csl_item [ 'type' ] = mediawiki . get ( 'itemType' , 'book' ) if 'title' in mediawiki : csl_item [ 'title' ] = mediawiki [ 'title' ] if 'author' in mediawiki : csl_author = list () for last , first in mediawiki [ 'author' ]: csl_author . append ({ 'given' : first , 'family' : last , }) if csl_author : csl_item [ 'author' ] = csl_author if 'date' in mediawiki : year_pattern = re . compile ( r '[0-9]{4}' ) match = year_pattern . search ( mediawiki [ 'date' ]) if match : year = int ( match . group ()) csl_item [ 'issued' ] = { 'date-parts' : [[ year ]]} else : logging . debug ( f 'get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n ' f 'metadata retrieved from {url} \\n ' f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if 'publisher' in mediawiki : csl_item [ 'publisher' ] = mediawiki [ 'publisher' ] if 'place' in mediawiki : csl_item [ 'publisher-place' ] = mediawiki [ 'place' ] if 'volume' in mediawiki : csl_item [ 'volume' ] = mediawiki [ 'volume' ] if 'edition' in mediawiki : csl_item [ 'edition' ] = mediawiki [ 'edition' ] if 'abstractNote' in mediawiki : csl_item [ 'abstract' ] = mediawiki [ 'abstractNote' ] csl_item [ 'ISBN' ] = isbn if 'source' in mediawiki : csl_item [ 'source' ] = mediawiki [ 'source' ][ 0 ] if 'url' in mediawiki : csl_item [ 'URL' ] = mediawiki [ 'url' ] return csl_item get_isbn_csl_item_isbnlib def get_isbn_csl_item_isbnlib ( isbn ) Generate CSL JSON Data for an ISBN using isbnlib. View Source def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn , cache = None ) csl_json = isbnlib . registry . bibformatters [ 'csl' ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data get_isbn_csl_item_zotero def get_isbn_csl_item_zotero ( isbn ) Generate CSL JSON Data for an ISBN using Zotero's translation-server. View Source def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import export_as_csl , search_query zotero_data = search_query ( f 'isbn:{isbn}' ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item","title":"Isbn"},{"location":"reference/manubot/cite/isbn/#module-manubotciteisbn","text":"View Source import collections import json import logging import re def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f 'Error in {retriever.__name__} for {isbn} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) raise Exception ( f 'all get_isbn_csl_item methods failed for {isbn}' ) def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import export_as_csl , search_query zotero_data = search_query ( f 'isbn:{isbn}' ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'User-Agent' : get_manubot_user_agent (), } url = f 'https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}' response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ 'title' ] == 'Not found.' : raise KeyError ( f 'Metadata for ISBN {isbn} not found at {url}' ) else : raise Exception ( f 'Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n ' f '{json.dumps(result.text)}' ) mediawiki , = result csl_item = collections . OrderedDict () csl_item [ 'type' ] = mediawiki . get ( 'itemType' , 'book' ) if 'title' in mediawiki : csl_item [ 'title' ] = mediawiki [ 'title' ] if 'author' in mediawiki : csl_author = list () for last , first in mediawiki [ 'author' ]: csl_author . append ({ 'given' : first , 'family' : last , }) if csl_author : csl_item [ 'author' ] = csl_author if 'date' in mediawiki : year_pattern = re . compile ( r '[0-9]{4}' ) match = year_pattern . search ( mediawiki [ 'date' ]) if match : year = int ( match . group ()) csl_item [ 'issued' ] = { 'date-parts' : [[ year ]]} else : logging . debug ( f 'get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n ' f 'metadata retrieved from {url} \\n ' f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if 'publisher' in mediawiki : csl_item [ 'publisher' ] = mediawiki [ 'publisher' ] if 'place' in mediawiki : csl_item [ 'publisher-place' ] = mediawiki [ 'place' ] if 'volume' in mediawiki : csl_item [ 'volume' ] = mediawiki [ 'volume' ] if 'edition' in mediawiki : csl_item [ 'edition' ] = mediawiki [ 'edition' ] if 'abstractNote' in mediawiki : csl_item [ 'abstract' ] = mediawiki [ 'abstractNote' ] csl_item [ 'ISBN' ] = isbn if 'source' in mediawiki : csl_item [ 'source' ] = mediawiki [ 'source' ][ 0 ] if 'url' in mediawiki : csl_item [ 'URL' ] = mediawiki [ 'url' ] return csl_item def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn , cache = None ) csl_json = isbnlib . registry . bibformatters [ 'csl' ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data isbn_retrievers = [ get_isbn_csl_item_zotero , get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , ]","title":"Module manubot.cite.isbn"},{"location":"reference/manubot/cite/isbn/#variables","text":"isbn_retrievers","title":"Variables"},{"location":"reference/manubot/cite/isbn/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item","text":"def get_isbn_csl_item ( isbn ) Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable isbn_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_isbn_csl_item ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN. Converts all ISBNs to 13-digit format. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `isbn_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" import isbnlib isbn = isbnlib . to_isbn13 ( isbn ) for retriever in isbn_retrievers : try : return retriever ( isbn ) except Exception as error : logging . warning ( f 'Error in {retriever.__name__} for {isbn} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) raise Exception ( f 'all get_isbn_csl_item methods failed for {isbn}' )","title":"get_isbn_csl_item"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_citoid","text":"def get_isbn_csl_item_citoid ( isbn ) Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation View Source def get_isbn_csl_item_citoid ( isbn ): \"\"\" Return CSL JSON Data for an ISBN using the Wikipedia Citoid API. https://en.wikipedia.org/api/rest_v1/#!/Citation/getCitation \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'User-Agent' : get_manubot_user_agent (), } url = f 'https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/{isbn}' response = requests . get ( url , headers = headers ) result = response . json () if isinstance ( result , dict ): if result [ 'title' ] == 'Not found.' : raise KeyError ( f 'Metadata for ISBN {isbn} not found at {url}' ) else : raise Exception ( f 'Unable to extract CSL from JSON metadata for ISBN {isbn}: \\n ' f '{json.dumps(result.text)}' ) mediawiki , = result csl_item = collections . OrderedDict () csl_item [ 'type' ] = mediawiki . get ( 'itemType' , 'book' ) if 'title' in mediawiki : csl_item [ 'title' ] = mediawiki [ 'title' ] if 'author' in mediawiki : csl_author = list () for last , first in mediawiki [ 'author' ]: csl_author . append ({ 'given' : first , 'family' : last , }) if csl_author : csl_item [ 'author' ] = csl_author if 'date' in mediawiki : year_pattern = re . compile ( r '[0-9]{4}' ) match = year_pattern . search ( mediawiki [ 'date' ]) if match : year = int ( match . group ()) csl_item [ 'issued' ] = { 'date-parts' : [[ year ]]} else : logging . debug ( f 'get_isbn_csl_item_citoid: issue extracting date for ISBN {isbn} \\n ' f 'metadata retrieved from {url} \\n ' f 'unable to extract year from date field: {mediawiki[\"date\"]}' ) if 'publisher' in mediawiki : csl_item [ 'publisher' ] = mediawiki [ 'publisher' ] if 'place' in mediawiki : csl_item [ 'publisher-place' ] = mediawiki [ 'place' ] if 'volume' in mediawiki : csl_item [ 'volume' ] = mediawiki [ 'volume' ] if 'edition' in mediawiki : csl_item [ 'edition' ] = mediawiki [ 'edition' ] if 'abstractNote' in mediawiki : csl_item [ 'abstract' ] = mediawiki [ 'abstractNote' ] csl_item [ 'ISBN' ] = isbn if 'source' in mediawiki : csl_item [ 'source' ] = mediawiki [ 'source' ][ 0 ] if 'url' in mediawiki : csl_item [ 'URL' ] = mediawiki [ 'url' ] return csl_item","title":"get_isbn_csl_item_citoid"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_isbnlib","text":"def get_isbn_csl_item_isbnlib ( isbn ) Generate CSL JSON Data for an ISBN using isbnlib. View Source def get_isbn_csl_item_isbnlib ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using isbnlib. \"\"\" import isbnlib metadata = isbnlib . meta ( isbn , cache = None ) csl_json = isbnlib . registry . bibformatters [ 'csl' ]( metadata ) csl_data = json . loads ( csl_json ) return csl_data","title":"get_isbn_csl_item_isbnlib"},{"location":"reference/manubot/cite/isbn/#get_isbn_csl_item_zotero","text":"def get_isbn_csl_item_zotero ( isbn ) Generate CSL JSON Data for an ISBN using Zotero's translation-server. View Source def get_isbn_csl_item_zotero ( isbn ): \"\"\" Generate CSL JSON Data for an ISBN using Zotero's translation-server. \"\"\" from manubot.cite.zotero import export_as_csl , search_query zotero_data = search_query ( f 'isbn:{isbn}' ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item","title":"get_isbn_csl_item_zotero"},{"location":"reference/manubot/cite/pubmed/","text":"Module manubot.cite.pubmed View Source import collections import functools import json import logging import xml.etree.ElementTree import requests from manubot.util import get_manubot_user_agent def get_pmc_csl_item ( pmcid ): \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( 'PMC' ) csl_item = _get_literature_citation_exporter_csl_item ( 'pmc' , pmcid [ 3 :]) if 'URL' not in csl_item : csl_item [ 'URL' ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item def _get_literature_citation_exporter_csl_item ( database , identifier ): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp \"\"\" if database not in { 'pubmed' , 'pmc' }: logging . error ( f 'Error calling _get_literature_citation_exporter_csl_item. \\n ' f 'database must be either \"pubmed\" or \"pmc\", not {database}' ) assert False if not identifier : logging . error ( f 'Error calling _get_literature_citation_exporter_csl_item. \\n ' f 'identifier cannot be blank' ) assert False params = { 'format' : 'csl' , 'id' : identifier , } headers = { 'User-Agent' : get_manubot_user_agent (), } url = f 'https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/{database}/' response = requests . get ( url , params , headers = headers ) try : csl_item = response . json () except Exception as error : logging . error ( f 'Error fetching {database} metadata for {identifier}. \\n ' f 'Invalid JSON response from {response.url}: \\n {response.text}' ) raise error assert isinstance ( csl_item , dict ) if csl_item . get ( 'status' , 'okay' ) == 'error' : logging . error ( f 'Error fetching {database} metadata for {identifier}. \\n ' f 'Literature Citation Exporter returned JSON indicating an error for {response.url} \\n ' f '{json.dumps(csl_item, indent=2)}' ) assert False return csl_item def get_pubmed_csl_item ( pmid ): \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { 'db' : 'pubmed' , 'id' : pmid , 'rettype' : 'full' , } headers = { 'User-Agent' : get_manubot_user_agent (), } url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi' with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) element_tree , = list ( element_tree ) except Exception as error : logging . error ( f 'Error fetching PubMed metadata for {pmid}. \\n ' f 'Invalid XML response from {response.url}: \\n {response.text}' ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f 'Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text}' logging . error ( msg ) raise error return csl_item def csl_item_from_pubmed_article ( article ): \"\"\" article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" csl_item = collections . OrderedDict () if not article . find ( \"MedlineCitation/Article\" ): raise NotImplementedError ( 'Unsupported PubMed record: no <Article> element' ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ 'title' ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ 'volume' ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ 'issue' ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ 'page' ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ 'container-title' ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ 'container-title-short' ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ 'ISSN' ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ 'issued' ] = { 'date-parts' : [ date_parts ]} authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = collections . OrderedDict () given = author . findtext ( 'ForeName' ) if given : author_csl [ 'given' ] = given family = author . findtext ( 'LastName' ) if family : author_csl [ 'family' ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ 'author' ] = authors_csl for id_type , key in ( 'pubmed' , 'PMID' ), ( 'pmc' , 'PMCID' ), ( 'doi' , 'DOI' ): xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == 'DOI' else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ 'abstract' ] = abstract csl_item [ 'URL' ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ 'type' ] = 'article-journal' return csl_item month_abbrev_to_int = { 'Jan' : 1 , 'Feb' : 2 , 'Mar' : 3 , 'Apr' : 4 , 'May' : 5 , 'Jun' : 6 , 'Jul' : 7 , 'Aug' : 8 , 'Sep' : 9 , 'Oct' : 10 , 'Nov' : 11 , 'Dec' : 12 , } def extract_publication_date_parts ( article ): \"\"\" Extract date published from a PubmedArticle xml element tree. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in 'Year' , 'Month' , 'Day' : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( 'Year' ) if year : date_parts . append ( int ( year )) month = date . findtext ( 'Month' ) if month : try : date_parts . append ( month_abbrev_to_int [ month ]) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( 'Day' ) if day : date_parts . append ( int ( day )) return date_parts def get_pmcid_and_pmid_for_doi ( doi ): \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( '10.' ) params = { 'ids' : doi , 'tool' : 'manubot' , } url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/' response = requests . get ( url , params ) if not response . ok : logging . warning ( f 'Status code {response.status_code} querying {response.url} \\n ' ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f 'Error fetching PMC ID conversion for {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return {} records = element_tree . findall ( 'record' ) if len ( records ) != 1 : logging . warning ( f 'Expected PubMed Central ID converter to return a single XML record for {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return {} record , = records if record . findtext ( 'status' , default = 'okay' ) == 'error' : return {} id_dict = {} for id_type in 'pmcid' , 'pmid' : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict def get_pmid_for_doi ( doi ): \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( '10.' ) params = { 'db' : 'pubmed' , 'term' : f '{doi}[DOI]' , } headers = { 'User-Agent' : get_manubot_user_agent (), } url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi' with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f 'Status code {response.status_code} querying {response.url} \\n ' ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f 'Error in ESearch XML for DOI: {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return None id_elems = element_tree . findall ( 'IdList/Id' ) if len ( id_elems ) != 1 : logging . debug ( f 'No PMIDs found for {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return None id_elem , = id_elems return id_elem . text def get_pubmed_ids_for_doi ( doi ): \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ 'PMID' ] = pmid return pubmed_ids @functools.lru_cache () def _get_eutils_rate_limiter (): \"\"\" Rate limiter to cap NCBI E-utilities queries to 3 per second as per https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ \"\"\" from ratelimiter import RateLimiter return RateLimiter ( max_calls = 2 , period = 1 ) Variables month_abbrev_to_int Functions csl_item_from_pubmed_article def csl_item_from_pubmed_article ( article ) article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_from_pubmed_article ( article ) : \"\"\" article is a PubmedArticle xml element tree https : // github . com / citation - style - language / schema / blob / master / csl - data . json \"\"\" csl_item = collections . OrderedDict () if not article . find ( \" MedlineCitation/Article \" ) : raise NotImplementedError ( ' Unsupported PubMed record: no <Article> element ' ) title = article . findtext ( \" MedlineCitation/Article/ArticleTitle \" ) if title : csl_item [ ' title ' ] = title volume = article . findtext ( \" MedlineCitation/Article/Journal/JournalIssue/Volume \" ) if volume : csl_item [ ' volume ' ] = volume issue = article . findtext ( \" MedlineCitation/Article/Journal/JournalIssue/Issue \" ) if issue : csl_item [ ' issue ' ] = issue page = article . findtext ( \" MedlineCitation/Article/Pagination/MedlinePgn \" ) if page : csl_item [ ' page ' ] = page journal = article . findtext ( \" MedlineCitation/Article/Journal/Title \" ) if journal : csl_item [ ' container-title ' ] = journal journal_short = article . findtext ( \" MedlineCitation/Article/Journal/ISOAbbreviation \" ) if journal_short : csl_item [ ' container-title-short ' ] = journal_short issn = article . findtext ( \" MedlineCitation/Article/Journal/ISSN \" ) if issn : csl_item [ ' ISSN ' ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ ' issued ' ] = { ' date-parts ' : [ date_parts ]} authors_csl = list () authors = article . findall ( \" MedlineCitation/Article/AuthorList/Author \" ) for author in authors : author_csl = collections . OrderedDict () given = author . findtext ( ' ForeName ' ) if given : author_csl [ ' given ' ] = given family = author . findtext ( ' LastName ' ) if family : author_csl [ ' family ' ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ ' author ' ] = authors_csl for id_type , key in ( ' pubmed ' , ' PMID ' ) , ( ' pmc ' , ' PMCID ' ) , ( ' doi ' , ' DOI ' ) : xpath = f \" PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}'] \" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == ' DOI ' else value abstract = article . findtext ( \" MedlineCitation/Article/Abstract/AbstractText \" ) if abstract : csl_item [ ' abstract ' ] = abstract csl_item [ ' URL ' ] = f \" https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']} \" csl_item [ ' type ' ] = ' article-journal ' return csl_item extract_publication_date_parts def extract_publication_date_parts ( article ) Extract date published from a PubmedArticle xml element tree. View Source def extract_publication_date_parts ( article ) : \"\"\" Extract date published from a PubmedArticle xml element tree . \"\"\" date_parts = [] # Electronic articles date = article . find ( \" MedlineCitation/Article/ArticleDate \" ) if date : for part in ' Year ' , ' Month ' , ' Day ' : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \" MedlineCitation/Article/Journal/JournalIssue/PubDate \" ) year = date . findtext ( ' Year ' ) if year : date_parts . append ( int ( year )) month = date . findtext ( ' Month ' ) if month : try : date_parts . append ( month_abbrev_to_int [ month ] ) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( ' Day ' ) if day : date_parts . append ( int ( day )) return date_parts get_pmc_csl_item def get_pmc_csl_item ( pmcid ) Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 View Source def get_pmc_csl_item ( pmcid ) : \"\"\" Get the CSL Item for a PubMed Central record by its PMID , PMCID , or DOI , using the NCBI Citation Exporter API . https : // api . ncbi . nlm . nih . gov / lit / ctxp https : // github . com / manubot / manubot / issues / 21 https : // twitter . com / dhimmel / status / 1061787168820092929 \"\"\" assert pmcid . startswith ( ' PMC ' ) csl_item = _get_literature_citation_exporter_csl_item ( ' pmc ' , pmcid [ 3 :] ) if ' URL ' not in csl_item : csl_item [ ' URL ' ] = f \" https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/ \" return csl_item get_pmcid_and_pmid_for_doi def get_pmcid_and_pmid_for_doi ( doi ) Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ View Source def get_pmcid_and_pmid_for_doi ( doi ) : \"\"\" Query PMC ' s ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central . https : // www . ncbi . nlm . nih . gov / pmc / tools / id - converter - api / \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( ' 10. ' ) params = { ' ids ' : doi , ' tool ' : ' manubot ' , } url = ' https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/ ' response = requests . get ( url , params ) if not response . ok : logging . warning ( f ' Status code {response.status_code} querying {response.url} \\n ' ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f ' Error fetching PMC ID conversion for {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return {} records = element_tree . findall ( ' record ' ) if len ( records ) != 1 : logging . warning ( f ' Expected PubMed Central ID converter to return a single XML record for {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return {} record , = records if record . findtext ( ' status ' , default = ' okay ' ) == ' error ' : return {} id_dict = {} for id_type in ' pmcid ' , ' pmid ' : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper () ] = id_ return id_dict get_pmid_for_doi def get_pmid_for_doi ( doi ) Query NCBI's E-utilities to retrieve the PMID for a DOI. View Source def get_pmid_for_doi ( doi ) : \"\"\" Query NCBI ' s E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( ' 10. ' ) params = { ' db ' : ' pubmed ' , ' term ' : f ' {doi}[DOI] ' , } headers = { ' User-Agent ' : get_manubot_user_agent () , } url = ' https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi ' with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f ' Status code {response.status_code} querying {response.url} \\n ' ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f ' Error in ESearch XML for DOI: {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return None id_elems = element_tree . findall ( ' IdList/Id ' ) if len ( id_elems ) != 1 : logging . debug ( f ' No PMIDs found for {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return None id_elem , = id_elems return id_elem . text get_pubmed_csl_item def get_pubmed_csl_item ( pmid ) Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 View Source def get_pubmed_csl_item ( pmid ) : \"\"\" Query NCBI E - Utilities to create CSL Items for PubMed IDs . https : // github . com / manubot / manubot / issues / 21 https : // github . com / ncbi / citation - exporter / issues / 3 # issuecomment - 355313143 \"\"\" pmid = str ( pmid ) params = { ' db ' : ' pubmed ' , ' id ' : pmid , ' rettype ' : ' full ' , } headers = { ' User-Agent ' : get_manubot_user_agent () , } url = ' https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi ' with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) element_tree , = list ( element_tree ) except Exception as error : logging . error ( f ' Error fetching PubMed metadata for {pmid}. \\n ' f ' Invalid XML response from {response.url}: \\n {response.text} ' ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f ' Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text} ' logging . error ( msg ) raise error return csl_item get_pubmed_ids_for_doi def get_pubmed_ids_for_doi ( doi ) Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. View Source def get_pubmed_ids_for_doi ( doi ) : \"\"\" Return a dictionary with PMCID and PMID , if they exist , for the specified DOI . See https : // github . com / manubot / manubot / issues / 45 . \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ ' PMID ' ] = pmid return pubmed_ids","title":"Pubmed"},{"location":"reference/manubot/cite/pubmed/#module-manubotcitepubmed","text":"View Source import collections import functools import json import logging import xml.etree.ElementTree import requests from manubot.util import get_manubot_user_agent def get_pmc_csl_item ( pmcid ): \"\"\" Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 \"\"\" assert pmcid . startswith ( 'PMC' ) csl_item = _get_literature_citation_exporter_csl_item ( 'pmc' , pmcid [ 3 :]) if 'URL' not in csl_item : csl_item [ 'URL' ] = f \"https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/\" return csl_item def _get_literature_citation_exporter_csl_item ( database , identifier ): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp \"\"\" if database not in { 'pubmed' , 'pmc' }: logging . error ( f 'Error calling _get_literature_citation_exporter_csl_item. \\n ' f 'database must be either \"pubmed\" or \"pmc\", not {database}' ) assert False if not identifier : logging . error ( f 'Error calling _get_literature_citation_exporter_csl_item. \\n ' f 'identifier cannot be blank' ) assert False params = { 'format' : 'csl' , 'id' : identifier , } headers = { 'User-Agent' : get_manubot_user_agent (), } url = f 'https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/{database}/' response = requests . get ( url , params , headers = headers ) try : csl_item = response . json () except Exception as error : logging . error ( f 'Error fetching {database} metadata for {identifier}. \\n ' f 'Invalid JSON response from {response.url}: \\n {response.text}' ) raise error assert isinstance ( csl_item , dict ) if csl_item . get ( 'status' , 'okay' ) == 'error' : logging . error ( f 'Error fetching {database} metadata for {identifier}. \\n ' f 'Literature Citation Exporter returned JSON indicating an error for {response.url} \\n ' f '{json.dumps(csl_item, indent=2)}' ) assert False return csl_item def get_pubmed_csl_item ( pmid ): \"\"\" Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 \"\"\" pmid = str ( pmid ) params = { 'db' : 'pubmed' , 'id' : pmid , 'rettype' : 'full' , } headers = { 'User-Agent' : get_manubot_user_agent (), } url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi' with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) element_tree , = list ( element_tree ) except Exception as error : logging . error ( f 'Error fetching PubMed metadata for {pmid}. \\n ' f 'Invalid XML response from {response.url}: \\n {response.text}' ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f 'Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text}' logging . error ( msg ) raise error return csl_item def csl_item_from_pubmed_article ( article ): \"\"\" article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json \"\"\" csl_item = collections . OrderedDict () if not article . find ( \"MedlineCitation/Article\" ): raise NotImplementedError ( 'Unsupported PubMed record: no <Article> element' ) title = article . findtext ( \"MedlineCitation/Article/ArticleTitle\" ) if title : csl_item [ 'title' ] = title volume = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Volume\" ) if volume : csl_item [ 'volume' ] = volume issue = article . findtext ( \"MedlineCitation/Article/Journal/JournalIssue/Issue\" ) if issue : csl_item [ 'issue' ] = issue page = article . findtext ( \"MedlineCitation/Article/Pagination/MedlinePgn\" ) if page : csl_item [ 'page' ] = page journal = article . findtext ( \"MedlineCitation/Article/Journal/Title\" ) if journal : csl_item [ 'container-title' ] = journal journal_short = article . findtext ( \"MedlineCitation/Article/Journal/ISOAbbreviation\" ) if journal_short : csl_item [ 'container-title-short' ] = journal_short issn = article . findtext ( \"MedlineCitation/Article/Journal/ISSN\" ) if issn : csl_item [ 'ISSN' ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ 'issued' ] = { 'date-parts' : [ date_parts ]} authors_csl = list () authors = article . findall ( \"MedlineCitation/Article/AuthorList/Author\" ) for author in authors : author_csl = collections . OrderedDict () given = author . findtext ( 'ForeName' ) if given : author_csl [ 'given' ] = given family = author . findtext ( 'LastName' ) if family : author_csl [ 'family' ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ 'author' ] = authors_csl for id_type , key in ( 'pubmed' , 'PMID' ), ( 'pmc' , 'PMCID' ), ( 'doi' , 'DOI' ): xpath = f \"PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}']\" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == 'DOI' else value abstract = article . findtext ( \"MedlineCitation/Article/Abstract/AbstractText\" ) if abstract : csl_item [ 'abstract' ] = abstract csl_item [ 'URL' ] = f \"https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']}\" csl_item [ 'type' ] = 'article-journal' return csl_item month_abbrev_to_int = { 'Jan' : 1 , 'Feb' : 2 , 'Mar' : 3 , 'Apr' : 4 , 'May' : 5 , 'Jun' : 6 , 'Jul' : 7 , 'Aug' : 8 , 'Sep' : 9 , 'Oct' : 10 , 'Nov' : 11 , 'Dec' : 12 , } def extract_publication_date_parts ( article ): \"\"\" Extract date published from a PubmedArticle xml element tree. \"\"\" date_parts = [] # Electronic articles date = article . find ( \"MedlineCitation/Article/ArticleDate\" ) if date : for part in 'Year' , 'Month' , 'Day' : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \"MedlineCitation/Article/Journal/JournalIssue/PubDate\" ) year = date . findtext ( 'Year' ) if year : date_parts . append ( int ( year )) month = date . findtext ( 'Month' ) if month : try : date_parts . append ( month_abbrev_to_int [ month ]) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( 'Day' ) if day : date_parts . append ( int ( day )) return date_parts def get_pmcid_and_pmid_for_doi ( doi ): \"\"\" Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( '10.' ) params = { 'ids' : doi , 'tool' : 'manubot' , } url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/' response = requests . get ( url , params ) if not response . ok : logging . warning ( f 'Status code {response.status_code} querying {response.url} \\n ' ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f 'Error fetching PMC ID conversion for {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return {} records = element_tree . findall ( 'record' ) if len ( records ) != 1 : logging . warning ( f 'Expected PubMed Central ID converter to return a single XML record for {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return {} record , = records if record . findtext ( 'status' , default = 'okay' ) == 'error' : return {} id_dict = {} for id_type in 'pmcid' , 'pmid' : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper ()] = id_ return id_dict def get_pmid_for_doi ( doi ): \"\"\" Query NCBI's E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( '10.' ) params = { 'db' : 'pubmed' , 'term' : f '{doi}[DOI]' , } headers = { 'User-Agent' : get_manubot_user_agent (), } url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi' with _get_eutils_rate_limiter (): response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f 'Status code {response.status_code} querying {response.url} \\n ' ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f 'Error in ESearch XML for DOI: {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return None id_elems = element_tree . findall ( 'IdList/Id' ) if len ( id_elems ) != 1 : logging . debug ( f 'No PMIDs found for {doi}. \\n ' f 'Response from {response.url}: \\n {response.text}' ) return None id_elem , = id_elems return id_elem . text def get_pubmed_ids_for_doi ( doi ): \"\"\" Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ 'PMID' ] = pmid return pubmed_ids @functools.lru_cache () def _get_eutils_rate_limiter (): \"\"\" Rate limiter to cap NCBI E-utilities queries to 3 per second as per https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ \"\"\" from ratelimiter import RateLimiter return RateLimiter ( max_calls = 2 , period = 1 )","title":"Module manubot.cite.pubmed"},{"location":"reference/manubot/cite/pubmed/#variables","text":"month_abbrev_to_int","title":"Variables"},{"location":"reference/manubot/cite/pubmed/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/pubmed/#csl_item_from_pubmed_article","text":"def csl_item_from_pubmed_article ( article ) article is a PubmedArticle xml element tree https://github.com/citation-style-language/schema/blob/master/csl-data.json View Source def csl_item_from_pubmed_article ( article ) : \"\"\" article is a PubmedArticle xml element tree https : // github . com / citation - style - language / schema / blob / master / csl - data . json \"\"\" csl_item = collections . OrderedDict () if not article . find ( \" MedlineCitation/Article \" ) : raise NotImplementedError ( ' Unsupported PubMed record: no <Article> element ' ) title = article . findtext ( \" MedlineCitation/Article/ArticleTitle \" ) if title : csl_item [ ' title ' ] = title volume = article . findtext ( \" MedlineCitation/Article/Journal/JournalIssue/Volume \" ) if volume : csl_item [ ' volume ' ] = volume issue = article . findtext ( \" MedlineCitation/Article/Journal/JournalIssue/Issue \" ) if issue : csl_item [ ' issue ' ] = issue page = article . findtext ( \" MedlineCitation/Article/Pagination/MedlinePgn \" ) if page : csl_item [ ' page ' ] = page journal = article . findtext ( \" MedlineCitation/Article/Journal/Title \" ) if journal : csl_item [ ' container-title ' ] = journal journal_short = article . findtext ( \" MedlineCitation/Article/Journal/ISOAbbreviation \" ) if journal_short : csl_item [ ' container-title-short ' ] = journal_short issn = article . findtext ( \" MedlineCitation/Article/Journal/ISSN \" ) if issn : csl_item [ ' ISSN ' ] = issn date_parts = extract_publication_date_parts ( article ) if date_parts : csl_item [ ' issued ' ] = { ' date-parts ' : [ date_parts ]} authors_csl = list () authors = article . findall ( \" MedlineCitation/Article/AuthorList/Author \" ) for author in authors : author_csl = collections . OrderedDict () given = author . findtext ( ' ForeName ' ) if given : author_csl [ ' given ' ] = given family = author . findtext ( ' LastName ' ) if family : author_csl [ ' family ' ] = family authors_csl . append ( author_csl ) if authors_csl : csl_item [ ' author ' ] = authors_csl for id_type , key in ( ' pubmed ' , ' PMID ' ) , ( ' pmc ' , ' PMCID ' ) , ( ' doi ' , ' DOI ' ) : xpath = f \" PubmedData/ArticleIdList/ArticleId[@IdType='{id_type}'] \" value = article . findtext ( xpath ) if value : csl_item [ key ] = value . lower () if key == ' DOI ' else value abstract = article . findtext ( \" MedlineCitation/Article/Abstract/AbstractText \" ) if abstract : csl_item [ ' abstract ' ] = abstract csl_item [ ' URL ' ] = f \" https://www.ncbi.nlm.nih.gov/pubmed/{csl_item['PMID']} \" csl_item [ ' type ' ] = ' article-journal ' return csl_item","title":"csl_item_from_pubmed_article"},{"location":"reference/manubot/cite/pubmed/#extract_publication_date_parts","text":"def extract_publication_date_parts ( article ) Extract date published from a PubmedArticle xml element tree. View Source def extract_publication_date_parts ( article ) : \"\"\" Extract date published from a PubmedArticle xml element tree . \"\"\" date_parts = [] # Electronic articles date = article . find ( \" MedlineCitation/Article/ArticleDate \" ) if date : for part in ' Year ' , ' Month ' , ' Day ' : part = date . findtext ( part ) if not part : break date_parts . append ( int ( part )) return date_parts # Print articles date = article . find ( \" MedlineCitation/Article/Journal/JournalIssue/PubDate \" ) year = date . findtext ( ' Year ' ) if year : date_parts . append ( int ( year )) month = date . findtext ( ' Month ' ) if month : try : date_parts . append ( month_abbrev_to_int [ month ] ) except KeyError : date_parts . append ( int ( month )) day = date . findtext ( ' Day ' ) if day : date_parts . append ( int ( day )) return date_parts","title":"extract_publication_date_parts"},{"location":"reference/manubot/cite/pubmed/#get_pmc_csl_item","text":"def get_pmc_csl_item ( pmcid ) Get the CSL Item for a PubMed Central record by its PMID, PMCID, or DOI, using the NCBI Citation Exporter API. https://api.ncbi.nlm.nih.gov/lit/ctxp https://github.com/manubot/manubot/issues/21 https://twitter.com/dhimmel/status/1061787168820092929 View Source def get_pmc_csl_item ( pmcid ) : \"\"\" Get the CSL Item for a PubMed Central record by its PMID , PMCID , or DOI , using the NCBI Citation Exporter API . https : // api . ncbi . nlm . nih . gov / lit / ctxp https : // github . com / manubot / manubot / issues / 21 https : // twitter . com / dhimmel / status / 1061787168820092929 \"\"\" assert pmcid . startswith ( ' PMC ' ) csl_item = _get_literature_citation_exporter_csl_item ( ' pmc ' , pmcid [ 3 :] ) if ' URL ' not in csl_item : csl_item [ ' URL ' ] = f \" https://www.ncbi.nlm.nih.gov/pmc/articles/{csl_item.get('PMCID', pmcid)}/ \" return csl_item","title":"get_pmc_csl_item"},{"location":"reference/manubot/cite/pubmed/#get_pmcid_and_pmid_for_doi","text":"def get_pmcid_and_pmid_for_doi ( doi ) Query PMC's ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central. https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/ View Source def get_pmcid_and_pmid_for_doi ( doi ) : \"\"\" Query PMC ' s ID Converter API to retrieve the PMCID and PMID for a DOI. Does not work for DOIs that are in Pubmed but not PubMed Central . https : // www . ncbi . nlm . nih . gov / pmc / tools / id - converter - api / \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( ' 10. ' ) params = { ' ids ' : doi , ' tool ' : ' manubot ' , } url = ' https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/ ' response = requests . get ( url , params ) if not response . ok : logging . warning ( f ' Status code {response.status_code} querying {response.url} \\n ' ) return {} try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f ' Error fetching PMC ID conversion for {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return {} records = element_tree . findall ( ' record ' ) if len ( records ) != 1 : logging . warning ( f ' Expected PubMed Central ID converter to return a single XML record for {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return {} record , = records if record . findtext ( ' status ' , default = ' okay ' ) == ' error ' : return {} id_dict = {} for id_type in ' pmcid ' , ' pmid ' : id_ = record . get ( id_type ) if id_ : id_dict [ id_type . upper () ] = id_ return id_dict","title":"get_pmcid_and_pmid_for_doi"},{"location":"reference/manubot/cite/pubmed/#get_pmid_for_doi","text":"def get_pmid_for_doi ( doi ) Query NCBI's E-utilities to retrieve the PMID for a DOI. View Source def get_pmid_for_doi ( doi ) : \"\"\" Query NCBI ' s E-utilities to retrieve the PMID for a DOI. \"\"\" assert isinstance ( doi , str ) assert doi . startswith ( ' 10. ' ) params = { ' db ' : ' pubmed ' , ' term ' : f ' {doi}[DOI] ' , } headers = { ' User-Agent ' : get_manubot_user_agent () , } url = ' https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi ' with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) if not response . ok : logging . warning ( f ' Status code {response.status_code} querying {response.url} \\n ' ) return None try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) except Exception : logging . warning ( f ' Error in ESearch XML for DOI: {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return None id_elems = element_tree . findall ( ' IdList/Id ' ) if len ( id_elems ) != 1 : logging . debug ( f ' No PMIDs found for {doi}. \\n ' f ' Response from {response.url}: \\n {response.text} ' ) return None id_elem , = id_elems return id_elem . text","title":"get_pmid_for_doi"},{"location":"reference/manubot/cite/pubmed/#get_pubmed_csl_item","text":"def get_pubmed_csl_item ( pmid ) Query NCBI E-Utilities to create CSL Items for PubMed IDs. https://github.com/manubot/manubot/issues/21 https://github.com/ncbi/citation-exporter/issues/3#issuecomment-355313143 View Source def get_pubmed_csl_item ( pmid ) : \"\"\" Query NCBI E - Utilities to create CSL Items for PubMed IDs . https : // github . com / manubot / manubot / issues / 21 https : // github . com / ncbi / citation - exporter / issues / 3 # issuecomment - 355313143 \"\"\" pmid = str ( pmid ) params = { ' db ' : ' pubmed ' , ' id ' : pmid , ' rettype ' : ' full ' , } headers = { ' User-Agent ' : get_manubot_user_agent () , } url = ' https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi ' with _get_eutils_rate_limiter () : response = requests . get ( url , params , headers = headers ) try : element_tree = xml . etree . ElementTree . fromstring ( response . text ) element_tree , = list ( element_tree ) except Exception as error : logging . error ( f ' Error fetching PubMed metadata for {pmid}. \\n ' f ' Invalid XML response from {response.url}: \\n {response.text} ' ) raise error try : csl_item = csl_item_from_pubmed_article ( element_tree ) except Exception as error : msg = f ' Error parsing the following PubMed metadata for PMID {pmid}: \\n {response.text} ' logging . error ( msg ) raise error return csl_item","title":"get_pubmed_csl_item"},{"location":"reference/manubot/cite/pubmed/#get_pubmed_ids_for_doi","text":"def get_pubmed_ids_for_doi ( doi ) Return a dictionary with PMCID and PMID, if they exist, for the specified DOI. See https://github.com/manubot/manubot/issues/45. View Source def get_pubmed_ids_for_doi ( doi ) : \"\"\" Return a dictionary with PMCID and PMID , if they exist , for the specified DOI . See https : // github . com / manubot / manubot / issues / 45 . \"\"\" pubmed_ids = get_pmcid_and_pmid_for_doi ( doi ) if not pubmed_ids : pmid = get_pmid_for_doi ( doi ) if pmid : pubmed_ids [ ' PMID ' ] = pmid return pubmed_ids","title":"get_pubmed_ids_for_doi"},{"location":"reference/manubot/cite/url/","text":"Module manubot.cite.url View Source import json import logging import re def get_url_csl_item ( url ): \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f 'Error in {retriever.__name__} for {url} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) raise Exception ( f 'all get_url_csl_item methods failed for {url}' ) def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import ( export_as_csl , web_query , ) zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'Connection' : 'close' , # https://github.com/kennethreitz/requests/issues/4023 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( 'http://greycite.knowledgeblog.org/json' , params = { 'uri' : url }, headers = headers , ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( '' , response . text ) csl_item = json . loads ( text ) csl_item [ 'type' ] = 'webpage' return csl_item def get_url_csl_item_manual ( url ): \"\"\" Manually create csl_item for a URL. \"\"\" return { 'URL' : url , 'type' : 'webpage' , } url_retrievers = [ get_url_csl_item_zotero , get_url_csl_item_greycite , get_url_csl_item_manual , ] Variables url_retrievers Functions get_url_csl_item def get_url_csl_item ( url ) Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable url_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_url_csl_item ( url ) : \"\"\" Get csl_item for a URL trying a sequence of strategies . This function uses a list of CSL JSON Item metadata retrievers , specified by the module - level variable ` url_retrievers `. The methods are attempted in order , with this function returning the metadata from the first non - failing method . \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f ' Error in {retriever.__name__} for {url} ' f ' due to a {error.__class__.__name__}: \\n {error} ' ) logging . info ( error , exc_info = True ) raise Exception ( f ' all get_url_csl_item methods failed for {url} ' ) get_url_csl_item_greycite def get_url_csl_item_greycite ( url ) Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C View Source def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'Connection' : 'close' , # https://github.com/kennethreitz/requests/issues/4023 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( 'http://greycite.knowledgeblog.org/json' , params = { 'uri' : url }, headers = headers , ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( '' , response . text ) csl_item = json . loads ( text ) csl_item [ 'type' ] = 'webpage' return csl_item get_url_csl_item_manual def get_url_csl_item_manual ( url ) Manually create csl_item for a URL. View Source def get_url_csl_item_manual ( url ) : \"\"\" Manually create csl_item for a URL . \"\"\" return { ' URL ' : url , ' type ' : ' webpage ' , } get_url_csl_item_zotero def get_url_csl_item_zotero ( url ) Use Zotero's translation-server to generate a CSL Item for the specified URL. View Source def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import ( export_as_csl , web_query , ) zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item","title":"Url"},{"location":"reference/manubot/cite/url/#module-manubotciteurl","text":"View Source import json import logging import re def get_url_csl_item ( url ): \"\"\" Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable `url_retrievers`. The methods are attempted in order, with this function returning the metadata from the first non-failing method. \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f 'Error in {retriever.__name__} for {url} ' f 'due to a {error.__class__.__name__}: \\n {error}' ) logging . info ( error , exc_info = True ) raise Exception ( f 'all get_url_csl_item methods failed for {url}' ) def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import ( export_as_csl , web_query , ) zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'Connection' : 'close' , # https://github.com/kennethreitz/requests/issues/4023 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( 'http://greycite.knowledgeblog.org/json' , params = { 'uri' : url }, headers = headers , ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( '' , response . text ) csl_item = json . loads ( text ) csl_item [ 'type' ] = 'webpage' return csl_item def get_url_csl_item_manual ( url ): \"\"\" Manually create csl_item for a URL. \"\"\" return { 'URL' : url , 'type' : 'webpage' , } url_retrievers = [ get_url_csl_item_zotero , get_url_csl_item_greycite , get_url_csl_item_manual , ]","title":"Module manubot.cite.url"},{"location":"reference/manubot/cite/url/#variables","text":"url_retrievers","title":"Variables"},{"location":"reference/manubot/cite/url/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/url/#get_url_csl_item","text":"def get_url_csl_item ( url ) Get csl_item for a URL trying a sequence of strategies. This function uses a list of CSL JSON Item metadata retrievers, specified by the module-level variable url_retrievers . The methods are attempted in order, with this function returning the metadata from the first non-failing method. View Source def get_url_csl_item ( url ) : \"\"\" Get csl_item for a URL trying a sequence of strategies . This function uses a list of CSL JSON Item metadata retrievers , specified by the module - level variable ` url_retrievers `. The methods are attempted in order , with this function returning the metadata from the first non - failing method . \"\"\" for retriever in url_retrievers : try : return retriever ( url ) except Exception as error : logging . warning ( f ' Error in {retriever.__name__} for {url} ' f ' due to a {error.__class__.__name__}: \\n {error} ' ) logging . info ( error , exc_info = True ) raise Exception ( f ' all get_url_csl_item methods failed for {url} ' )","title":"get_url_csl_item"},{"location":"reference/manubot/cite/url/#get_url_csl_item_greycite","text":"def get_url_csl_item_greycite ( url ) Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C View Source def get_url_csl_item_greycite ( url ): \"\"\" Uses Greycite which has experiened uptime problems in the past. API calls seem to take at least 15 seconds. Browser requests are much faster. Setting header did not have an effect. Consider mimicking browser using selenium. More information on Greycite at: http://greycite.knowledgeblog.org/ http://knowledgeblog.org/greycite https://arxiv.org/abs/1304.7151 https://git.io/v9N2C \"\"\" import requests from manubot.util import get_manubot_user_agent headers = { 'Connection' : 'close' , # https://github.com/kennethreitz/requests/issues/4023 'User-Agent' : get_manubot_user_agent (), } response = requests . get ( 'http://greycite.knowledgeblog.org/json' , params = { 'uri' : url }, headers = headers , ) # Some Greycite responses were valid JSON besides for an error appended # like \"<p>*** Date set from uri<p>\" or \"<p>*** fetch error : 404<p>\". pattern = re . compile ( r \"<p>\\*\\*\\*.*<p>\" ) text = pattern . sub ( '' , response . text ) csl_item = json . loads ( text ) csl_item [ 'type' ] = 'webpage' return csl_item","title":"get_url_csl_item_greycite"},{"location":"reference/manubot/cite/url/#get_url_csl_item_manual","text":"def get_url_csl_item_manual ( url ) Manually create csl_item for a URL. View Source def get_url_csl_item_manual ( url ) : \"\"\" Manually create csl_item for a URL . \"\"\" return { ' URL ' : url , ' type ' : ' webpage ' , }","title":"get_url_csl_item_manual"},{"location":"reference/manubot/cite/url/#get_url_csl_item_zotero","text":"def get_url_csl_item_zotero ( url ) Use Zotero's translation-server to generate a CSL Item for the specified URL. View Source def get_url_csl_item_zotero ( url ): \"\"\" Use Zotero's translation-server to generate a CSL Item for the specified URL. \"\"\" from manubot.cite.zotero import ( export_as_csl , web_query , ) zotero_data = web_query ( url ) csl_data = export_as_csl ( zotero_data ) csl_item , = csl_data return csl_item","title":"get_url_csl_item_zotero"},{"location":"reference/manubot/cite/wikidata/","text":"Module manubot.cite.wikidata View Source def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f 'https://www.wikidata.org/wiki/{identifier}' from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if 'DOI' in csl_item : csl_item [ 'DOI' ] = csl_item [ 'DOI' ] . lower () if not 'URL' in csl_item : csl_item [ 'URL' ] = url return csl_item Functions get_wikidata_csl_item def get_wikidata_csl_item ( identifier ) Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. View Source def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f 'https://www.wikidata.org/wiki/{identifier}' from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if 'DOI' in csl_item : csl_item [ 'DOI' ] = csl_item [ 'DOI' ] . lower () if not 'URL' in csl_item : csl_item [ 'URL' ] = url return csl_item","title":"Wikidata"},{"location":"reference/manubot/cite/wikidata/#module-manubotcitewikidata","text":"View Source def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f 'https://www.wikidata.org/wiki/{identifier}' from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if 'DOI' in csl_item : csl_item [ 'DOI' ] = csl_item [ 'DOI' ] . lower () if not 'URL' in csl_item : csl_item [ 'URL' ] = url return csl_item","title":"Module manubot.cite.wikidata"},{"location":"reference/manubot/cite/wikidata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/wikidata/#get_wikidata_csl_item","text":"def get_wikidata_csl_item ( identifier ) Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. View Source def get_wikidata_csl_item ( identifier ): \"\"\" Get a CSL JSON item with the citation metadata for a Wikidata item. identifier should be a Wikidata item ID corresponding to a citeable work, such as Q50051684. \"\"\" url = f 'https://www.wikidata.org/wiki/{identifier}' from manubot.cite.url import get_url_csl_item_zotero csl_item = get_url_csl_item_zotero ( url ) if 'DOI' in csl_item : csl_item [ 'DOI' ] = csl_item [ 'DOI' ] . lower () if not 'URL' in csl_item : csl_item [ 'URL' ] = url return csl_item","title":"get_wikidata_csl_item"},{"location":"reference/manubot/cite/zotero/","text":"Module manubot.cite.zotero Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. View Source \"\"\" Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. \"\"\" import json import logging import requests from manubot.util import get_manubot_user_agent base_url = 'https://translate.manubot.org' def web_query ( url ): \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { 'User-Agent' : get_manubot_user_agent (), 'Content-Type' : 'text/plain' , } params = { 'single' : 1 , } api_url = f '{base_url}/web' response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f 'Error parsing web_query output as JSON for {url}: \\n {response.text}' ) raise error if response . status_code == 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f 'web_query returned multiple results for {url}: \\n ' + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f 'multiple results for {url}' ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def search_query ( identifier ): \"\"\" Supports DOI, ISBN, PMID, arXiv ID. curl -d 10.2307/4486062 -H 'Content-Type: text/plain' http://127.0.0.1:1969/search \"\"\" api_url = f '{base_url}/search' headers = { 'User-Agent' : get_manubot_user_agent (), 'Content-Type' : 'text/plain' , } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f 'Error parsing search_query output as JSON for {identifier}: \\n {response.text}' ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def _passthrough_zotero_data ( zotero_data ): \"\"\" Address known issues with Zotero metadata. Assumes zotero data should contain a single bibliographic record. \"\"\" if not isinstance ( zotero_data , list ): raise ValueError ( '_passthrough_zotero_data: zotero_data should be a list' ) if len ( zotero_data ) > 1 : # Sometimes translation-server creates multiple data items for a single record. # If so, keep only the parent item, and remove child items (such as notes). # https://github.com/zotero/translation-server/issues/67 zotero_data = zotero_data [: 1 ] return zotero_data def export_as_csl ( zotero_data ): \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f '{base_url}/export' params = { 'format' : 'csljson' , } headers = { 'User-Agent' : get_manubot_user_agent (), } response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f 'export_as_csl: translation-server returned status code {response.status_code}' logging . warning ( f '{message} with the following output: \\n {response.text}' ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f 'Error parsing export_as_csl output as JSON: \\n {response.text}' ) raise error return csl_json Variables base_url Functions export_as_csl def export_as_csl ( zotero_data ) Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: curl --verbose --data @items.json --header 'Content-Type: application/json' 'https://translate.manubot.org/export?format=csljson' View Source def export_as_csl ( zotero_data ) : \"\"\" Export Zotero JSON data to CSL JSON using a translation - server / export query . Performs a similar query to the following curl command : ``` curl -- verbose \\ -- data @ items . json \\ -- header ' Content-Type: application/json ' \\ ' https://translate.manubot.org/export?format=csljson ' ``` \"\"\" api_url = f ' {base_url}/export ' params = { ' format ' : ' csljson ' , } headers = { ' User-Agent ' : get_manubot_user_agent () , } response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f ' export_as_csl: translation-server returned status code {response.status_code} ' logging . warning ( f ' {message} with the following output: \\n {response.text} ' ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f ' Error parsing export_as_csl output as JSON: \\n {response.text} ' ) raise error return csl_json search_query def search_query ( identifier ) Supports DOI, ISBN, PMID, arXiv ID. curl -d 10.2307/4486062 -H 'Content-Type: text/plain' http://127.0.0.1:1969/search View Source def search_query ( identifier ) : \"\"\" Supports DOI , ISBN , PMID , arXiv ID . curl - d 10 . 2307 / 4486062 - H ' Content-Type: text/plain ' http : // 127 . 0 . 0 . 1 : 1969 / search \"\"\" api_url = f ' {base_url}/search ' headers = { ' User-Agent ' : get_manubot_user_agent () , ' Content-Type ' : ' text/plain ' , } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f ' Error parsing search_query output as JSON for {identifier}: \\n {response.text} ' ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data web_query def web_query ( url ) Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. View Source def web_query ( url ) : \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL ' s metadata. \"\"\" headers = { ' User-Agent ' : get_manubot_user_agent () , ' Content-Type ' : ' text/plain ' , } params = { ' single ' : 1 , } api_url = f ' {base_url}/web ' response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f ' Error parsing web_query output as JSON for {url}: \\n {response.text} ' ) raise error if response . status_code == 300 : # When single = 1 is specified , multiple results should never be returned logging . warning ( f ' web_query returned multiple results for {url}: \\n ' + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f ' multiple results for {url} ' ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"Zotero"},{"location":"reference/manubot/cite/zotero/#module-manubotcitezotero","text":"Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. View Source \"\"\" Methods to interact with a Zotero translation-server. https://github.com/zotero/translation-server The Manubot team currently hosts a public translation server at https://translate.manubot.org. More information on this instance at https://github.com/manubot/manubot/issues/82. \"\"\" import json import logging import requests from manubot.util import get_manubot_user_agent base_url = 'https://translate.manubot.org' def web_query ( url ): \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. \"\"\" headers = { 'User-Agent' : get_manubot_user_agent (), 'Content-Type' : 'text/plain' , } params = { 'single' : 1 , } api_url = f '{base_url}/web' response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f 'Error parsing web_query output as JSON for {url}: \\n {response.text}' ) raise error if response . status_code == 300 : # When single=1 is specified, multiple results should never be returned logging . warning ( f 'web_query returned multiple results for {url}: \\n ' + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f 'multiple results for {url}' ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def search_query ( identifier ): \"\"\" Supports DOI, ISBN, PMID, arXiv ID. curl -d 10.2307/4486062 -H 'Content-Type: text/plain' http://127.0.0.1:1969/search \"\"\" api_url = f '{base_url}/search' headers = { 'User-Agent' : get_manubot_user_agent (), 'Content-Type' : 'text/plain' , } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f 'Error parsing search_query output as JSON for {identifier}: \\n {response.text}' ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data def _passthrough_zotero_data ( zotero_data ): \"\"\" Address known issues with Zotero metadata. Assumes zotero data should contain a single bibliographic record. \"\"\" if not isinstance ( zotero_data , list ): raise ValueError ( '_passthrough_zotero_data: zotero_data should be a list' ) if len ( zotero_data ) > 1 : # Sometimes translation-server creates multiple data items for a single record. # If so, keep only the parent item, and remove child items (such as notes). # https://github.com/zotero/translation-server/issues/67 zotero_data = zotero_data [: 1 ] return zotero_data def export_as_csl ( zotero_data ): \"\"\" Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: ``` curl --verbose \\ --data @items.json \\ --header 'Content-Type: application/json' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" api_url = f '{base_url}/export' params = { 'format' : 'csljson' , } headers = { 'User-Agent' : get_manubot_user_agent (), } response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f 'export_as_csl: translation-server returned status code {response.status_code}' logging . warning ( f '{message} with the following output: \\n {response.text}' ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f 'Error parsing export_as_csl output as JSON: \\n {response.text}' ) raise error return csl_json","title":"Module manubot.cite.zotero"},{"location":"reference/manubot/cite/zotero/#variables","text":"base_url","title":"Variables"},{"location":"reference/manubot/cite/zotero/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/zotero/#export_as_csl","text":"def export_as_csl ( zotero_data ) Export Zotero JSON data to CSL JSON using a translation-server /export query. Performs a similar query to the following curl command: curl --verbose --data @items.json --header 'Content-Type: application/json' 'https://translate.manubot.org/export?format=csljson' View Source def export_as_csl ( zotero_data ) : \"\"\" Export Zotero JSON data to CSL JSON using a translation - server / export query . Performs a similar query to the following curl command : ``` curl -- verbose \\ -- data @ items . json \\ -- header ' Content-Type: application/json ' \\ ' https://translate.manubot.org/export?format=csljson ' ``` \"\"\" api_url = f ' {base_url}/export ' params = { ' format ' : ' csljson ' , } headers = { ' User-Agent ' : get_manubot_user_agent () , } response = requests . post ( api_url , params = params , headers = headers , json = zotero_data ) if not response . ok : message = f ' export_as_csl: translation-server returned status code {response.status_code} ' logging . warning ( f ' {message} with the following output: \\n {response.text} ' ) raise requests . HTTPError ( message ) try : csl_json = response . json () except Exception as error : logging . warning ( f ' Error parsing export_as_csl output as JSON: \\n {response.text} ' ) raise error return csl_json","title":"export_as_csl"},{"location":"reference/manubot/cite/zotero/#search_query","text":"def search_query ( identifier ) Supports DOI, ISBN, PMID, arXiv ID. curl -d 10.2307/4486062 -H 'Content-Type: text/plain' http://127.0.0.1:1969/search View Source def search_query ( identifier ) : \"\"\" Supports DOI , ISBN , PMID , arXiv ID . curl - d 10 . 2307 / 4486062 - H ' Content-Type: text/plain ' http : // 127 . 0 . 0 . 1 : 1969 / search \"\"\" api_url = f ' {base_url}/search ' headers = { ' User-Agent ' : get_manubot_user_agent () , ' Content-Type ' : ' text/plain ' , } response = requests . post ( api_url , headers = headers , data = str ( identifier )) try : zotero_data = response . json () except Exception as error : logging . warning ( f ' Error parsing search_query output as JSON for {identifier}: \\n {response.text} ' ) raise error zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"search_query"},{"location":"reference/manubot/cite/zotero/#web_query","text":"def web_query ( url ) Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL's metadata. View Source def web_query ( url ) : \"\"\" Return Zotero citation metadata for a URL as a list containing a single element that is a dictionary with the URL ' s metadata. \"\"\" headers = { ' User-Agent ' : get_manubot_user_agent () , ' Content-Type ' : ' text/plain ' , } params = { ' single ' : 1 , } api_url = f ' {base_url}/web ' response = requests . post ( api_url , params = params , headers = headers , data = str ( url )) try : zotero_data = response . json () except Exception as error : logging . warning ( f ' Error parsing web_query output as JSON for {url}: \\n {response.text} ' ) raise error if response . status_code == 300 : # When single = 1 is specified , multiple results should never be returned logging . warning ( f ' web_query returned multiple results for {url}: \\n ' + json . dumps ( zotero_data , indent = 2 ) ) raise ValueError ( f ' multiple results for {url} ' ) zotero_data = _passthrough_zotero_data ( zotero_data ) return zotero_data","title":"web_query"},{"location":"reference/manubot/cite/tests/","text":"Module manubot.cite.tests Sub-modules manubot.cite.tests.test_cite_command manubot.cite.tests.test_citekey manubot.cite.tests.test_citekey_api manubot.cite.tests.test_citeproc manubot.cite.tests.test_csl_item manubot.cite.tests.test_doi manubot.cite.tests.test_isbn manubot.cite.tests.test_pubmed manubot.cite.tests.test_url manubot.cite.tests.test_wikidata manubot.cite.tests.test_zotero","title":"Index"},{"location":"reference/manubot/cite/tests/#module-manubotcitetests","text":"","title":"Module manubot.cite.tests"},{"location":"reference/manubot/cite/tests/#sub-modules","text":"manubot.cite.tests.test_cite_command manubot.cite.tests.test_citekey manubot.cite.tests.test_citekey_api manubot.cite.tests.test_citeproc manubot.cite.tests.test_csl_item manubot.cite.tests.test_doi manubot.cite.tests.test_isbn manubot.cite.tests.test_pubmed manubot.cite.tests.test_url manubot.cite.tests.test_wikidata manubot.cite.tests.test_zotero","title":"Sub-modules"},{"location":"reference/manubot/cite/tests/test_cite_command/","text":"Module manubot.cite.tests.test_cite_command View Source import json import pathlib import shutil import subprocess import pytest from manubot.util import shlex_join from manubot.pandoc.util import ( get_pandoc_info , ) def test_cite_command_empty (): process = subprocess . run ( [ 'manubot' , 'cite' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'the following arguments are required: citekeys' in process . stderr def test_cite_command_stdout (): process = subprocess . run ( [ 'manubot' , 'cite' , 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 0 csl , = json . loads ( process . stdout ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1' def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / 'csl-items.json' process = subprocess . run ( [ 'manubot' , 'cite' , '--output' , str ( path ), 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : csl , = json . load ( read_file ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1' @pytest.mark.parametrize ([ 'args' , 'expected' ], [ ([], 'references-plain.txt' ), ([ '--format' , 'plain' ], 'references-plain.txt' ), ([ '--format' , 'markdown' ], 'references-markdown.md' ), ([ '--format' , 'html' ], 'references-html.html' ), ([ '--format' , 'jats' ], 'references-jats.xml' ), ], ids = [ 'no-args' , '--format=plain' , '--format=markdown' , '--format=html' , '--format=jats' ]) @pytest.mark.skipif ( not shutil . which ( 'pandoc' ), reason = 'pandoc installation not found on system' ) @pytest.mark.skipif ( not shutil . which ( 'pandoc-citeproc' ), reason = 'pandoc-citeproc installation not found on system' ) def test_cite_command_render_stdout ( args , expected ): \"\"\" Test the stdout output of `manubot cite --render` with various formats. The output is sensitive to the version of Pandoc used, so rather than fail when the system's pandoc is outdated, the test is skipped. \"\"\" pandoc_version = get_pandoc_info ()[ 'pandoc version' ] for output in 'markdown' , 'html' , 'jats' : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) expected = ( pathlib . Path ( __file__ ) . parent . joinpath ( 'cite-command-rendered' , expected ) . read_text () ) args = [ 'manubot' , 'cite' , '--render' , '--csl' , 'https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl' , 'arxiv:1806.05726v1' , 'doi:10.7717/peerj.338' , 'pmid:29618526' , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( shlex_join ( process . args )) print ( process . stdout ) print ( process . stderr ) assert process . stdout == expected def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 ) Functions teardown_module def teardown_module ( module ) Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. View Source def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 ) test_cite_command_empty def test_cite_command_empty ( ) View Source def test_cite_command_empty (): process = subprocess . run ( [ 'manubot' , 'cite' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'the following arguments are required: citekeys' in process . stderr test_cite_command_file def test_cite_command_file ( tmpdir ) View Source def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / 'csl-items.json' process = subprocess . run ( [ 'manubot' , 'cite' , '--output' , str ( path ), 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : csl , = json . load ( read_file ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1' test_cite_command_render_stdout def test_cite_command_render_stdout ( args , expected ) Test the stdout output of manubot cite --render with various formats. The output is sensitive to the version of Pandoc used, so rather than fail when the system's pandoc is outdated, the test is skipped. View Source @ pytest . mark . parametrize ( [ ' args ' , ' expected ' ], [ ( [], ' references-plain.txt ' ) , ( [ ' --format ' , ' plain ' ], ' references-plain.txt ' ) , ( [ ' --format ' , ' markdown ' ], ' references-markdown.md ' ) , ( [ ' --format ' , ' html ' ], ' references-html.html ' ) , ( [ ' --format ' , ' jats ' ], ' references-jats.xml ' ) , ], ids = [ ' no-args ' , ' --format=plain ' , ' --format=markdown ' , ' --format=html ' , ' --format=jats ' ] ) @ pytest . mark . skipif ( not shutil . which ( ' pandoc ' ) , reason = ' pandoc installation not found on system ' ) @ pytest . mark . skipif ( not shutil . which ( ' pandoc-citeproc ' ) , reason = ' pandoc-citeproc installation not found on system ' ) def test_cite_command_render_stdout ( args , expected ) : \"\"\" Test the stdout output of ` manubot cite -- render ` with various formats . The output is sensitive to the version of Pandoc used , so rather than fail when the system ' s pandoc is outdated, the test is skipped. \"\"\" pandoc_version = get_pandoc_info () [ ' pandoc version ' ] for output in ' markdown ' , ' html ' , ' jats ' : if output in args and pandoc_version < ( 2 , 5 ) : pytest . skip ( f \" Test {output} output assumes pandoc >= 2.5 \" ) if pandoc_version < ( 2 , 0 ) : pytest . skip ( \" Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL \" ) expected = ( pathlib . Path ( __file__ ) . parent . joinpath ( ' cite-command-rendered ' , expected ) . read_text () ) args = [ ' manubot ' , ' cite ' , ' --render ' , ' --csl ' , ' https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl ' , ' arxiv:1806.05726v1 ' , ' doi:10.7717/peerj.338 ' , ' pmid:29618526 ' , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( shlex_join ( process . args )) print ( process . stdout ) print ( process . stderr ) assert process . stdout == expected test_cite_command_stdout def test_cite_command_stdout ( ) View Source def test_cite_command_stdout (): process = subprocess . run ( [ 'manubot' , 'cite' , 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 0 csl , = json . loads ( process . stdout ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1'","title":"Test Cite Command"},{"location":"reference/manubot/cite/tests/test_cite_command/#module-manubotciteteststest_cite_command","text":"View Source import json import pathlib import shutil import subprocess import pytest from manubot.util import shlex_join from manubot.pandoc.util import ( get_pandoc_info , ) def test_cite_command_empty (): process = subprocess . run ( [ 'manubot' , 'cite' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'the following arguments are required: citekeys' in process . stderr def test_cite_command_stdout (): process = subprocess . run ( [ 'manubot' , 'cite' , 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 0 csl , = json . loads ( process . stdout ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1' def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / 'csl-items.json' process = subprocess . run ( [ 'manubot' , 'cite' , '--output' , str ( path ), 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : csl , = json . load ( read_file ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1' @pytest.mark.parametrize ([ 'args' , 'expected' ], [ ([], 'references-plain.txt' ), ([ '--format' , 'plain' ], 'references-plain.txt' ), ([ '--format' , 'markdown' ], 'references-markdown.md' ), ([ '--format' , 'html' ], 'references-html.html' ), ([ '--format' , 'jats' ], 'references-jats.xml' ), ], ids = [ 'no-args' , '--format=plain' , '--format=markdown' , '--format=html' , '--format=jats' ]) @pytest.mark.skipif ( not shutil . which ( 'pandoc' ), reason = 'pandoc installation not found on system' ) @pytest.mark.skipif ( not shutil . which ( 'pandoc-citeproc' ), reason = 'pandoc-citeproc installation not found on system' ) def test_cite_command_render_stdout ( args , expected ): \"\"\" Test the stdout output of `manubot cite --render` with various formats. The output is sensitive to the version of Pandoc used, so rather than fail when the system's pandoc is outdated, the test is skipped. \"\"\" pandoc_version = get_pandoc_info ()[ 'pandoc version' ] for output in 'markdown' , 'html' , 'jats' : if output in args and pandoc_version < ( 2 , 5 ): pytest . skip ( f \"Test {output} output assumes pandoc >= 2.5\" ) if pandoc_version < ( 2 , 0 ): pytest . skip ( \"Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL\" ) expected = ( pathlib . Path ( __file__ ) . parent . joinpath ( 'cite-command-rendered' , expected ) . read_text () ) args = [ 'manubot' , 'cite' , '--render' , '--csl' , 'https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl' , 'arxiv:1806.05726v1' , 'doi:10.7717/peerj.338' , 'pmid:29618526' , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( shlex_join ( process . args )) print ( process . stdout ) print ( process . stderr ) assert process . stdout == expected def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 )","title":"Module manubot.cite.tests.test_cite_command"},{"location":"reference/manubot/cite/tests/test_cite_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_cite_command/#teardown_module","text":"def teardown_module ( module ) Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. View Source def teardown_module ( module ): \"\"\" Avoid too many requests to NCBI E-Utils in the test_pubmed.py, which is executed following this module. E-Utility requests are capped at 3 per second, which is usually controled by _get_eutils_rate_limiter, but this does not seem to work across test modules. \"\"\" import time time . sleep ( 1 )","title":"teardown_module"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_empty","text":"def test_cite_command_empty ( ) View Source def test_cite_command_empty (): process = subprocess . run ( [ 'manubot' , 'cite' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'the following arguments are required: citekeys' in process . stderr","title":"test_cite_command_empty"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_file","text":"def test_cite_command_file ( tmpdir ) View Source def test_cite_command_file ( tmpdir ): path = pathlib . Path ( tmpdir ) / 'csl-items.json' process = subprocess . run ( [ 'manubot' , 'cite' , '--output' , str ( path ), 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . stderr . decode ()) assert process . returncode == 0 with path . open () as read_file : csl , = json . load ( read_file ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1'","title":"test_cite_command_file"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_render_stdout","text":"def test_cite_command_render_stdout ( args , expected ) Test the stdout output of manubot cite --render with various formats. The output is sensitive to the version of Pandoc used, so rather than fail when the system's pandoc is outdated, the test is skipped. View Source @ pytest . mark . parametrize ( [ ' args ' , ' expected ' ], [ ( [], ' references-plain.txt ' ) , ( [ ' --format ' , ' plain ' ], ' references-plain.txt ' ) , ( [ ' --format ' , ' markdown ' ], ' references-markdown.md ' ) , ( [ ' --format ' , ' html ' ], ' references-html.html ' ) , ( [ ' --format ' , ' jats ' ], ' references-jats.xml ' ) , ], ids = [ ' no-args ' , ' --format=plain ' , ' --format=markdown ' , ' --format=html ' , ' --format=jats ' ] ) @ pytest . mark . skipif ( not shutil . which ( ' pandoc ' ) , reason = ' pandoc installation not found on system ' ) @ pytest . mark . skipif ( not shutil . which ( ' pandoc-citeproc ' ) , reason = ' pandoc-citeproc installation not found on system ' ) def test_cite_command_render_stdout ( args , expected ) : \"\"\" Test the stdout output of ` manubot cite -- render ` with various formats . The output is sensitive to the version of Pandoc used , so rather than fail when the system ' s pandoc is outdated, the test is skipped. \"\"\" pandoc_version = get_pandoc_info () [ ' pandoc version ' ] for output in ' markdown ' , ' html ' , ' jats ' : if output in args and pandoc_version < ( 2 , 5 ) : pytest . skip ( f \" Test {output} output assumes pandoc >= 2.5 \" ) if pandoc_version < ( 2 , 0 ) : pytest . skip ( \" Test requires pandoc >= 2.0 to support --lua-filter and --csl=URL \" ) expected = ( pathlib . Path ( __file__ ) . parent . joinpath ( ' cite-command-rendered ' , expected ) . read_text () ) args = [ ' manubot ' , ' cite ' , ' --render ' , ' --csl ' , ' https://github.com/greenelab/manubot-rootstock/raw/e83e51dcd89256403bb787c3d9a46e4ee8d04a9e/build/assets/style.csl ' , ' arxiv:1806.05726v1 ' , ' doi:10.7717/peerj.338 ' , ' pmid:29618526 ' , ] + args process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( shlex_join ( process . args )) print ( process . stdout ) print ( process . stderr ) assert process . stdout == expected","title":"test_cite_command_render_stdout"},{"location":"reference/manubot/cite/tests/test_cite_command/#test_cite_command_stdout","text":"def test_cite_command_stdout ( ) View Source def test_cite_command_stdout (): process = subprocess . run ( [ 'manubot' , 'cite' , 'arxiv:1806.05726v1' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 0 csl , = json . loads ( process . stdout ) assert csl [ 'URL' ] == 'https://arxiv.org/abs/1806.05726v1'","title":"test_cite_command_stdout"},{"location":"reference/manubot/cite/tests/test_citekey/","text":"Module manubot.cite.tests.test_citekey Tests rest of functions in manubot.cite, not covered by test_citekey_api.py. View Source \"\"\"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py.\"\"\" import pytest from manubot.cite.citekey import ( citekey_pattern , shorten_citekey , infer_citekey_prefix , inspect_citekey , ) @pytest.mark.parametrize ( \"citation_string\" , [ ( '@doi:10.5061/dryad.q447c/1' ), ( '@arxiv:1407.3561v1' ), ( '@doi:10.1007/978-94-015-6859-3_4' ), ( '@tag:tag_with_underscores' ), ( '@tag:tag-with-hyphens' ), ( '@url:https://greenelab.github.io/manubot-rootstock/' ), ( '@tag:abc123' ), ( '@tag:123abc' ), ]) def test_citekey_pattern_match ( citation_string ): match = citekey_pattern . fullmatch ( citation_string ) assert match @pytest.mark.parametrize ( \"citation_string\" , [ ( 'doi:10.5061/dryad.q447c/1' ), ( '@tag:abc123-' ), ( '@tag:abc123_' ), ( '@-tag:abc123' ), ( '@_tag:abc123' ), ]) def test_citekey_pattern_no_match ( citation_string ): match = citekey_pattern . fullmatch ( citation_string ) assert match is None @pytest.mark.parametrize ( \"standard_citekey,expected\" , [ ( 'doi:10.5061/dryad.q447c/1' , 'kQFQ8EaO' ), ( 'arxiv:1407.3561v1' , '16kozZ9Ys' ), ( 'pmid:24159271' , '11sli93ov' ), ( 'url:http://blog.dhimmel.com/irreproducible-timestamps/' , 'QBWMEuxW' ), ]) def test_shorten_citekey ( standard_citekey , expected ): short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected @pytest.mark.parametrize ( 'citekey' , [ 'doi:10.7717/peerj.705' , 'doi:10/b6vnmd' , 'pmcid:PMC4304851' , 'pmid:25648772' , 'arxiv:1407.3561' , 'isbn:978-1-339-91988-1' , 'isbn:1-339-91988-5' , 'wikidata:Q1' , 'wikidata:Q50051684' , 'url:https://peerj.com/articles/705/' , ]) def test_inspect_citekey_passes ( citekey ): \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" assert inspect_citekey ( citekey ) is None @pytest.mark.parametrize ([ 'citekey' , 'contains' ], [ ( 'doi:10.771/peerj.705' , 'Double check the DOI' ), ( 'doi:10/b6v_nmd' , 'Double check the shortDOI' ), ( 'doi:7717/peerj.705' , \"must start with '10.'\" ), ( 'doi:b6vnmd' , \"must start with '10.'\" ), ( 'pmcid:25648772' , \"must start with 'PMC'\" ), ( 'pmid:PMC4304851' , \"Should 'pmid:PMC4304851' switch the citation source to 'pmcid'?\" ), ( 'isbn:1-339-91988-X' , 'identifier violates the ISBN syntax' ), ( 'wikidata:P212' , \"item IDs must start with 'Q'\" ), ( 'wikidata:QABCD' , 'does not conform to the Wikidata regex' ), ]) def test_inspect_citekey_fails ( citekey , contains ): \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = inspect_citekey ( citekey ) assert report is not None assert isinstance ( report , str ) assert contains in report @pytest.mark.parametrize ([ 'citation' , 'expect' ], [ ( 'doi:not-a-real-doi' , 'doi:not-a-real-doi' ), ( 'DOI:not-a-real-doi' , 'doi:not-a-real-doi' ), ( 'uRl:mixed-case-prefix' , 'url:mixed-case-prefix' ), ( 'raw:raw-citation' , 'raw:raw-citation' ), ( 'no-prefix' , 'raw:no-prefix' ), ( 'no-prefix:but-colon' , 'raw:no-prefix:but-colon' ), ]) def test_infer_citekey_prefix ( citation , expect ): assert infer_citekey_prefix ( citation ) == expect Variables citekey_pattern Functions test_citekey_pattern_match def test_citekey_pattern_match ( citation_string ) View Source @pytest . mark . parametrize ( \"citation_string\" , [ ('@doi:10.5061/dryad.q447c/1'), ('@arxiv:1407.3561v1'), ('@doi:10.1007/978-94-015-6859-3_4'), ('@tag:tag_with_underscores'), ('@tag:tag-with-hyphens'), ('@url:https://greenelab.github.io/manubot-rootstock/'), ('@tag:abc123'), ('@tag:123abc'), ] ) def test_citekey_pattern_match ( citation_string ) : match = citekey_pattern . fullmatch ( citation_string ) assert match test_citekey_pattern_no_match def test_citekey_pattern_no_match ( citation_string ) View Source @pytest . mark . parametrize ( \"citation_string\" , [ ('doi:10.5061/dryad.q447c/1'), ('@tag:abc123-'), ('@tag:abc123_'), ('@-tag:abc123'), ('@_tag:abc123'), ] ) def test_citekey_pattern_no_match ( citation_string ) : match = citekey_pattern . fullmatch ( citation_string ) assert match is None test_infer_citekey_prefix def test_infer_citekey_prefix ( citation , expect ) View Source @pytest . mark . parametrize ( [ 'citation', 'expect' ] , [ ('doi:not-a-real-doi', 'doi:not-a-real-doi'), ('DOI:not-a-real-doi', 'doi:not-a-real-doi'), ('uRl:mixed-case-prefix', 'url:mixed-case-prefix'), ('raw:raw-citation', 'raw:raw-citation'), ('no-prefix', 'raw:no-prefix'), ('no-prefix:but-colon', 'raw:no-prefix:but-colon'), ] ) def test_infer_citekey_prefix ( citation , expect ) : assert infer_citekey_prefix ( citation ) == expect test_inspect_citekey_fails def test_inspect_citekey_fails ( citekey , contains ) These citekeys should fail inspection by inspect_citekey. View Source @pytest . mark . parametrize ( [ 'citekey', 'contains' ] , [ ('doi:10.771/peerj.705', 'Double check the DOI'), ('doi:10/b6v_nmd', 'Double check the shortDOI'), ('doi:7717/peerj.705', \"must start with '10.'\"), ('doi:b6vnmd', \"must start with '10.'\"), ('pmcid:25648772', \"must start with 'PMC'\"), ('pmid:PMC4304851', \"Should 'pmid:PMC4304851' switch the citation source to 'pmcid'?\"), ('isbn:1-339-91988-X', 'identifier violates the ISBN syntax'), ('wikidata:P212', \"item IDs must start with 'Q'\"), ('wikidata:QABCD', 'does not conform to the Wikidata regex'), ] ) def test_inspect_citekey_fails ( citekey , contains ) : \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = inspect_citekey ( citekey ) assert report is not None assert isinstance ( report , str ) assert contains in report test_inspect_citekey_passes def test_inspect_citekey_passes ( citekey ) These citekeys should pass inspection by inspect_citekey. View Source @pytest . mark . parametrize ( 'citekey' , [ 'doi:10.7717/peerj.705', 'doi:10/b6vnmd', 'pmcid:PMC4304851', 'pmid:25648772', 'arxiv:1407.3561', 'isbn:978-1-339-91988-1', 'isbn:1-339-91988-5', 'wikidata:Q1', 'wikidata:Q50051684', 'url:https://peerj.com/articles/705/', ] ) def test_inspect_citekey_passes ( citekey ) : \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" assert inspect_citekey ( citekey ) is None test_shorten_citekey def test_shorten_citekey ( standard_citekey , expected ) View Source @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ ('doi:10.5061/dryad.q447c/1', 'kQFQ8EaO'), ('arxiv:1407.3561v1', '16kozZ9Ys'), ('pmid:24159271', '11sli93ov'), ('url:http://blog.dhimmel.com/irreproducible-timestamps/', 'QBWMEuxW'), ] ) def test_shorten_citekey ( standard_citekey , expected ) : short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected","title":"Test Citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#module-manubotciteteststest_citekey","text":"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py. View Source \"\"\"Tests rest of functions in manubot.cite, not covered by test_citekey_api.py.\"\"\" import pytest from manubot.cite.citekey import ( citekey_pattern , shorten_citekey , infer_citekey_prefix , inspect_citekey , ) @pytest.mark.parametrize ( \"citation_string\" , [ ( '@doi:10.5061/dryad.q447c/1' ), ( '@arxiv:1407.3561v1' ), ( '@doi:10.1007/978-94-015-6859-3_4' ), ( '@tag:tag_with_underscores' ), ( '@tag:tag-with-hyphens' ), ( '@url:https://greenelab.github.io/manubot-rootstock/' ), ( '@tag:abc123' ), ( '@tag:123abc' ), ]) def test_citekey_pattern_match ( citation_string ): match = citekey_pattern . fullmatch ( citation_string ) assert match @pytest.mark.parametrize ( \"citation_string\" , [ ( 'doi:10.5061/dryad.q447c/1' ), ( '@tag:abc123-' ), ( '@tag:abc123_' ), ( '@-tag:abc123' ), ( '@_tag:abc123' ), ]) def test_citekey_pattern_no_match ( citation_string ): match = citekey_pattern . fullmatch ( citation_string ) assert match is None @pytest.mark.parametrize ( \"standard_citekey,expected\" , [ ( 'doi:10.5061/dryad.q447c/1' , 'kQFQ8EaO' ), ( 'arxiv:1407.3561v1' , '16kozZ9Ys' ), ( 'pmid:24159271' , '11sli93ov' ), ( 'url:http://blog.dhimmel.com/irreproducible-timestamps/' , 'QBWMEuxW' ), ]) def test_shorten_citekey ( standard_citekey , expected ): short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected @pytest.mark.parametrize ( 'citekey' , [ 'doi:10.7717/peerj.705' , 'doi:10/b6vnmd' , 'pmcid:PMC4304851' , 'pmid:25648772' , 'arxiv:1407.3561' , 'isbn:978-1-339-91988-1' , 'isbn:1-339-91988-5' , 'wikidata:Q1' , 'wikidata:Q50051684' , 'url:https://peerj.com/articles/705/' , ]) def test_inspect_citekey_passes ( citekey ): \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" assert inspect_citekey ( citekey ) is None @pytest.mark.parametrize ([ 'citekey' , 'contains' ], [ ( 'doi:10.771/peerj.705' , 'Double check the DOI' ), ( 'doi:10/b6v_nmd' , 'Double check the shortDOI' ), ( 'doi:7717/peerj.705' , \"must start with '10.'\" ), ( 'doi:b6vnmd' , \"must start with '10.'\" ), ( 'pmcid:25648772' , \"must start with 'PMC'\" ), ( 'pmid:PMC4304851' , \"Should 'pmid:PMC4304851' switch the citation source to 'pmcid'?\" ), ( 'isbn:1-339-91988-X' , 'identifier violates the ISBN syntax' ), ( 'wikidata:P212' , \"item IDs must start with 'Q'\" ), ( 'wikidata:QABCD' , 'does not conform to the Wikidata regex' ), ]) def test_inspect_citekey_fails ( citekey , contains ): \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = inspect_citekey ( citekey ) assert report is not None assert isinstance ( report , str ) assert contains in report @pytest.mark.parametrize ([ 'citation' , 'expect' ], [ ( 'doi:not-a-real-doi' , 'doi:not-a-real-doi' ), ( 'DOI:not-a-real-doi' , 'doi:not-a-real-doi' ), ( 'uRl:mixed-case-prefix' , 'url:mixed-case-prefix' ), ( 'raw:raw-citation' , 'raw:raw-citation' ), ( 'no-prefix' , 'raw:no-prefix' ), ( 'no-prefix:but-colon' , 'raw:no-prefix:but-colon' ), ]) def test_infer_citekey_prefix ( citation , expect ): assert infer_citekey_prefix ( citation ) == expect","title":"Module manubot.cite.tests.test_citekey"},{"location":"reference/manubot/cite/tests/test_citekey/#variables","text":"citekey_pattern","title":"Variables"},{"location":"reference/manubot/cite/tests/test_citekey/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citekey/#test_citekey_pattern_match","text":"def test_citekey_pattern_match ( citation_string ) View Source @pytest . mark . parametrize ( \"citation_string\" , [ ('@doi:10.5061/dryad.q447c/1'), ('@arxiv:1407.3561v1'), ('@doi:10.1007/978-94-015-6859-3_4'), ('@tag:tag_with_underscores'), ('@tag:tag-with-hyphens'), ('@url:https://greenelab.github.io/manubot-rootstock/'), ('@tag:abc123'), ('@tag:123abc'), ] ) def test_citekey_pattern_match ( citation_string ) : match = citekey_pattern . fullmatch ( citation_string ) assert match","title":"test_citekey_pattern_match"},{"location":"reference/manubot/cite/tests/test_citekey/#test_citekey_pattern_no_match","text":"def test_citekey_pattern_no_match ( citation_string ) View Source @pytest . mark . parametrize ( \"citation_string\" , [ ('doi:10.5061/dryad.q447c/1'), ('@tag:abc123-'), ('@tag:abc123_'), ('@-tag:abc123'), ('@_tag:abc123'), ] ) def test_citekey_pattern_no_match ( citation_string ) : match = citekey_pattern . fullmatch ( citation_string ) assert match is None","title":"test_citekey_pattern_no_match"},{"location":"reference/manubot/cite/tests/test_citekey/#test_infer_citekey_prefix","text":"def test_infer_citekey_prefix ( citation , expect ) View Source @pytest . mark . parametrize ( [ 'citation', 'expect' ] , [ ('doi:not-a-real-doi', 'doi:not-a-real-doi'), ('DOI:not-a-real-doi', 'doi:not-a-real-doi'), ('uRl:mixed-case-prefix', 'url:mixed-case-prefix'), ('raw:raw-citation', 'raw:raw-citation'), ('no-prefix', 'raw:no-prefix'), ('no-prefix:but-colon', 'raw:no-prefix:but-colon'), ] ) def test_infer_citekey_prefix ( citation , expect ) : assert infer_citekey_prefix ( citation ) == expect","title":"test_infer_citekey_prefix"},{"location":"reference/manubot/cite/tests/test_citekey/#test_inspect_citekey_fails","text":"def test_inspect_citekey_fails ( citekey , contains ) These citekeys should fail inspection by inspect_citekey. View Source @pytest . mark . parametrize ( [ 'citekey', 'contains' ] , [ ('doi:10.771/peerj.705', 'Double check the DOI'), ('doi:10/b6v_nmd', 'Double check the shortDOI'), ('doi:7717/peerj.705', \"must start with '10.'\"), ('doi:b6vnmd', \"must start with '10.'\"), ('pmcid:25648772', \"must start with 'PMC'\"), ('pmid:PMC4304851', \"Should 'pmid:PMC4304851' switch the citation source to 'pmcid'?\"), ('isbn:1-339-91988-X', 'identifier violates the ISBN syntax'), ('wikidata:P212', \"item IDs must start with 'Q'\"), ('wikidata:QABCD', 'does not conform to the Wikidata regex'), ] ) def test_inspect_citekey_fails ( citekey , contains ) : \"\"\" These citekeys should fail inspection by inspect_citekey. \"\"\" report = inspect_citekey ( citekey ) assert report is not None assert isinstance ( report , str ) assert contains in report","title":"test_inspect_citekey_fails"},{"location":"reference/manubot/cite/tests/test_citekey/#test_inspect_citekey_passes","text":"def test_inspect_citekey_passes ( citekey ) These citekeys should pass inspection by inspect_citekey. View Source @pytest . mark . parametrize ( 'citekey' , [ 'doi:10.7717/peerj.705', 'doi:10/b6vnmd', 'pmcid:PMC4304851', 'pmid:25648772', 'arxiv:1407.3561', 'isbn:978-1-339-91988-1', 'isbn:1-339-91988-5', 'wikidata:Q1', 'wikidata:Q50051684', 'url:https://peerj.com/articles/705/', ] ) def test_inspect_citekey_passes ( citekey ) : \"\"\" These citekeys should pass inspection by inspect_citekey. \"\"\" assert inspect_citekey ( citekey ) is None","title":"test_inspect_citekey_passes"},{"location":"reference/manubot/cite/tests/test_citekey/#test_shorten_citekey","text":"def test_shorten_citekey ( standard_citekey , expected ) View Source @pytest . mark . parametrize ( \"standard_citekey,expected\" , [ ('doi:10.5061/dryad.q447c/1', 'kQFQ8EaO'), ('arxiv:1407.3561v1', '16kozZ9Ys'), ('pmid:24159271', '11sli93ov'), ('url:http://blog.dhimmel.com/irreproducible-timestamps/', 'QBWMEuxW'), ] ) def test_shorten_citekey ( standard_citekey , expected ) : short_citekey = shorten_citekey ( standard_citekey ) assert short_citekey == expected","title":"test_shorten_citekey"},{"location":"reference/manubot/cite/tests/test_citekey_api/","text":"Module manubot.cite.tests.test_citekey_api Tests API-level functions in manubot.cite. Both functions are found in citekey.py View Source \"\"\"Tests API-level functions in manubot.cite. Both functions are found in citekey.py\"\"\" import pytest from manubot.cite import ( citekey_to_csl_item , standardize_citekey , ) @pytest.mark.parametrize ( \"citekey,expected\" , [ ( 'doi:10.5061/DRYAD.q447c/1' , 'doi:10.5061/dryad.q447c/1' ), ( 'doi:10.5061/dryad.q447c/1' , 'doi:10.5061/dryad.q447c/1' ), ( 'doi:10/b6vnmd' , 'doi:10.1016/s0933-3657(96)00367-3' ), ( 'doi:10/B6VNMD' , 'doi:10.1016/s0933-3657(96)00367-3' ), ( 'doi:10/xxxxxxxxxxxxxYY' , 'doi:10/xxxxxxxxxxxxxyy' ), # passthrough non-existent shortDOI ( 'pmid:24159271' , 'pmid:24159271' ), ( 'isbn:1339919885' , 'isbn:9781339919881' ), ( 'isbn:1-339-91988-5' , 'isbn:9781339919881' ), ( 'isbn:978-0-387-95069-3' , 'isbn:9780387950693' ), ( 'isbn:9780387950938' , 'isbn:9780387950938' ), ( 'isbn:1-55860-510-X' , 'isbn:9781558605107' ), ( 'isbn:1-55860-510-x' , 'isbn:9781558605107' ), ]) def test_standardize_citekey ( citekey , expected ): \"\"\" Standardize identifiers based on their source \"\"\" output = standardize_citekey ( citekey ) assert output == expected @pytest.mark.xfail ( reason = 'https://twitter.com/dhimmel/status/950443969313419264' ) def test_citekey_to_csl_item_doi_datacite (): citekey = 'doi:10.7287/peerj.preprints.3100v1' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == '11cb5HXoY' assert csl_item [ 'URL' ] == 'https://doi.org/10.7287/peerj.preprints.3100v1' assert csl_item [ 'DOI' ] == '10.7287/peerj.preprints.3100v1' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'title' ] == 'Sci-Hub provides access to nearly all scholarly literature' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Himmelstein' assert authors [ - 1 ][ 'family' ] == 'Greene' def test_citekey_to_csl_item_arxiv (): citekey = 'arxiv:cond-mat/0703470v2' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'ES92tcdg' assert csl_item [ 'URL' ] == 'https://arxiv.org/abs/cond-mat/0703470v2' assert csl_item [ 'number' ] == 'cond-mat/0703470v2' assert csl_item [ 'version' ] == '2' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'container-title' ] == 'arXiv' assert csl_item [ 'title' ] == 'Portraits of Complex Networks' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'literal' ] == 'J. P. Bagrow' assert csl_item [ 'DOI' ] == '10.1209/0295-5075/81/68004' def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f 'pmcid:PMC3041534' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'RoOhUFKU' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/' assert csl_item [ 'container-title-short' ] == 'Summit Transl Bioinform' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' assert 'generated by Manubot' in csl_item [ 'note' ] assert 'standard_id: pmcid:PMC3041534' in csl_item [ 'note' ] def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = 'pmid:21347133' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'y9ONtSZ9' assert csl_item [ 'type' ] == 'article-journal' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pubmed/21347133' assert csl_item [ 'container-title' ] == 'Summit on translational bioinformatics' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.' assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ 'author' ] assert authors [ 0 ][ 'given' ] == 'Taxiarchis' assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = 'pmid:27094199' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ 'id' ] == 'alaFV9OY' assert csl_item [ 'type' ] == 'article-journal' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pubmed/27094199' assert csl_item [ 'container-title' ] == 'Circulation. Cardiovascular genetics' assert csl_item [ 'container-title-short' ] == 'Circ Cardiovasc Genet' assert csl_item [ 'page' ] == '179-84' assert csl_item [ 'title' ] == 'Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.' assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2016 , 4 ]] authors = csl_item [ 'author' ] assert authors [ 0 ][ 'given' ] == 'Casey S' assert authors [ 0 ][ 'family' ] == 'Greene' assert csl_item [ 'PMID' ] == '27094199' assert csl_item [ 'DOI' ] == '10.1161/circgenetics.115.001181' def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = 'pmid:29028984' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2018 , 3 , 15 ]] def test_citekey_to_csl_item_pubmed_book (): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" with pytest . raises ( NotImplementedError ): citekey_to_csl_item ( 'pmid:29227604' ) def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( 'isbn:9780387950693' ) assert csl_item [ 'type' ] == 'book' assert csl_item [ 'title' ] == 'Complex analysis' Functions test_citekey_to_csl_item_arxiv def test_citekey_to_csl_item_arxiv ( ) View Source def test_citekey_to_csl_item_arxiv (): citekey = 'arxiv:cond-mat/0703470v2' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'ES92tcdg' assert csl_item [ 'URL' ] == 'https://arxiv.org/abs/cond-mat/0703470v2' assert csl_item [ 'number' ] == 'cond-mat/0703470v2' assert csl_item [ 'version' ] == '2' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'container-title' ] == 'arXiv' assert csl_item [ 'title' ] == 'Portraits of Complex Networks' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'literal' ] == 'J. P. Bagrow' assert csl_item [ 'DOI' ] == '10.1209/0295-5075/81/68004' test_citekey_to_csl_item_doi_datacite def test_citekey_to_csl_item_doi_datacite ( ) View Source @pytest . mark . xfail ( reason = 'https://twitter.com/dhimmel/status/950443969313419264' ) def test_citekey_to_csl_item_doi_datacite () : citekey = 'doi:10.7287/peerj.preprints.3100v1' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == '11cb5HXoY' assert csl_item [ 'URL' ] == 'https://doi.org/10.7287/peerj.preprints.3100v1' assert csl_item [ 'DOI' ] == '10.7287/peerj.preprints.3100v1' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'title' ] == 'Sci-Hub provides access to nearly all scholarly literature' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Himmelstein' assert authors [ -1 ][ 'family' ] == 'Greene' test_citekey_to_csl_item_isbn def test_citekey_to_csl_item_isbn ( ) View Source def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( 'isbn:9780387950693' ) assert csl_item [ 'type' ] == 'book' assert csl_item [ 'title' ] == 'Complex analysis' test_citekey_to_csl_item_pmc def test_citekey_to_csl_item_pmc ( ) https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 View Source def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f 'pmcid:PMC3041534' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'RoOhUFKU' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/' assert csl_item [ 'container-title-short' ] == 'Summit Transl Bioinform' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' assert 'generated by Manubot' in csl_item [ 'note' ] assert 'standard_id: pmcid:PMC3041534' in csl_item [ 'note' ] test_citekey_to_csl_item_pubmed_1 def test_citekey_to_csl_item_pubmed_1 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full View Source def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = 'pmid:21347133' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'y9ONtSZ9' assert csl_item [ 'type' ] == 'article-journal' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pubmed/21347133' assert csl_item [ 'container-title' ] == 'Summit on translational bioinformatics' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.' assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ 'author' ] assert authors [ 0 ][ 'given' ] == 'Taxiarchis' assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' test_citekey_to_csl_item_pubmed_2 def test_citekey_to_csl_item_pubmed_2 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full View Source def test_citekey_to_csl_item_pubmed_2 () : \"\"\" Generated from XML returned by https : // eutils . ncbi . nlm . nih . gov / entrez / eutils / efetch . fcgi ? db = pubmed & id = 27094199 & rettype = full \"\"\" citekey = ' pmid:27094199 ' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ ' id ' ] == ' alaFV9OY ' assert csl_item [ ' type ' ] == ' article-journal ' assert csl_item [ ' URL ' ] == ' https://www.ncbi.nlm.nih.gov/pubmed/27094199 ' assert csl_item [ ' container-title ' ] == ' Circulation. Cardiovascular genetics ' assert csl_item [ ' container-title-short ' ] == ' Circ Cardiovasc Genet ' assert csl_item [ ' page ' ] == ' 179-84 ' assert csl_item [ ' title ' ] == ' Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits. ' assert csl_item [ ' issued ' ][ ' date-parts ' ] == [[ 2016 , 4 ]] authors = csl_item [ ' author ' ] assert authors [ 0 ][ ' given ' ] == ' Casey S ' assert authors [ 0 ][ ' family ' ] == ' Greene ' assert csl_item [ ' PMID ' ] == ' 27094199 ' assert csl_item [ ' DOI ' ] == ' 10.1161/circgenetics.115.001181 ' test_citekey_to_csl_item_pubmed_book def test_citekey_to_csl_item_pubmed_book ( ) Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full View Source def test_citekey_to_csl_item_pubmed_book (): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" with pytest . raises ( NotImplementedError ): citekey_to_csl_item ( 'pmid:29227604' ) test_citekey_to_csl_item_pubmed_with_numeric_month def test_citekey_to_csl_item_pubmed_with_numeric_month ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 View Source def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = 'pmid:29028984' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2018 , 3 , 15 ]] test_standardize_citekey def test_standardize_citekey ( citekey , expected ) Standardize identifiers based on their source View Source @pytest . mark . parametrize ( \"citekey,expected\" , [ ('doi:10.5061/DRYAD.q447c/1', 'doi:10.5061/dryad.q447c/1'), ('doi:10.5061/dryad.q447c/1', 'doi:10.5061/dryad.q447c/1'), ('doi:10/b6vnmd', 'doi:10.1016/s0933-3657(96)00367-3'), ('doi:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3'), ('doi:10/xxxxxxxxxxxxxYY', 'doi:10/xxxxxxxxxxxxxyy'), # passthrough non-existent shortDOI ('pmid:24159271', 'pmid:24159271'), ('isbn:1339919885', 'isbn:9781339919881'), ('isbn:1-339-91988-5', 'isbn:9781339919881'), ('isbn:978-0-387-95069-3', 'isbn:9780387950693'), ('isbn:9780387950938', 'isbn:9780387950938'), ('isbn:1-55860-510-X', 'isbn:9781558605107'), ('isbn:1-55860-510-x', 'isbn:9781558605107'), ] ) def test_standardize_citekey ( citekey , expected ) : \"\"\" Standardize identifiers based on their source \"\"\" output = standardize_citekey ( citekey ) assert output == expected","title":"Test Citekey Api"},{"location":"reference/manubot/cite/tests/test_citekey_api/#module-manubotciteteststest_citekey_api","text":"Tests API-level functions in manubot.cite. Both functions are found in citekey.py View Source \"\"\"Tests API-level functions in manubot.cite. Both functions are found in citekey.py\"\"\" import pytest from manubot.cite import ( citekey_to_csl_item , standardize_citekey , ) @pytest.mark.parametrize ( \"citekey,expected\" , [ ( 'doi:10.5061/DRYAD.q447c/1' , 'doi:10.5061/dryad.q447c/1' ), ( 'doi:10.5061/dryad.q447c/1' , 'doi:10.5061/dryad.q447c/1' ), ( 'doi:10/b6vnmd' , 'doi:10.1016/s0933-3657(96)00367-3' ), ( 'doi:10/B6VNMD' , 'doi:10.1016/s0933-3657(96)00367-3' ), ( 'doi:10/xxxxxxxxxxxxxYY' , 'doi:10/xxxxxxxxxxxxxyy' ), # passthrough non-existent shortDOI ( 'pmid:24159271' , 'pmid:24159271' ), ( 'isbn:1339919885' , 'isbn:9781339919881' ), ( 'isbn:1-339-91988-5' , 'isbn:9781339919881' ), ( 'isbn:978-0-387-95069-3' , 'isbn:9780387950693' ), ( 'isbn:9780387950938' , 'isbn:9780387950938' ), ( 'isbn:1-55860-510-X' , 'isbn:9781558605107' ), ( 'isbn:1-55860-510-x' , 'isbn:9781558605107' ), ]) def test_standardize_citekey ( citekey , expected ): \"\"\" Standardize identifiers based on their source \"\"\" output = standardize_citekey ( citekey ) assert output == expected @pytest.mark.xfail ( reason = 'https://twitter.com/dhimmel/status/950443969313419264' ) def test_citekey_to_csl_item_doi_datacite (): citekey = 'doi:10.7287/peerj.preprints.3100v1' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == '11cb5HXoY' assert csl_item [ 'URL' ] == 'https://doi.org/10.7287/peerj.preprints.3100v1' assert csl_item [ 'DOI' ] == '10.7287/peerj.preprints.3100v1' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'title' ] == 'Sci-Hub provides access to nearly all scholarly literature' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Himmelstein' assert authors [ - 1 ][ 'family' ] == 'Greene' def test_citekey_to_csl_item_arxiv (): citekey = 'arxiv:cond-mat/0703470v2' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'ES92tcdg' assert csl_item [ 'URL' ] == 'https://arxiv.org/abs/cond-mat/0703470v2' assert csl_item [ 'number' ] == 'cond-mat/0703470v2' assert csl_item [ 'version' ] == '2' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'container-title' ] == 'arXiv' assert csl_item [ 'title' ] == 'Portraits of Complex Networks' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'literal' ] == 'J. P. Bagrow' assert csl_item [ 'DOI' ] == '10.1209/0295-5075/81/68004' def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f 'pmcid:PMC3041534' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'RoOhUFKU' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/' assert csl_item [ 'container-title-short' ] == 'Summit Transl Bioinform' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' assert 'generated by Manubot' in csl_item [ 'note' ] assert 'standard_id: pmcid:PMC3041534' in csl_item [ 'note' ] def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = 'pmid:21347133' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'y9ONtSZ9' assert csl_item [ 'type' ] == 'article-journal' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pubmed/21347133' assert csl_item [ 'container-title' ] == 'Summit on translational bioinformatics' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.' assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ 'author' ] assert authors [ 0 ][ 'given' ] == 'Taxiarchis' assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' def test_citekey_to_csl_item_pubmed_2 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full \"\"\" citekey = 'pmid:27094199' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ 'id' ] == 'alaFV9OY' assert csl_item [ 'type' ] == 'article-journal' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pubmed/27094199' assert csl_item [ 'container-title' ] == 'Circulation. Cardiovascular genetics' assert csl_item [ 'container-title-short' ] == 'Circ Cardiovasc Genet' assert csl_item [ 'page' ] == '179-84' assert csl_item [ 'title' ] == 'Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits.' assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2016 , 4 ]] authors = csl_item [ 'author' ] assert authors [ 0 ][ 'given' ] == 'Casey S' assert authors [ 0 ][ 'family' ] == 'Greene' assert csl_item [ 'PMID' ] == '27094199' assert csl_item [ 'DOI' ] == '10.1161/circgenetics.115.001181' def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = 'pmid:29028984' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2018 , 3 , 15 ]] def test_citekey_to_csl_item_pubmed_book (): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" with pytest . raises ( NotImplementedError ): citekey_to_csl_item ( 'pmid:29227604' ) def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( 'isbn:9780387950693' ) assert csl_item [ 'type' ] == 'book' assert csl_item [ 'title' ] == 'Complex analysis'","title":"Module manubot.cite.tests.test_citekey_api"},{"location":"reference/manubot/cite/tests/test_citekey_api/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_arxiv","text":"def test_citekey_to_csl_item_arxiv ( ) View Source def test_citekey_to_csl_item_arxiv (): citekey = 'arxiv:cond-mat/0703470v2' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'ES92tcdg' assert csl_item [ 'URL' ] == 'https://arxiv.org/abs/cond-mat/0703470v2' assert csl_item [ 'number' ] == 'cond-mat/0703470v2' assert csl_item [ 'version' ] == '2' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'container-title' ] == 'arXiv' assert csl_item [ 'title' ] == 'Portraits of Complex Networks' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'literal' ] == 'J. P. Bagrow' assert csl_item [ 'DOI' ] == '10.1209/0295-5075/81/68004'","title":"test_citekey_to_csl_item_arxiv"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_doi_datacite","text":"def test_citekey_to_csl_item_doi_datacite ( ) View Source @pytest . mark . xfail ( reason = 'https://twitter.com/dhimmel/status/950443969313419264' ) def test_citekey_to_csl_item_doi_datacite () : citekey = 'doi:10.7287/peerj.preprints.3100v1' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == '11cb5HXoY' assert csl_item [ 'URL' ] == 'https://doi.org/10.7287/peerj.preprints.3100v1' assert csl_item [ 'DOI' ] == '10.7287/peerj.preprints.3100v1' assert csl_item [ 'type' ] == 'report' assert csl_item [ 'title' ] == 'Sci-Hub provides access to nearly all scholarly literature' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Himmelstein' assert authors [ -1 ][ 'family' ] == 'Greene'","title":"test_citekey_to_csl_item_doi_datacite"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_isbn","text":"def test_citekey_to_csl_item_isbn ( ) View Source def test_citekey_to_csl_item_isbn (): csl_item = citekey_to_csl_item ( 'isbn:9780387950693' ) assert csl_item [ 'type' ] == 'book' assert csl_item [ 'title' ] == 'Complex analysis'","title":"test_citekey_to_csl_item_isbn"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pmc","text":"def test_citekey_to_csl_item_pmc ( ) https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 View Source def test_citekey_to_csl_item_pmc (): \"\"\" https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=csl&id=3041534 \"\"\" citekey = f 'pmcid:PMC3041534' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'RoOhUFKU' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041534/' assert csl_item [ 'container-title-short' ] == 'Summit Transl Bioinform' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities' authors = csl_item [ 'author' ] assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534' assert 'generated by Manubot' in csl_item [ 'note' ] assert 'standard_id: pmcid:PMC3041534' in csl_item [ 'note' ]","title":"test_citekey_to_csl_item_pmc"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_1","text":"def test_citekey_to_csl_item_pubmed_1 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full View Source def test_citekey_to_csl_item_pubmed_1 (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=21347133&rettype=full \"\"\" citekey = 'pmid:21347133' csl_item = citekey_to_csl_item ( citekey ) assert csl_item [ 'id' ] == 'y9ONtSZ9' assert csl_item [ 'type' ] == 'article-journal' assert csl_item [ 'URL' ] == 'https://www.ncbi.nlm.nih.gov/pubmed/21347133' assert csl_item [ 'container-title' ] == 'Summit on translational bioinformatics' assert csl_item [ 'title' ] == 'Secondary Use of EHR: Data Quality Issues and Informatics Opportunities.' assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2010 , 3 , 1 ]] authors = csl_item [ 'author' ] assert authors [ 0 ][ 'given' ] == 'Taxiarchis' assert authors [ 0 ][ 'family' ] == 'Botsis' assert csl_item [ 'PMID' ] == '21347133' assert csl_item [ 'PMCID' ] == 'PMC3041534'","title":"test_citekey_to_csl_item_pubmed_1"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_2","text":"def test_citekey_to_csl_item_pubmed_2 ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=27094199&rettype=full View Source def test_citekey_to_csl_item_pubmed_2 () : \"\"\" Generated from XML returned by https : // eutils . ncbi . nlm . nih . gov / entrez / eutils / efetch . fcgi ? db = pubmed & id = 27094199 & rettype = full \"\"\" citekey = ' pmid:27094199 ' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ ' id ' ] == ' alaFV9OY ' assert csl_item [ ' type ' ] == ' article-journal ' assert csl_item [ ' URL ' ] == ' https://www.ncbi.nlm.nih.gov/pubmed/27094199 ' assert csl_item [ ' container-title ' ] == ' Circulation. Cardiovascular genetics ' assert csl_item [ ' container-title-short ' ] == ' Circ Cardiovasc Genet ' assert csl_item [ ' page ' ] == ' 179-84 ' assert csl_item [ ' title ' ] == ' Genetic Association-Guided Analysis of Gene Networks for the Study of Complex Traits. ' assert csl_item [ ' issued ' ][ ' date-parts ' ] == [[ 2016 , 4 ]] authors = csl_item [ ' author ' ] assert authors [ 0 ][ ' given ' ] == ' Casey S ' assert authors [ 0 ][ ' family ' ] == ' Greene ' assert csl_item [ ' PMID ' ] == ' 27094199 ' assert csl_item [ ' DOI ' ] == ' 10.1161/circgenetics.115.001181 '","title":"test_citekey_to_csl_item_pubmed_2"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_book","text":"def test_citekey_to_csl_item_pubmed_book ( ) Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full View Source def test_citekey_to_csl_item_pubmed_book (): \"\"\" Extracting CSL metadata from books in PubMed is not supported. Logic not implemented to parse XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29227604&rettype=full \"\"\" with pytest . raises ( NotImplementedError ): citekey_to_csl_item ( 'pmid:29227604' )","title":"test_citekey_to_csl_item_pubmed_book"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_citekey_to_csl_item_pubmed_with_numeric_month","text":"def test_citekey_to_csl_item_pubmed_with_numeric_month ( ) Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 View Source def test_citekey_to_csl_item_pubmed_with_numeric_month (): \"\"\" Generated from XML returned by https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=29028984&rettype=full See https://github.com/manubot/manubot/issues/69 \"\"\" citekey = 'pmid:29028984' csl_item = citekey_to_csl_item ( citekey ) print ( csl_item ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2018 , 3 , 15 ]]","title":"test_citekey_to_csl_item_pubmed_with_numeric_month"},{"location":"reference/manubot/cite/tests/test_citekey_api/#test_standardize_citekey","text":"def test_standardize_citekey ( citekey , expected ) Standardize identifiers based on their source View Source @pytest . mark . parametrize ( \"citekey,expected\" , [ ('doi:10.5061/DRYAD.q447c/1', 'doi:10.5061/dryad.q447c/1'), ('doi:10.5061/dryad.q447c/1', 'doi:10.5061/dryad.q447c/1'), ('doi:10/b6vnmd', 'doi:10.1016/s0933-3657(96)00367-3'), ('doi:10/B6VNMD', 'doi:10.1016/s0933-3657(96)00367-3'), ('doi:10/xxxxxxxxxxxxxYY', 'doi:10/xxxxxxxxxxxxxyy'), # passthrough non-existent shortDOI ('pmid:24159271', 'pmid:24159271'), ('isbn:1339919885', 'isbn:9781339919881'), ('isbn:1-339-91988-5', 'isbn:9781339919881'), ('isbn:978-0-387-95069-3', 'isbn:9780387950693'), ('isbn:9780387950938', 'isbn:9780387950938'), ('isbn:1-55860-510-X', 'isbn:9781558605107'), ('isbn:1-55860-510-x', 'isbn:9781558605107'), ] ) def test_standardize_citekey ( citekey , expected ) : \"\"\" Standardize identifiers based on their source \"\"\" output = standardize_citekey ( citekey ) assert output == expected","title":"test_standardize_citekey"},{"location":"reference/manubot/cite/tests/test_citeproc/","text":"Module manubot.cite.tests.test_citeproc View Source import json import pathlib import pytest from manubot.cite.citeproc import ( append_to_csl_item_note , parse_csl_item_note , remove_jsonschema_errors , ) directory = pathlib . Path ( __file__ ) . parent csl_instances = [ x . name for x in directory . glob ( 'csl-json/*-csl' ) ] def load_json ( path ): return json . loads ( path . read_text ( encoding = 'utf-8-sig' )) def test_json_is_readable_on_windows_in_different_oem_encoding (): name = 'crossref-deep-review-csl' path = directory / 'csl-json' / name / 'raw.json' content = path . read_text ( encoding = 'utf-8-sig' ) assert content json1 = load_json ( path ) assert json1 @pytest.mark.parametrize ( 'name' , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / 'csl-json' / name raw = load_json ( data_dir / 'raw.json' ) expected = load_json ( data_dir / 'pruned.json' ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected @pytest.mark.parametrize ([ 'input_note' , 'text' , 'dictionary' , 'expected_note' ], [ ( 'preexisting note' , '' , {}, 'preexisting note' ), ( 'preexisting note' , '' , { 'key' : 'the value' }, 'preexisting note \\n key: the value' ), ( '' , '' , { 'KEYOKAY' : 'the value' }, 'KEYOKAY: the value' ), ( 'preexisting note' , '' , { 'KEY-NOT-OKAY' : 'the value' }, 'preexisting note' ), ( '' , '' , { 'standard_citation' : 'doi:10.7554/elife.32822' }, 'standard_citation: doi:10.7554/elife.32822' ), ( 'This CSL Item was produced using Manubot.' , '' , { 'standard_citation' : 'doi:10.7554/elife.32822' }, 'This CSL Item was produced using Manubot. \\n standard_citation: doi:10.7554/elife.32822' ), ]) def test_append_to_csl_item_note ( input_note , text , dictionary , expected_note ): csl_item = { 'id' : 'test_csl_item' , 'type' : 'entry' , 'note' : input_note , } append_to_csl_item_note ( csl_item , text , dictionary ) output_note = csl_item [ 'note' ] assert output_note == expected_note @pytest.mark.parametrize ([ 'note' , 'dictionary' ], [ ( 'This is a note \\n key_one: value \\n KEYTWO: value 2 ' , { 'key_one' : 'value' , 'KEYTWO' : 'value 2' }), ( 'BAD_KEY: good value \\n good-key: good value' , { 'good-key' : 'good value' }), ( 'This is a note {:key_one: value} {:KEYTWO: value 2 } ' , { 'key_one' : 'value' , 'KEYTWO' : 'value 2' }), ( '{:BAD_KEY: good value} \\n {:good-key: good value}' , { 'good-key' : 'good value' }), ( 'Mixed line-entry and braced-entry syntax \\n GOODKEY: good value \\n {:good-key: good value}' , { 'GOODKEY' : 'good value' , 'good-key' : 'good value' }), ( 'Note without any key-value pairs' , {}), ( 'Other text \\n standard_citation: doi:10/ckcj \\n More other text' , { 'standard_citation' : 'doi:10/ckcj' }), ]) def test_parse_csl_item_note ( note , dictionary ): assert parse_csl_item_note ( note ) == dictionary Variables csl_instances directory Functions load_json def load_json ( path ) View Source def load_json ( path ) : return json . loads ( path . read_text ( encoding = ' utf-8-sig ' )) test_append_to_csl_item_note def test_append_to_csl_item_note ( input_note , text , dictionary , expected_note ) View Source @pytest . mark . parametrize ( [ 'input_note', 'text', 'dictionary', 'expected_note' ] , [ ('preexisting note', '', {}, 'preexisting note'), ('preexisting note', '', {'key': 'the value'}, 'preexisting note\\nkey: the value'), ('', '', {'KEYOKAY': 'the value'}, 'KEYOKAY: the value'), ('preexisting note', '', {'KEY-NOT-OKAY': 'the value'}, 'preexisting note'), ('', '', {'standard_citation': 'doi:10.7554/elife.32822'}, 'standard_citation: doi:10.7554/elife.32822'), ('This CSL Item was produced using Manubot.', '', {'standard_citation': 'doi:10.7554/elife.32822'}, 'This CSL Item was produced using Manubot.\\nstandard_citation: doi:10.7554/elife.32822'), ] ) def test_append_to_csl_item_note ( input_note , text , dictionary , expected_note ) : csl_item = { 'id' : 'test_csl_item' , 'type' : 'entry' , 'note' : input_note , } append_to_csl_item_note ( csl_item , text , dictionary ) output_note = csl_item [ 'note' ] assert output_note == expected_note test_json_is_readable_on_windows_in_different_oem_encoding def test_json_is_readable_on_windows_in_different_oem_encoding ( ) View Source def test_json_is_readable_on_windows_in_different_oem_encoding (): name = 'crossref-deep-review-csl' path = directory / 'csl-json' / name / 'raw.json' content = path . read_text ( encoding = 'utf-8-sig' ) assert content json1 = load_json ( path ) assert json1 test_parse_csl_item_note def test_parse_csl_item_note ( note , dictionary ) View Source @pytest . mark . parametrize ( [ 'note', 'dictionary' ] , [ ('This is a note\\nkey_one: value\\nKEYTWO: value 2 ', {'key_one': 'value', 'KEYTWO': 'value 2'}), ('BAD_KEY: good value\\ngood-key: good value', {'good-key': 'good value'}), ('This is a note {:key_one: value} {:KEYTWO: value 2 } ', {'key_one': 'value', 'KEYTWO': 'value 2'}), ('{:BAD_KEY: good value}\\n{:good-key: good value}', {'good-key': 'good value'}), ('Mixed line-entry and braced-entry syntax\\nGOODKEY: good value\\n{:good-key: good value}', {'GOODKEY': 'good value', 'good-key': 'good value'}), ('Note without any key-value pairs', {}), ('Other text\\nstandard_citation: doi:10/ckcj\\nMore other text', {'standard_citation': 'doi:10/ckcj'}), ] ) def test_parse_csl_item_note ( note , dictionary ) : assert parse_csl_item_note ( note ) == dictionary test_remove_jsonschema_errors def test_remove_jsonschema_errors ( name ) Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use manubot cite to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. View Source @ pytest . mark . parametrize ( ' name ' , csl_instances ) def test_remove_jsonschema_errors ( name ) : \"\"\" Tests whether CSL pruning of raw . json produces the expected pruned . json . The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically , therefore its performance should be evaluated empirically with thorough test coverage . It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested . To create a new test case , derive pruned . json from raw . json , by manually deleting any invalid fields . Do not use ` manubot cite ` to directly generate pruned . json as that also relies on remove_jsonschema_errors for pruning . \"\"\" data_dir = directory / ' csl-json ' / name raw = load_json ( data_dir / ' raw.json ' ) expected = load_json ( data_dir / ' pruned.json ' ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"Test Citeproc"},{"location":"reference/manubot/cite/tests/test_citeproc/#module-manubotciteteststest_citeproc","text":"View Source import json import pathlib import pytest from manubot.cite.citeproc import ( append_to_csl_item_note , parse_csl_item_note , remove_jsonschema_errors , ) directory = pathlib . Path ( __file__ ) . parent csl_instances = [ x . name for x in directory . glob ( 'csl-json/*-csl' ) ] def load_json ( path ): return json . loads ( path . read_text ( encoding = 'utf-8-sig' )) def test_json_is_readable_on_windows_in_different_oem_encoding (): name = 'crossref-deep-review-csl' path = directory / 'csl-json' / name / 'raw.json' content = path . read_text ( encoding = 'utf-8-sig' ) assert content json1 = load_json ( path ) assert json1 @pytest.mark.parametrize ( 'name' , csl_instances ) def test_remove_jsonschema_errors ( name ): \"\"\" Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use `manubot cite` to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. \"\"\" data_dir = directory / 'csl-json' / name raw = load_json ( data_dir / 'raw.json' ) expected = load_json ( data_dir / 'pruned.json' ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected @pytest.mark.parametrize ([ 'input_note' , 'text' , 'dictionary' , 'expected_note' ], [ ( 'preexisting note' , '' , {}, 'preexisting note' ), ( 'preexisting note' , '' , { 'key' : 'the value' }, 'preexisting note \\n key: the value' ), ( '' , '' , { 'KEYOKAY' : 'the value' }, 'KEYOKAY: the value' ), ( 'preexisting note' , '' , { 'KEY-NOT-OKAY' : 'the value' }, 'preexisting note' ), ( '' , '' , { 'standard_citation' : 'doi:10.7554/elife.32822' }, 'standard_citation: doi:10.7554/elife.32822' ), ( 'This CSL Item was produced using Manubot.' , '' , { 'standard_citation' : 'doi:10.7554/elife.32822' }, 'This CSL Item was produced using Manubot. \\n standard_citation: doi:10.7554/elife.32822' ), ]) def test_append_to_csl_item_note ( input_note , text , dictionary , expected_note ): csl_item = { 'id' : 'test_csl_item' , 'type' : 'entry' , 'note' : input_note , } append_to_csl_item_note ( csl_item , text , dictionary ) output_note = csl_item [ 'note' ] assert output_note == expected_note @pytest.mark.parametrize ([ 'note' , 'dictionary' ], [ ( 'This is a note \\n key_one: value \\n KEYTWO: value 2 ' , { 'key_one' : 'value' , 'KEYTWO' : 'value 2' }), ( 'BAD_KEY: good value \\n good-key: good value' , { 'good-key' : 'good value' }), ( 'This is a note {:key_one: value} {:KEYTWO: value 2 } ' , { 'key_one' : 'value' , 'KEYTWO' : 'value 2' }), ( '{:BAD_KEY: good value} \\n {:good-key: good value}' , { 'good-key' : 'good value' }), ( 'Mixed line-entry and braced-entry syntax \\n GOODKEY: good value \\n {:good-key: good value}' , { 'GOODKEY' : 'good value' , 'good-key' : 'good value' }), ( 'Note without any key-value pairs' , {}), ( 'Other text \\n standard_citation: doi:10/ckcj \\n More other text' , { 'standard_citation' : 'doi:10/ckcj' }), ]) def test_parse_csl_item_note ( note , dictionary ): assert parse_csl_item_note ( note ) == dictionary","title":"Module manubot.cite.tests.test_citeproc"},{"location":"reference/manubot/cite/tests/test_citeproc/#variables","text":"csl_instances directory","title":"Variables"},{"location":"reference/manubot/cite/tests/test_citeproc/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_citeproc/#load_json","text":"def load_json ( path ) View Source def load_json ( path ) : return json . loads ( path . read_text ( encoding = ' utf-8-sig ' ))","title":"load_json"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_append_to_csl_item_note","text":"def test_append_to_csl_item_note ( input_note , text , dictionary , expected_note ) View Source @pytest . mark . parametrize ( [ 'input_note', 'text', 'dictionary', 'expected_note' ] , [ ('preexisting note', '', {}, 'preexisting note'), ('preexisting note', '', {'key': 'the value'}, 'preexisting note\\nkey: the value'), ('', '', {'KEYOKAY': 'the value'}, 'KEYOKAY: the value'), ('preexisting note', '', {'KEY-NOT-OKAY': 'the value'}, 'preexisting note'), ('', '', {'standard_citation': 'doi:10.7554/elife.32822'}, 'standard_citation: doi:10.7554/elife.32822'), ('This CSL Item was produced using Manubot.', '', {'standard_citation': 'doi:10.7554/elife.32822'}, 'This CSL Item was produced using Manubot.\\nstandard_citation: doi:10.7554/elife.32822'), ] ) def test_append_to_csl_item_note ( input_note , text , dictionary , expected_note ) : csl_item = { 'id' : 'test_csl_item' , 'type' : 'entry' , 'note' : input_note , } append_to_csl_item_note ( csl_item , text , dictionary ) output_note = csl_item [ 'note' ] assert output_note == expected_note","title":"test_append_to_csl_item_note"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_json_is_readable_on_windows_in_different_oem_encoding","text":"def test_json_is_readable_on_windows_in_different_oem_encoding ( ) View Source def test_json_is_readable_on_windows_in_different_oem_encoding (): name = 'crossref-deep-review-csl' path = directory / 'csl-json' / name / 'raw.json' content = path . read_text ( encoding = 'utf-8-sig' ) assert content json1 = load_json ( path ) assert json1","title":"test_json_is_readable_on_windows_in_different_oem_encoding"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_parse_csl_item_note","text":"def test_parse_csl_item_note ( note , dictionary ) View Source @pytest . mark . parametrize ( [ 'note', 'dictionary' ] , [ ('This is a note\\nkey_one: value\\nKEYTWO: value 2 ', {'key_one': 'value', 'KEYTWO': 'value 2'}), ('BAD_KEY: good value\\ngood-key: good value', {'good-key': 'good value'}), ('This is a note {:key_one: value} {:KEYTWO: value 2 } ', {'key_one': 'value', 'KEYTWO': 'value 2'}), ('{:BAD_KEY: good value}\\n{:good-key: good value}', {'good-key': 'good value'}), ('Mixed line-entry and braced-entry syntax\\nGOODKEY: good value\\n{:good-key: good value}', {'GOODKEY': 'good value', 'good-key': 'good value'}), ('Note without any key-value pairs', {}), ('Other text\\nstandard_citation: doi:10/ckcj\\nMore other text', {'standard_citation': 'doi:10/ckcj'}), ] ) def test_parse_csl_item_note ( note , dictionary ) : assert parse_csl_item_note ( note ) == dictionary","title":"test_parse_csl_item_note"},{"location":"reference/manubot/cite/tests/test_citeproc/#test_remove_jsonschema_errors","text":"def test_remove_jsonschema_errors ( name ) Tests whether CSL pruning of raw.json produces the expected pruned.json. The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically, therefore its performance should be evaluated empirically with thorough test coverage. It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested. To create a new test case, derive pruned.json from raw.json, by manually deleting any invalid fields. Do not use manubot cite to directly generate pruned.json as that also relies on remove_jsonschema_errors for pruning. View Source @ pytest . mark . parametrize ( ' name ' , csl_instances ) def test_remove_jsonschema_errors ( name ) : \"\"\" Tests whether CSL pruning of raw . json produces the expected pruned . json . The fidelity of the remove_jsonschema_errors implementation is difficult to assess theoretically , therefore its performance should be evaluated empirically with thorough test coverage . It is recommended that all conceivable conformations of invalid CSL that citeproc methods may generate be tested . To create a new test case , derive pruned . json from raw . json , by manually deleting any invalid fields . Do not use ` manubot cite ` to directly generate pruned . json as that also relies on remove_jsonschema_errors for pruning . \"\"\" data_dir = directory / ' csl-json ' / name raw = load_json ( data_dir / ' raw.json ' ) expected = load_json ( data_dir / ' pruned.json ' ) pruned = remove_jsonschema_errors ( raw ) assert pruned == expected","title":"test_remove_jsonschema_errors"},{"location":"reference/manubot/cite/tests/test_csl_item/","text":"Module manubot.cite.tests.test_csl_item View Source import copy import pytest from manubot.cite.csl_item import ( csl_item_set_standard_id ) @pytest.mark.parametrize ( [ 'csl_item' , 'standard_citation' ], [ ( { 'id' : 'my-id' , 'standard_citation' : 'doi:10.7554/elife.32822' }, 'doi:10.7554/elife.32822' , ), ( { 'id' : 'doi:10.7554/elife.32822' }, 'doi:10.7554/elife.32822' , ), ( { 'id' : 'doi:10.7554/ELIFE.32822' }, 'doi:10.7554/elife.32822' , ), ( { 'id' : 'my-id' }, 'raw:my-id' , ), ], ids = [ 'from_standard_citation' , 'from_doi_id' , 'from_doi_id_standardize' , 'from_raw_id' , ] ) def test_csl_item_set_standard_id ( csl_item , standard_citation ): output = csl_item_set_standard_id ( csl_item ) assert output is csl_item assert output [ 'id' ] == standard_citation def test_csl_item_set_standard_id_repeated (): csl_item = { 'id' : 'pmid:1' , 'type' : 'article-journal' , } # csl_item_0 = copy.deepcopy(csl_item) csl_item_1 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert 'standard_citation' not in 'csl_item' csl_item_2 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert csl_item_1 == csl_item_2 def test_csl_item_set_standard_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = { 'id' : 'original-id' , 'type' : 'article-journal' , 'note' : 'standard_id: doi:10.1371/journal.PPAT.1006256' , } csl_item_set_standard_id ( csl_item ) assert csl_item [ 'id' ] == 'doi:10.1371/journal.ppat.1006256' from manubot.cite.citeproc import parse_csl_item_note note_dict = parse_csl_item_note ( csl_item [ 'note' ]) assert note_dict [ 'original_id' ] == 'original-id' assert note_dict [ 'original_standard_id' ] == 'doi:10.1371/journal.PPAT.1006256' Functions test_csl_item_set_standard_id def test_csl_item_set_standard_id ( csl_item , standard_citation ) View Source @pytest . mark . parametrize ( [ 'csl_item', 'standard_citation' ] , [ ( {'id': 'my-id', 'standard_citation': 'doi:10.7554/elife.32822'}, 'doi:10.7554/elife.32822', ), ( {'id': 'doi:10.7554/elife.32822'}, 'doi:10.7554/elife.32822', ), ( {'id': 'doi:10.7554/ELIFE.32822'}, 'doi:10.7554/elife.32822', ), ( {'id': 'my-id'}, 'raw:my-id', ), ] , ids =[ 'from_standard_citation', 'from_doi_id', 'from_doi_id_standardize', 'from_raw_id', ] ) def test_csl_item_set_standard_id ( csl_item , standard_citation ) : output = csl_item_set_standard_id ( csl_item ) assert output is csl_item assert output [ 'id' ] == standard_citation test_csl_item_set_standard_id_note def test_csl_item_set_standard_id_note ( ) Test extracting standard_id from a note and setting additional note fields. View Source def test_csl_item_set_standard_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = { 'id' : 'original-id' , 'type' : 'article-journal' , 'note' : 'standard_id: doi:10.1371/journal.PPAT.1006256' , } csl_item_set_standard_id ( csl_item ) assert csl_item [ 'id' ] == 'doi:10.1371/journal.ppat.1006256' from manubot.cite.citeproc import parse_csl_item_note note_dict = parse_csl_item_note ( csl_item [ 'note' ]) assert note_dict [ 'original_id' ] == 'original-id' assert note_dict [ 'original_standard_id' ] == 'doi:10.1371/journal.PPAT.1006256' test_csl_item_set_standard_id_repeated def test_csl_item_set_standard_id_repeated ( ) View Source def test_csl_item_set_standard_id_repeated (): csl_item = { 'id' : 'pmid:1' , 'type' : 'article-journal' , } # csl_item_0 = copy . deepcopy ( csl_item ) csl_item_1 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert 'standard_citation' not in 'csl_item' csl_item_2 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert csl_item_1 == csl_item_2","title":"Test Csl Item"},{"location":"reference/manubot/cite/tests/test_csl_item/#module-manubotciteteststest_csl_item","text":"View Source import copy import pytest from manubot.cite.csl_item import ( csl_item_set_standard_id ) @pytest.mark.parametrize ( [ 'csl_item' , 'standard_citation' ], [ ( { 'id' : 'my-id' , 'standard_citation' : 'doi:10.7554/elife.32822' }, 'doi:10.7554/elife.32822' , ), ( { 'id' : 'doi:10.7554/elife.32822' }, 'doi:10.7554/elife.32822' , ), ( { 'id' : 'doi:10.7554/ELIFE.32822' }, 'doi:10.7554/elife.32822' , ), ( { 'id' : 'my-id' }, 'raw:my-id' , ), ], ids = [ 'from_standard_citation' , 'from_doi_id' , 'from_doi_id_standardize' , 'from_raw_id' , ] ) def test_csl_item_set_standard_id ( csl_item , standard_citation ): output = csl_item_set_standard_id ( csl_item ) assert output is csl_item assert output [ 'id' ] == standard_citation def test_csl_item_set_standard_id_repeated (): csl_item = { 'id' : 'pmid:1' , 'type' : 'article-journal' , } # csl_item_0 = copy.deepcopy(csl_item) csl_item_1 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert 'standard_citation' not in 'csl_item' csl_item_2 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert csl_item_1 == csl_item_2 def test_csl_item_set_standard_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = { 'id' : 'original-id' , 'type' : 'article-journal' , 'note' : 'standard_id: doi:10.1371/journal.PPAT.1006256' , } csl_item_set_standard_id ( csl_item ) assert csl_item [ 'id' ] == 'doi:10.1371/journal.ppat.1006256' from manubot.cite.citeproc import parse_csl_item_note note_dict = parse_csl_item_note ( csl_item [ 'note' ]) assert note_dict [ 'original_id' ] == 'original-id' assert note_dict [ 'original_standard_id' ] == 'doi:10.1371/journal.PPAT.1006256'","title":"Module manubot.cite.tests.test_csl_item"},{"location":"reference/manubot/cite/tests/test_csl_item/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_set_standard_id","text":"def test_csl_item_set_standard_id ( csl_item , standard_citation ) View Source @pytest . mark . parametrize ( [ 'csl_item', 'standard_citation' ] , [ ( {'id': 'my-id', 'standard_citation': 'doi:10.7554/elife.32822'}, 'doi:10.7554/elife.32822', ), ( {'id': 'doi:10.7554/elife.32822'}, 'doi:10.7554/elife.32822', ), ( {'id': 'doi:10.7554/ELIFE.32822'}, 'doi:10.7554/elife.32822', ), ( {'id': 'my-id'}, 'raw:my-id', ), ] , ids =[ 'from_standard_citation', 'from_doi_id', 'from_doi_id_standardize', 'from_raw_id', ] ) def test_csl_item_set_standard_id ( csl_item , standard_citation ) : output = csl_item_set_standard_id ( csl_item ) assert output is csl_item assert output [ 'id' ] == standard_citation","title":"test_csl_item_set_standard_id"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_set_standard_id_note","text":"def test_csl_item_set_standard_id_note ( ) Test extracting standard_id from a note and setting additional note fields. View Source def test_csl_item_set_standard_id_note (): \"\"\" Test extracting standard_id from a note and setting additional note fields. \"\"\" csl_item = { 'id' : 'original-id' , 'type' : 'article-journal' , 'note' : 'standard_id: doi:10.1371/journal.PPAT.1006256' , } csl_item_set_standard_id ( csl_item ) assert csl_item [ 'id' ] == 'doi:10.1371/journal.ppat.1006256' from manubot.cite.citeproc import parse_csl_item_note note_dict = parse_csl_item_note ( csl_item [ 'note' ]) assert note_dict [ 'original_id' ] == 'original-id' assert note_dict [ 'original_standard_id' ] == 'doi:10.1371/journal.PPAT.1006256'","title":"test_csl_item_set_standard_id_note"},{"location":"reference/manubot/cite/tests/test_csl_item/#test_csl_item_set_standard_id_repeated","text":"def test_csl_item_set_standard_id_repeated ( ) View Source def test_csl_item_set_standard_id_repeated (): csl_item = { 'id' : 'pmid:1' , 'type' : 'article-journal' , } # csl_item_0 = copy . deepcopy ( csl_item ) csl_item_1 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert 'standard_citation' not in 'csl_item' csl_item_2 = copy . deepcopy ( csl_item_set_standard_id ( csl_item )) assert csl_item_1 == csl_item_2","title":"test_csl_item_set_standard_id_repeated"},{"location":"reference/manubot/cite/tests/test_doi/","text":"Module manubot.cite.tests.test_doi View Source import pytest from manubot.cite.doi import ( expand_short_doi , ) def test_expand_short_doi (): doi = expand_short_doi ( '10/b6vnmd' ) assert doi == \"10.1016/s0933-3657(96)00367-3\" def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = 'Handle not found. Double check short_doi' ): expand_short_doi ( '10/b6vnmdxxxxxx' ) def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = 'shortDOIs start with `10/`' ): expand_short_doi ( '10.1016/S0933-3657(96)00367-3' ) Functions test_expand_short_doi def test_expand_short_doi ( ) View Source def test_expand_short_doi (): doi = expand_short_doi ( '10/b6vnmd' ) assert doi == \"10.1016/s0933-3657(96)00367-3\" test_expand_short_doi_invalid def test_expand_short_doi_invalid ( ) View Source def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = 'Handle not found. Double check short_doi' ): expand_short_doi ( '10/b6vnmdxxxxxx' ) test_expand_short_doi_not_short def test_expand_short_doi_not_short ( ) View Source def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = 'shortDOIs start with `10/`' ): expand_short_doi ( '10.1016/S0933-3657(96)00367-3' )","title":"Test Doi"},{"location":"reference/manubot/cite/tests/test_doi/#module-manubotciteteststest_doi","text":"View Source import pytest from manubot.cite.doi import ( expand_short_doi , ) def test_expand_short_doi (): doi = expand_short_doi ( '10/b6vnmd' ) assert doi == \"10.1016/s0933-3657(96)00367-3\" def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = 'Handle not found. Double check short_doi' ): expand_short_doi ( '10/b6vnmdxxxxxx' ) def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = 'shortDOIs start with `10/`' ): expand_short_doi ( '10.1016/S0933-3657(96)00367-3' )","title":"Module manubot.cite.tests.test_doi"},{"location":"reference/manubot/cite/tests/test_doi/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi","text":"def test_expand_short_doi ( ) View Source def test_expand_short_doi (): doi = expand_short_doi ( '10/b6vnmd' ) assert doi == \"10.1016/s0933-3657(96)00367-3\"","title":"test_expand_short_doi"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi_invalid","text":"def test_expand_short_doi_invalid ( ) View Source def test_expand_short_doi_invalid (): with pytest . raises ( ValueError , match = 'Handle not found. Double check short_doi' ): expand_short_doi ( '10/b6vnmdxxxxxx' )","title":"test_expand_short_doi_invalid"},{"location":"reference/manubot/cite/tests/test_doi/#test_expand_short_doi_not_short","text":"def test_expand_short_doi_not_short ( ) View Source def test_expand_short_doi_not_short (): with pytest . raises ( ValueError , match = 'shortDOIs start with `10/`' ): expand_short_doi ( '10.1016/S0933-3657(96)00367-3' )","title":"test_expand_short_doi_not_short"},{"location":"reference/manubot/cite/tests/test_isbn/","text":"Module manubot.cite.tests.test_isbn View Source import pytest from manubot.cite.isbn import ( get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , get_isbn_csl_item_zotero , ) @pytest.mark.xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark (): csl_item = get_isbn_csl_item_isbnlib ( '9780312353780' ) assert csl_item [ 'type' ] == 'book' assert csl_item [ 'title' ] . startswith ( '\"F\" is for Fugitive' ) def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( '9780719561023' ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2004 ]] assert csl_item [ 'ISBN' ] == '9780719561023' def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r 'Metadata for ISBN [0-9]{10,13} not found' ): get_isbn_csl_item_citoid ( '9781439566039' ) def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = '9780262517638' csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ 'author' ][ 0 ][ 'family' ] == 'Suber' Functions test_citekey_to_csl_item_isbnlib_title_with_quotation_mark def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark ( ) View Source @ pytest . mark . xfail ( reason = \" Quotation in title removed at some upstream point \" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark () : csl_item = get_isbn_csl_item_isbnlib ( ' 9780312353780 ' ) assert csl_item [ ' type ' ] == ' book ' assert csl_item [ ' title ' ]. startswith ( ' \"F\" is for Fugitive ' ) test_get_isbn_csl_item_citoid_not_found def test_get_isbn_csl_item_citoid_not_found ( ) isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 View Source def test_get_isbn_csl_item_citoid_not_found () : \"\"\" isbn : 9781439566039 is not found by Citoid : https : // en . wikipedia . org / api / rest_v1 / data / citation / mediawiki / 9781439566039 \"\"\" with pytest . raises ( KeyError , match = r ' Metadata for ISBN [0-9]{10,13} not found ' ) : get_isbn_csl_item_citoid ( ' 9781439566039 ' ) test_get_isbn_csl_item_citoid_weird_date def test_get_isbn_csl_item_citoid_weird_date ( ) isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 View Source def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \" ( 2004 printing ) \" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( '9780719561023' ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2004 ]] assert csl_item [ 'ISBN' ] == '9780719561023' test_get_isbn_csl_item_zotero_with_note_issue def test_get_isbn_csl_item_zotero_with_note_issue ( ) translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 View Source def test_get_isbn_csl_item_zotero_with_note_issue () : \"\"\" translation - server returns two metadata records for this ISBN . The second has itemType = note and previously caused CSL export to fail . https : // github . com / zotero / translation - server / issues / 67 \"\"\" isbn = ' 9780262517638 ' csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ ' author ' ][ 0 ][ ' family ' ] == ' Suber '","title":"Test Isbn"},{"location":"reference/manubot/cite/tests/test_isbn/#module-manubotciteteststest_isbn","text":"View Source import pytest from manubot.cite.isbn import ( get_isbn_csl_item_citoid , get_isbn_csl_item_isbnlib , get_isbn_csl_item_zotero , ) @pytest.mark.xfail ( reason = \"Quotation in title removed at some upstream point\" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark (): csl_item = get_isbn_csl_item_isbnlib ( '9780312353780' ) assert csl_item [ 'type' ] == 'book' assert csl_item [ 'title' ] . startswith ( '\"F\" is for Fugitive' ) def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( '9780719561023' ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2004 ]] assert csl_item [ 'ISBN' ] == '9780719561023' def test_get_isbn_csl_item_citoid_not_found (): \"\"\" isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 \"\"\" with pytest . raises ( KeyError , match = r 'Metadata for ISBN [0-9]{10,13} not found' ): get_isbn_csl_item_citoid ( '9781439566039' ) def test_get_isbn_csl_item_zotero_with_note_issue (): \"\"\" translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 \"\"\" isbn = '9780262517638' csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ 'author' ][ 0 ][ 'family' ] == 'Suber'","title":"Module manubot.cite.tests.test_isbn"},{"location":"reference/manubot/cite/tests/test_isbn/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_isbn/#test_citekey_to_csl_item_isbnlib_title_with_quotation_mark","text":"def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark ( ) View Source @ pytest . mark . xfail ( reason = \" Quotation in title removed at some upstream point \" ) def test_citekey_to_csl_item_isbnlib_title_with_quotation_mark () : csl_item = get_isbn_csl_item_isbnlib ( ' 9780312353780 ' ) assert csl_item [ ' type ' ] == ' book ' assert csl_item [ ' title ' ]. startswith ( ' \"F\" is for Fugitive ' )","title":"test_citekey_to_csl_item_isbnlib_title_with_quotation_mark"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_citoid_not_found","text":"def test_get_isbn_csl_item_citoid_not_found ( ) isbn:9781439566039 is not found by Citoid: https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9781439566039 View Source def test_get_isbn_csl_item_citoid_not_found () : \"\"\" isbn : 9781439566039 is not found by Citoid : https : // en . wikipedia . org / api / rest_v1 / data / citation / mediawiki / 9781439566039 \"\"\" with pytest . raises ( KeyError , match = r ' Metadata for ISBN [0-9]{10,13} not found ' ) : get_isbn_csl_item_citoid ( ' 9781439566039 ' )","title":"test_get_isbn_csl_item_citoid_not_found"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_citoid_weird_date","text":"def test_get_isbn_csl_item_citoid_weird_date ( ) isbn:9780719561023 has a date value of \"(2004 printing)\" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 View Source def test_get_isbn_csl_item_citoid_weird_date (): \"\"\" isbn:9780719561023 has a date value of \" ( 2004 printing ) \" https://en.wikipedia.org/api/rest_v1/data/citation/mediawiki/9780719561023 \"\"\" csl_item = get_isbn_csl_item_citoid ( '9780719561023' ) assert csl_item [ 'issued' ][ 'date-parts' ] == [[ 2004 ]] assert csl_item [ 'ISBN' ] == '9780719561023'","title":"test_get_isbn_csl_item_citoid_weird_date"},{"location":"reference/manubot/cite/tests/test_isbn/#test_get_isbn_csl_item_zotero_with_note_issue","text":"def test_get_isbn_csl_item_zotero_with_note_issue ( ) translation-server returns two metadata records for this ISBN. The second has itemType=note and previously caused CSL export to fail. https://github.com/zotero/translation-server/issues/67 View Source def test_get_isbn_csl_item_zotero_with_note_issue () : \"\"\" translation - server returns two metadata records for this ISBN . The second has itemType = note and previously caused CSL export to fail . https : // github . com / zotero / translation - server / issues / 67 \"\"\" isbn = ' 9780262517638 ' csl_item = get_isbn_csl_item_zotero ( isbn ) assert csl_item [ ' author ' ][ 0 ][ ' family ' ] == ' Suber '","title":"test_get_isbn_csl_item_zotero_with_note_issue"},{"location":"reference/manubot/cite/tests/test_pubmed/","text":"Module manubot.cite.tests.test_pubmed View Source import pytest from manubot.cite.pubmed import ( get_pmcid_and_pmid_for_doi , get_pmid_for_doi , get_pubmed_ids_for_doi , ) @pytest.mark.parametrize (( 'doi' , 'pmid' ), [ ( '10.1098/rsif.2017.0387' , '29618526' ), # in PubMed and PMC ( '10.1161/CIRCGENETICS.115.001181' , '27094199' ), # in PubMed but not PMC ( '10.7717/peerj-cs.134' , None ), # DOI in journal not indexed by PubMed ( '10.1161/CIRC' , None ), # invalid DOI ]) def test_get_pmid_for_doi ( doi , pmid ): output = get_pmid_for_doi ( doi ) assert pmid == output @pytest.mark.parametrize (( 'doi' , 'id_dict' ), [ ( '10.1098/rsif.2017.0387' , { 'PMCID' : 'PMC5938574' , 'PMID' : '29618526' }), ( '10.7554/ELIFE.32822' , { 'PMCID' : 'PMC5832410' , 'PMID' : '29424689' }), ( '10.1161/CIRCGENETICS.115.001181' , {}), # only in PubMed, not in PMC ( '10.7717/peerj.000' , {}), # Non-existent DOI ( '10.peerj.000' , {}), # malformed DOI ]) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ): output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output @pytest.mark.parametrize (( 'doi' , 'id_dict' ), [ ( '10.1098/rsif.2017.0387' , { 'PMCID' : 'PMC5938574' , 'PMID' : '29618526' }), ( '10.7554/ELIFE.32822' , { 'PMCID' : 'PMC5832410' , 'PMID' : '29424689' }), ( '10.1161/CIRCGENETICS.115.001181' , { 'PMID' : '27094199' }), # only in PubMed, not in PMC ( '10.7717/peerj.000' , {}), # Non-existent DOI ]) def test_get_pubmed_ids_for_doi ( doi , id_dict ): output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output Functions test_get_pmcid_and_pmid_for_doi def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize (( 'doi' , 'id_dict' ), [ ('10.1098/rsif.2017.0387', {'PMCID': 'PMC5938574', 'PMID': '29618526'}), ('10.7554/ELIFE.32822', {'PMCID': 'PMC5832410', 'PMID': '29424689'}), ('10.1161/CIRCGENETICS.115.001181', {}), # only in PubMed, not in PMC ('10.7717/peerj.000', {}), # Non-existent DOI ('10.peerj.000', {}), # malformed DOI ] ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) : output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output test_get_pmid_for_doi def test_get_pmid_for_doi ( doi , pmid ) View Source @pytest . mark . parametrize (( 'doi' , 'pmid' ), [ ('10.1098/rsif.2017.0387', '29618526'), # in PubMed and PMC ('10.1161/CIRCGENETICS.115.001181', '27094199'), # in PubMed but not PMC ('10.7717/peerj-cs.134', None), # DOI in journal not indexed by PubMed ('10.1161/CIRC', None), # invalid DOI ] ) def test_get_pmid_for_doi ( doi , pmid ) : output = get_pmid_for_doi ( doi ) assert pmid == output test_get_pubmed_ids_for_doi def test_get_pubmed_ids_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize (( 'doi' , 'id_dict' ), [ ('10.1098/rsif.2017.0387', {'PMCID': 'PMC5938574', 'PMID': '29618526'}), ('10.7554/ELIFE.32822', {'PMCID': 'PMC5832410', 'PMID': '29424689'}), ('10.1161/CIRCGENETICS.115.001181', {'PMID': '27094199'}), # only in PubMed, not in PMC ('10.7717/peerj.000', {}), # Non-existent DOI ] ) def test_get_pubmed_ids_for_doi ( doi , id_dict ) : output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"Test Pubmed"},{"location":"reference/manubot/cite/tests/test_pubmed/#module-manubotciteteststest_pubmed","text":"View Source import pytest from manubot.cite.pubmed import ( get_pmcid_and_pmid_for_doi , get_pmid_for_doi , get_pubmed_ids_for_doi , ) @pytest.mark.parametrize (( 'doi' , 'pmid' ), [ ( '10.1098/rsif.2017.0387' , '29618526' ), # in PubMed and PMC ( '10.1161/CIRCGENETICS.115.001181' , '27094199' ), # in PubMed but not PMC ( '10.7717/peerj-cs.134' , None ), # DOI in journal not indexed by PubMed ( '10.1161/CIRC' , None ), # invalid DOI ]) def test_get_pmid_for_doi ( doi , pmid ): output = get_pmid_for_doi ( doi ) assert pmid == output @pytest.mark.parametrize (( 'doi' , 'id_dict' ), [ ( '10.1098/rsif.2017.0387' , { 'PMCID' : 'PMC5938574' , 'PMID' : '29618526' }), ( '10.7554/ELIFE.32822' , { 'PMCID' : 'PMC5832410' , 'PMID' : '29424689' }), ( '10.1161/CIRCGENETICS.115.001181' , {}), # only in PubMed, not in PMC ( '10.7717/peerj.000' , {}), # Non-existent DOI ( '10.peerj.000' , {}), # malformed DOI ]) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ): output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output @pytest.mark.parametrize (( 'doi' , 'id_dict' ), [ ( '10.1098/rsif.2017.0387' , { 'PMCID' : 'PMC5938574' , 'PMID' : '29618526' }), ( '10.7554/ELIFE.32822' , { 'PMCID' : 'PMC5832410' , 'PMID' : '29424689' }), ( '10.1161/CIRCGENETICS.115.001181' , { 'PMID' : '27094199' }), # only in PubMed, not in PMC ( '10.7717/peerj.000' , {}), # Non-existent DOI ]) def test_get_pubmed_ids_for_doi ( doi , id_dict ): output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"Module manubot.cite.tests.test_pubmed"},{"location":"reference/manubot/cite/tests/test_pubmed/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pmcid_and_pmid_for_doi","text":"def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize (( 'doi' , 'id_dict' ), [ ('10.1098/rsif.2017.0387', {'PMCID': 'PMC5938574', 'PMID': '29618526'}), ('10.7554/ELIFE.32822', {'PMCID': 'PMC5832410', 'PMID': '29424689'}), ('10.1161/CIRCGENETICS.115.001181', {}), # only in PubMed, not in PMC ('10.7717/peerj.000', {}), # Non-existent DOI ('10.peerj.000', {}), # malformed DOI ] ) def test_get_pmcid_and_pmid_for_doi ( doi , id_dict ) : output = get_pmcid_and_pmid_for_doi ( doi ) assert id_dict == output","title":"test_get_pmcid_and_pmid_for_doi"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pmid_for_doi","text":"def test_get_pmid_for_doi ( doi , pmid ) View Source @pytest . mark . parametrize (( 'doi' , 'pmid' ), [ ('10.1098/rsif.2017.0387', '29618526'), # in PubMed and PMC ('10.1161/CIRCGENETICS.115.001181', '27094199'), # in PubMed but not PMC ('10.7717/peerj-cs.134', None), # DOI in journal not indexed by PubMed ('10.1161/CIRC', None), # invalid DOI ] ) def test_get_pmid_for_doi ( doi , pmid ) : output = get_pmid_for_doi ( doi ) assert pmid == output","title":"test_get_pmid_for_doi"},{"location":"reference/manubot/cite/tests/test_pubmed/#test_get_pubmed_ids_for_doi","text":"def test_get_pubmed_ids_for_doi ( doi , id_dict ) View Source @pytest . mark . parametrize (( 'doi' , 'id_dict' ), [ ('10.1098/rsif.2017.0387', {'PMCID': 'PMC5938574', 'PMID': '29618526'}), ('10.7554/ELIFE.32822', {'PMCID': 'PMC5832410', 'PMID': '29424689'}), ('10.1161/CIRCGENETICS.115.001181', {'PMID': '27094199'}), # only in PubMed, not in PMC ('10.7717/peerj.000', {}), # Non-existent DOI ] ) def test_get_pubmed_ids_for_doi ( doi , id_dict ) : output = get_pubmed_ids_for_doi ( doi ) assert id_dict == output","title":"test_get_pubmed_ids_for_doi"},{"location":"reference/manubot/cite/tests/test_url/","text":"Module manubot.cite.tests.test_url View Source from manubot.cite.url import get_url_csl_item_zotero def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = 'https://nyti.ms/1NuB0WJ' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ 'title' ] == 'Unraveling the Ties of Altitude, Oxygen and Lung Cancer' assert csl_item [ 'author' ][ 0 ][ 'family' ] == 'Johnson' def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ 'title' ] == 'Open collaborative writing with Manubot' assert csl_item [ 'author' ][ 1 ][ 'family' ] == 'Slochower' # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ 'issued' ] [ 'date-parts' ][ 0 ]] == [ 2018 , 12 , 18 ] def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitraryly, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ 'title' ] . startswith ( 'Flexible and powerful data analysis' ) assert csl_item [ 'source' ] == 'GitHub' Functions test_get_url_csl_item_zotero_github def test_get_url_csl_item_zotero_github ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' 'https://translate.manubot.org/web' Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. View Source def test_get_url_csl_item_zotero_github () : \"\"\" This command creates two translation - server queries . The first query is equivalent to : ``` curl -- verbose \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927 ' \\ ' https://translate.manubot.org/web ' ``` Note : this test may have temporary failures , due to performance of translation - server . It seems that sometimes translation - server returns a different title for the same URL . A real mystery . See also : https : // github . com / manubot / manubot / pull / 139 # discussion_r328703233 Proposed action : Probably should inquire upstream or change the test . \"\"\" url = ' https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927 ' csl_item = get_url_csl_item_zotero ( url ) # FIXME : arbitraryly , csl_item [ ' abstract ' ], and not csl_item [ ' title ' ] contains the title . assert csl_item [ ' title ' ]. startswith ( ' Flexible and powerful data analysis ' ) assert csl_item [ ' source ' ] == ' GitHub ' test_get_url_csl_item_zotero_manubot def test_get_url_csl_item_zotero_manubot ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_manubot () : \"\"\" This command creates two translation - server queries . The first query is equivalent to : ``` curl -- verbose \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/ ' \\ ' https://translate.manubot.org/web ' ``` \"\"\" url = ' https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/ ' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ ' title ' ] == ' Open collaborative writing with Manubot ' assert csl_item [ ' author ' ][ 1 ][ ' family ' ] == ' Slochower ' # Zotero CSL exporter returns mixed string / int date - parts # https : // github . com / zotero / zotero / issues / 1603 assert [ int ( x ) for x in csl_item [ ' issued ' ] [ ' date-parts ' ][ 0 ]] == [ 2018 , 12 , 18 ] test_get_url_csl_item_zotero_nyt def test_get_url_csl_item_zotero_nyt ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://nyti.ms/1NuB0WJ' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = 'https://nyti.ms/1NuB0WJ' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ 'title' ] == 'Unraveling the Ties of Altitude, Oxygen and Lung Cancer' assert csl_item [ 'author' ][ 0 ][ 'family' ] == 'Johnson'","title":"Test Url"},{"location":"reference/manubot/cite/tests/test_url/#module-manubotciteteststest_url","text":"View Source from manubot.cite.url import get_url_csl_item_zotero def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = 'https://nyti.ms/1NuB0WJ' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ 'title' ] == 'Unraveling the Ties of Altitude, Oxygen and Lung Cancer' assert csl_item [ 'author' ][ 0 ][ 'family' ] == 'Johnson' def test_get_url_csl_item_zotero_manubot (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ 'title' ] == 'Open collaborative writing with Manubot' assert csl_item [ 'author' ][ 1 ][ 'family' ] == 'Slochower' # Zotero CSL exporter returns mixed string/int date-parts # https://github.com/zotero/zotero/issues/1603 assert [ int ( x ) for x in csl_item [ 'issued' ] [ 'date-parts' ][ 0 ]] == [ 2018 , 12 , 18 ] def test_get_url_csl_item_zotero_github (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' \\ 'https://translate.manubot.org/web' ``` Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. \"\"\" url = 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' csl_item = get_url_csl_item_zotero ( url ) # FIXME: arbitraryly, csl_item['abstract'], and not csl_item['title'] contains the title. assert csl_item [ 'title' ] . startswith ( 'Flexible and powerful data analysis' ) assert csl_item [ 'source' ] == 'GitHub'","title":"Module manubot.cite.tests.test_url"},{"location":"reference/manubot/cite/tests/test_url/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_github","text":"def test_get_url_csl_item_zotero_github ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927' 'https://translate.manubot.org/web' Note: this test may have temporary failures, due to performance of translation-server. It seems that sometimes translation-server returns a different title for the same URL. A real mystery. See also: https://github.com/manubot/manubot/pull/139#discussion_r328703233 Proposed action: Probably should inquire upstream or change the test. View Source def test_get_url_csl_item_zotero_github () : \"\"\" This command creates two translation - server queries . The first query is equivalent to : ``` curl -- verbose \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927 ' \\ ' https://translate.manubot.org/web ' ``` Note : this test may have temporary failures , due to performance of translation - server . It seems that sometimes translation - server returns a different title for the same URL . A real mystery . See also : https : // github . com / manubot / manubot / pull / 139 # discussion_r328703233 Proposed action : Probably should inquire upstream or change the test . \"\"\" url = ' https://github.com/pandas-dev/pandas/tree/d5e5bf761092c59eeb9b8750f05f2bc29fb45927 ' csl_item = get_url_csl_item_zotero ( url ) # FIXME : arbitraryly , csl_item [ ' abstract ' ], and not csl_item [ ' title ' ] contains the title . assert csl_item [ ' title ' ]. startswith ( ' Flexible and powerful data analysis ' ) assert csl_item [ ' source ' ] == ' GitHub '","title":"test_get_url_csl_item_zotero_github"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_manubot","text":"def test_get_url_csl_item_zotero_manubot ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_manubot () : \"\"\" This command creates two translation - server queries . The first query is equivalent to : ``` curl -- verbose \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/ ' \\ ' https://translate.manubot.org/web ' ``` \"\"\" url = ' https://greenelab.github.io/meta-review/v/0770300e1d5490a1ae8ff3a85ddca2cdc4ae0613/ ' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ ' title ' ] == ' Open collaborative writing with Manubot ' assert csl_item [ ' author ' ][ 1 ][ ' family ' ] == ' Slochower ' # Zotero CSL exporter returns mixed string / int date - parts # https : // github . com / zotero / zotero / issues / 1603 assert [ int ( x ) for x in csl_item [ ' issued ' ] [ ' date-parts ' ][ 0 ]] == [ 2018 , 12 , 18 ]","title":"test_get_url_csl_item_zotero_manubot"},{"location":"reference/manubot/cite/tests/test_url/#test_get_url_csl_item_zotero_nyt","text":"def test_get_url_csl_item_zotero_nyt ( ) This command creates two translation-server queries. The first query is equivalent to: curl --verbose --header \"Content-Type: text/plain\" --data 'https://nyti.ms/1NuB0WJ' 'https://translate.manubot.org/web' View Source def test_get_url_csl_item_zotero_nyt (): \"\"\" This command creates two translation-server queries. The first query is equivalent to: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'https://nyti.ms/1NuB0WJ' \\ 'https://translate.manubot.org/web' ``` \"\"\" url = 'https://nyti.ms/1NuB0WJ' csl_item = get_url_csl_item_zotero ( url ) assert csl_item [ 'title' ] == 'Unraveling the Ties of Altitude, Oxygen and Lung Cancer' assert csl_item [ 'author' ][ 0 ][ 'family' ] == 'Johnson'","title":"test_get_url_csl_item_zotero_nyt"},{"location":"reference/manubot/cite/tests/test_wikidata/","text":"Module manubot.cite.tests.test_wikidata View Source from manubot.cite.wikidata import get_wikidata_csl_item def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = 'Q50051684' csl_item = get_wikidata_csl_item ( wikidata_id ) assert 'Sci-Hub provides access to nearly all scholarly literature' in csl_item [ 'title' ] assert csl_item [ 'container-title' ] == 'eLife' assert csl_item [ 'DOI' ] == '10.7554/elife.32822' def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = 'Q50051684' csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ 'family' ] for author in csl_item [ 'author' ]] print ( family_names ) assert family_names == [ 'Himmelstein' , 'Romero' , 'Levernier' , 'Munro' , 'McLaughlin' , 'Greshake' , # actually should be Greshake Tzovaras 'Greene' , ] Functions test_get_wikidata_csl_item def test_get_wikidata_csl_item ( ) Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 View Source def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = 'Q50051684' csl_item = get_wikidata_csl_item ( wikidata_id ) assert 'Sci-Hub provides access to nearly all scholarly literature' in csl_item [ 'title' ] assert csl_item [ 'container-title' ] == 'eLife' assert csl_item [ 'DOI' ] == '10.7554/elife.32822' test_get_wikidata_csl_item_author_ordering def test_get_wikidata_csl_item_author_ordering ( ) Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 View Source def test_get_wikidata_csl_item_author_ordering () : \"\"\" Test extraction of author ordering from https : // www . wikidata . org / wiki / Q50051684 . Wikidata uses a \" series ordinal \" qualifier that must be considered or else author ordering may be wrong . Author ordering was previously not properly set by the Wikidata translator https : // github . com / zotero / translators / issues / 1790 \"\"\" wikidata_id = ' Q50051684 ' csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ ' family ' ] for author in csl_item [ ' author ' ]] print ( family_names ) assert family_names == [ ' Himmelstein ' , ' Romero ' , ' Levernier ' , ' Munro ' , ' McLaughlin ' , ' Greshake ' , # actually should be Greshake Tzovaras ' Greene ' , ]","title":"Test Wikidata"},{"location":"reference/manubot/cite/tests/test_wikidata/#module-manubotciteteststest_wikidata","text":"View Source from manubot.cite.wikidata import get_wikidata_csl_item def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = 'Q50051684' csl_item = get_wikidata_csl_item ( wikidata_id ) assert 'Sci-Hub provides access to nearly all scholarly literature' in csl_item [ 'title' ] assert csl_item [ 'container-title' ] == 'eLife' assert csl_item [ 'DOI' ] == '10.7554/elife.32822' def test_get_wikidata_csl_item_author_ordering (): \"\"\" Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 \"\"\" wikidata_id = 'Q50051684' csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ 'family' ] for author in csl_item [ 'author' ]] print ( family_names ) assert family_names == [ 'Himmelstein' , 'Romero' , 'Levernier' , 'Munro' , 'McLaughlin' , 'Greshake' , # actually should be Greshake Tzovaras 'Greene' , ]","title":"Module manubot.cite.tests.test_wikidata"},{"location":"reference/manubot/cite/tests/test_wikidata/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_wikidata/#test_get_wikidata_csl_item","text":"def test_get_wikidata_csl_item ( ) Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 View Source def test_get_wikidata_csl_item (): \"\"\" Test metadata extraction from https://www.wikidata.org/wiki/Q50051684 \"\"\" wikidata_id = 'Q50051684' csl_item = get_wikidata_csl_item ( wikidata_id ) assert 'Sci-Hub provides access to nearly all scholarly literature' in csl_item [ 'title' ] assert csl_item [ 'container-title' ] == 'eLife' assert csl_item [ 'DOI' ] == '10.7554/elife.32822'","title":"test_get_wikidata_csl_item"},{"location":"reference/manubot/cite/tests/test_wikidata/#test_get_wikidata_csl_item_author_ordering","text":"def test_get_wikidata_csl_item_author_ordering ( ) Test extraction of author ordering from https://www.wikidata.org/wiki/Q50051684. Wikidata uses a \"series ordinal\" qualifier that must be considered or else author ordering may be wrong. Author ordering was previously not properly set by the Wikidata translator https://github.com/zotero/translators/issues/1790 View Source def test_get_wikidata_csl_item_author_ordering () : \"\"\" Test extraction of author ordering from https : // www . wikidata . org / wiki / Q50051684 . Wikidata uses a \" series ordinal \" qualifier that must be considered or else author ordering may be wrong . Author ordering was previously not properly set by the Wikidata translator https : // github . com / zotero / translators / issues / 1790 \"\"\" wikidata_id = ' Q50051684 ' csl_item = get_wikidata_csl_item ( wikidata_id ) family_names = [ author [ ' family ' ] for author in csl_item [ ' author ' ]] print ( family_names ) assert family_names == [ ' Himmelstein ' , ' Romero ' , ' Levernier ' , ' Munro ' , ' McLaughlin ' , ' Greshake ' , # actually should be Greshake Tzovaras ' Greene ' , ]","title":"test_get_wikidata_csl_item_author_ordering"},{"location":"reference/manubot/cite/tests/test_zotero/","text":"Module manubot.cite.tests.test_zotero View Source import pytest from manubot.cite.zotero import ( web_query , search_query , export_as_csl ) def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ 'title' ] . startswith ( \"Meet the Robin Hood of Science\" ) def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ 'title' ] == \"Meet the Robin Hood of Science\" assert csl_item [ 'container-title' ] == 'Big Think' def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ 'title' ] == 'Sci-Hub provides access to nearly all scholarly literature' def test_web_query_returns_single_result_pubmed_url (): \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ 'title' ] == 'sci-hub[title] - PubMed - NCBI' def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'isbn:9781339919881' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] . startswith ( 'The hetnet awakens' ) def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'arxiv:1604.05363v1' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] == 'Comparing Published Scientific Journal Articles to Their Pre-print Versions' assert zotero_data [ 0 ][ 'creators' ][ - 1 ][ 'firstName' ] == 'Todd' assert zotero_data [ 0 ][ 'date' ] == '2016-04-18' @pytest.mark.parametrize ( 'identifier' , [ '30571677' , # https://www.ncbi.nlm.nih.gov/pubmed/30571677 'doi:10.1371/journal.pcbi.1006561' , # https://doi.org/10.1371/journal.pcbi.1006561 ]) def test_search_query ( identifier ): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] . startswith ( 'Ten simple rules for documenting scientific software' ) assert zotero_data [ 0 ][ 'creators' ][ 0 ][ 'lastName' ] == 'Lee' Functions test_export_as_csl def test_export_as_csl ( ) CSL export can be tested via curl: curl --header \"Content-Type: application/json\" --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' 'https://translate.manubot.org/export?format=csljson' View Source def test_export_as_csl () : \"\"\" CSL export can be tested via curl : ``` curl \\ -- header \" Content-Type: application/json \" \\ -- data ' [{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}] ' \\ ' https://translate.manubot.org/export?format=csljson ' ``` \"\"\" zotero_data = [ { \" key \" : \" IN22XN53 \" , \" version \" : 0 , \" itemType \" : \" webpage \" , \" creators \" : [], \" tags \" : [], \" title \" : \" Meet the Robin Hood of Science \" , \" websiteTitle \" : \" Big Think \" , \" date \" : \" 2016-02-09T20:12:00 \" , \" url \" : \" https://bigthink.com/neurobonkers/a-pirate-bay-for-science \" , \" abstractNote \" : \" How one researcher created a pirate bay for science more powerful than even libraries at top universities. \" , \" language \" : \" en \" , \" accessDate \" : \" 2018-12-06T20:10:14Z \" , } ] csl_item = export_as_csl ( zotero_data ) [ 0 ] assert csl_item [ ' title ' ] == \" Meet the Robin Hood of Science \" assert csl_item [ ' container-title ' ] == ' Big Think ' test_search_query def test_search_query ( identifier ) The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data '30571677' 'https://translate.manubot.org/search' translation-server does not support PMIDs with a pmid: prefix. https://github.com/zotero/translation-server/issues/71 View Source @ pytest . mark . parametrize ( ' identifier ' , [ ' 30571677 ' , # https : // www . ncbi . nlm . nih . gov / pubmed / 30571677 ' doi:10.1371/journal.pcbi.1006561 ' , # https : // doi . org / 10 . 1371 / journal . pcbi . 1006561 ] ) def test_search_query ( identifier ) : \"\"\" The translation - server search endpoint can be tested via curl : ``` curl -- verbose \\ -- header \" Content-Type: text/plain \" \\ -- data ' 30571677 ' \\ ' https://translate.manubot.org/search ' ``` translation - server does not support PMIDs with a ` pmid :` prefix . https : // github . com / zotero / translation - server / issues / 71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ ' title ' ]. startswith ( ' Ten simple rules for documenting scientific software ' ) assert zotero_data [ 0 ][ ' creators ' ][ 0 ][ ' lastName ' ] == ' Lee ' test_search_query_arxiv def test_search_query_arxiv ( ) Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data 'arxiv:1604.05363v1' 'https://translate.manubot.org/search' View Source def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'arxiv:1604.05363v1' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] == 'Comparing Published Scientific Journal Articles to Their Pre-print Versions' assert zotero_data [ 0 ][ 'creators' ][ - 1 ][ 'firstName' ] == 'Todd' assert zotero_data [ 0 ][ 'date' ] == '2016-04-18' test_search_query_isbn def test_search_query_isbn ( ) The translation-server search endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'isbn:9781339919881' 'https://translate.manubot.org/search' View Source def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'isbn:9781339919881' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ]. startswith ( 'The hetnet awakens' ) test_web_query def test_web_query ( ) The translation-server web endpoint can be tested via curl: curl -- header \" Content-Type: text/plain \" -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' ' https://translate.manubot.org/web ' An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 View Source def test_web_query () : \"\"\" The translation - server web endpoint can be tested via curl : ``` curl \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' \\ ' https://translate.manubot.org/web ' ``` An outdated installation of translation - server caused the web query for this URL to be extraordinarily slow but has now been fixed . See https : // github . com / zotero / translation - server / issues / 63 \"\"\" url = ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ ' title ' ]. startswith ( \" Meet the Robin Hood of Science \" ) test_web_query_returns_single_result_legacy_manubot_url def test_web_query_returns_single_result_legacy_manubot_url ( ) Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 curl --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_legacy_manubot_url () : \"\"\" Check that single = 1 is specified for web queries . Without this , Zotero can prefer translators that return multiple choices . This occurs with legacy Manubot mansucripts , which get assigned the DOI translator as top priority . https : // github . com / zotero / translation - server / issues / 65 ``` curl \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/ ' \\ ' https://translate.manubot.org/web?single=1 ' ``` \"\"\" url = ' https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/ ' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ ' title ' ] == ' Sci-Hub provides access to nearly all scholarly literature ' test_web_query_returns_single_result_pubmed_url def test_web_query_returns_single_result_pubmed_url ( ) See test_web_query_returns_single_result_legacy_manubot_url docstring. curl --header \"Content-Type: text/plain\" --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_pubmed_url () : \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ 'title' ] == 'sci-hub[title] - PubMed - NCBI'","title":"Test Zotero"},{"location":"reference/manubot/cite/tests/test_zotero/#module-manubotciteteststest_zotero","text":"View Source import pytest from manubot.cite.zotero import ( web_query , search_query , export_as_csl ) def test_web_query (): \"\"\" The translation-server web endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' \\ 'https://translate.manubot.org/web' ``` An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 \"\"\" url = 'https://bigthink.com/neurobonkers/a-pirate-bay-for-science' zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ 'title' ] . startswith ( \"Meet the Robin Hood of Science\" ) def test_export_as_csl (): \"\"\" CSL export can be tested via curl: ``` curl \\ --header \"Content-Type: application/json\" \\ --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' \\ 'https://translate.manubot.org/export?format=csljson' ``` \"\"\" zotero_data = [ { \"key\" : \"IN22XN53\" , \"version\" : 0 , \"itemType\" : \"webpage\" , \"creators\" : [], \"tags\" : [], \"title\" : \"Meet the Robin Hood of Science\" , \"websiteTitle\" : \"Big Think\" , \"date\" : \"2016-02-09T20:12:00\" , \"url\" : \"https://bigthink.com/neurobonkers/a-pirate-bay-for-science\" , \"abstractNote\" : \"How one researcher created a pirate bay for science more powerful than even libraries at top universities.\" , \"language\" : \"en\" , \"accessDate\" : \"2018-12-06T20:10:14Z\" , } ] csl_item = export_as_csl ( zotero_data )[ 0 ] assert csl_item [ 'title' ] == \"Meet the Robin Hood of Science\" assert csl_item [ 'container-title' ] == 'Big Think' def test_web_query_returns_single_result_legacy_manubot_url (): \"\"\" Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ 'title' ] == 'Sci-Hub provides access to nearly all scholarly literature' def test_web_query_returns_single_result_pubmed_url (): \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ 'title' ] == 'sci-hub[title] - PubMed - NCBI' def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \"Content-Type: text/plain\" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'isbn:9781339919881' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] . startswith ( 'The hetnet awakens' ) def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'arxiv:1604.05363v1' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] == 'Comparing Published Scientific Journal Articles to Their Pre-print Versions' assert zotero_data [ 0 ][ 'creators' ][ - 1 ][ 'firstName' ] == 'Todd' assert zotero_data [ 0 ][ 'date' ] == '2016-04-18' @pytest.mark.parametrize ( 'identifier' , [ '30571677' , # https://www.ncbi.nlm.nih.gov/pubmed/30571677 'doi:10.1371/journal.pcbi.1006561' , # https://doi.org/10.1371/journal.pcbi.1006561 ]) def test_search_query ( identifier ): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \"Content-Type: text/plain\" \\ --data '30571677' \\ 'https://translate.manubot.org/search' ``` translation-server does not support PMIDs with a `pmid:` prefix. https://github.com/zotero/translation-server/issues/71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] . startswith ( 'Ten simple rules for documenting scientific software' ) assert zotero_data [ 0 ][ 'creators' ][ 0 ][ 'lastName' ] == 'Lee'","title":"Module manubot.cite.tests.test_zotero"},{"location":"reference/manubot/cite/tests/test_zotero/#functions","text":"","title":"Functions"},{"location":"reference/manubot/cite/tests/test_zotero/#test_export_as_csl","text":"def test_export_as_csl ( ) CSL export can be tested via curl: curl --header \"Content-Type: application/json\" --data '[{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}]' 'https://translate.manubot.org/export?format=csljson' View Source def test_export_as_csl () : \"\"\" CSL export can be tested via curl : ``` curl \\ -- header \" Content-Type: application/json \" \\ -- data ' [{\"key\": \"IN22XN53\", \"itemType\": \"webpage\", \"date\": \"2016-02-09T20:12:00\"}] ' \\ ' https://translate.manubot.org/export?format=csljson ' ``` \"\"\" zotero_data = [ { \" key \" : \" IN22XN53 \" , \" version \" : 0 , \" itemType \" : \" webpage \" , \" creators \" : [], \" tags \" : [], \" title \" : \" Meet the Robin Hood of Science \" , \" websiteTitle \" : \" Big Think \" , \" date \" : \" 2016-02-09T20:12:00 \" , \" url \" : \" https://bigthink.com/neurobonkers/a-pirate-bay-for-science \" , \" abstractNote \" : \" How one researcher created a pirate bay for science more powerful than even libraries at top universities. \" , \" language \" : \" en \" , \" accessDate \" : \" 2018-12-06T20:10:14Z \" , } ] csl_item = export_as_csl ( zotero_data ) [ 0 ] assert csl_item [ ' title ' ] == \" Meet the Robin Hood of Science \" assert csl_item [ ' container-title ' ] == ' Big Think '","title":"test_export_as_csl"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query","text":"def test_search_query ( identifier ) The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data '30571677' 'https://translate.manubot.org/search' translation-server does not support PMIDs with a pmid: prefix. https://github.com/zotero/translation-server/issues/71 View Source @ pytest . mark . parametrize ( ' identifier ' , [ ' 30571677 ' , # https : // www . ncbi . nlm . nih . gov / pubmed / 30571677 ' doi:10.1371/journal.pcbi.1006561 ' , # https : // doi . org / 10 . 1371 / journal . pcbi . 1006561 ] ) def test_search_query ( identifier ) : \"\"\" The translation - server search endpoint can be tested via curl : ``` curl -- verbose \\ -- header \" Content-Type: text/plain \" \\ -- data ' 30571677 ' \\ ' https://translate.manubot.org/search ' ``` translation - server does not support PMIDs with a ` pmid :` prefix . https : // github . com / zotero / translation - server / issues / 71 \"\"\" zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ ' title ' ]. startswith ( ' Ten simple rules for documenting scientific software ' ) assert zotero_data [ 0 ][ ' creators ' ][ 0 ][ ' lastName ' ] == ' Lee '","title":"test_search_query"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query_arxiv","text":"def test_search_query_arxiv ( ) Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: curl --verbose --header \"Content-Type: text/plain\" --data 'arxiv:1604.05363v1' 'https://translate.manubot.org/search' View Source def test_search_query_arxiv (): \"\"\" Test citing https://arxiv.org/abs/1604.05363v1 The translation-server search endpoint can be tested via curl: ``` curl --verbose \\ --header \" Content - Type : text / plain \" \\ --data 'arxiv:1604.05363v1' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'arxiv:1604.05363v1' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ] == 'Comparing Published Scientific Journal Articles to Their Pre-print Versions' assert zotero_data [ 0 ][ 'creators' ][ - 1 ][ 'firstName' ] == 'Todd' assert zotero_data [ 0 ][ 'date' ] == '2016-04-18'","title":"test_search_query_arxiv"},{"location":"reference/manubot/cite/tests/test_zotero/#test_search_query_isbn","text":"def test_search_query_isbn ( ) The translation-server search endpoint can be tested via curl: curl --header \"Content-Type: text/plain\" --data 'isbn:9781339919881' 'https://translate.manubot.org/search' View Source def test_search_query_isbn (): \"\"\" The translation-server search endpoint can be tested via curl: ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'isbn:9781339919881' \\ 'https://translate.manubot.org/search' ``` \"\"\" identifier = 'isbn:9781339919881' zotero_data = search_query ( identifier ) assert zotero_data [ 0 ][ 'title' ]. startswith ( 'The hetnet awakens' )","title":"test_search_query_isbn"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query","text":"def test_web_query ( ) The translation-server web endpoint can be tested via curl: curl -- header \" Content-Type: text/plain \" -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' ' https://translate.manubot.org/web ' An outdated installation of translation-server caused the web query for this URL to be extraordinarily slow but has now been fixed. See https://github.com/zotero/translation-server/issues/63 View Source def test_web_query () : \"\"\" The translation - server web endpoint can be tested via curl : ``` curl \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' \\ ' https://translate.manubot.org/web ' ``` An outdated installation of translation - server caused the web query for this URL to be extraordinarily slow but has now been fixed . See https : // github . com / zotero / translation - server / issues / 63 \"\"\" url = ' https://bigthink.com/neurobonkers/a-pirate-bay-for-science ' zotero_data = web_query ( url ) assert isinstance ( zotero_data , list ) assert len ( zotero_data ) == 1 assert zotero_data [ 0 ][ ' title ' ]. startswith ( \" Meet the Robin Hood of Science \" )","title":"test_web_query"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query_returns_single_result_legacy_manubot_url","text":"def test_web_query_returns_single_result_legacy_manubot_url ( ) Check that single=1 is specified for web queries. Without this, Zotero can prefer translators that return multiple choices. This occurs with legacy Manubot mansucripts, which get assigned the DOI translator as top priority. https://github.com/zotero/translation-server/issues/65 curl --header \"Content-Type: text/plain\" --data 'https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_legacy_manubot_url () : \"\"\" Check that single = 1 is specified for web queries . Without this , Zotero can prefer translators that return multiple choices . This occurs with legacy Manubot mansucripts , which get assigned the DOI translator as top priority . https : // github . com / zotero / translation - server / issues / 65 ``` curl \\ -- header \" Content-Type: text/plain \" \\ -- data ' https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/ ' \\ ' https://translate.manubot.org/web?single=1 ' ``` \"\"\" url = ' https://greenelab.github.io/scihub-manuscript/v/cfe599e25405d38092bf972b6ea1c9e0dcf3deb9/ ' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ ' title ' ] == ' Sci-Hub provides access to nearly all scholarly literature '","title":"test_web_query_returns_single_result_legacy_manubot_url"},{"location":"reference/manubot/cite/tests/test_zotero/#test_web_query_returns_single_result_pubmed_url","text":"def test_web_query_returns_single_result_pubmed_url ( ) See test_web_query_returns_single_result_legacy_manubot_url docstring. curl --header \"Content-Type: text/plain\" --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' 'https://translate.manubot.org/web?single=1' View Source def test_web_query_returns_single_result_pubmed_url () : \"\"\" See test_web_query_returns_single_result_legacy_manubot_url docstring. ``` curl \\ --header \" Content - Type : text / plain \" \\ --data 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' \\ 'https://translate.manubot.org/web?single=1' ``` \"\"\" url = 'https://www.ncbi.nlm.nih.gov/pubmed/?term=sci-hub%5Btitle%5D' zotero_metadata = web_query ( url ) assert isinstance ( zotero_metadata , list ) assert len ( zotero_metadata ) == 1 zotero_metadata , = zotero_metadata assert zotero_metadata [ 'title' ] == 'sci-hub[title] - PubMed - NCBI'","title":"test_web_query_returns_single_result_pubmed_url"},{"location":"reference/manubot/pandoc/","text":"Module manubot.pandoc Sub-modules manubot.pandoc.bibliography manubot.pandoc.util","title":"Index"},{"location":"reference/manubot/pandoc/#module-manubotpandoc","text":"","title":"Module manubot.pandoc"},{"location":"reference/manubot/pandoc/#sub-modules","text":"manubot.pandoc.bibliography manubot.pandoc.util","title":"Sub-modules"},{"location":"reference/manubot/pandoc/bibliography/","text":"Module manubot.pandoc.bibliography View Source import json import logging import subprocess from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join def load_bibliography ( path = None , text = None , input_format = None ): \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ): raise ValueError ( 'load_bibliography: specify either path or text but not both.' ) if not get_pandoc_info ()[ 'pandoc-citeproc' ]: logging . error ( 'pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON' ) return [] args = [ 'pandoc-citeproc' , '--bib2json' , ] if input_format : args . extend ([ '--format' , input_format ]) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ 'input' ] = text logging . info ( 'call_pandoc subprocess args: \\n >>> ' + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = 'utf-8' , ** run_kwargs , ) logging . info ( f 'captured stderr: \\n {process.stderr}' ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f 'Error parsing bib2json output as JSON: \\n {process.stdout}' ) csl_json = [] return csl_json Functions load_bibliography def load_bibliography ( path = None , text = None , input_format = None ) Convert a bibliography to CSL JSON using pandoc-citeproc --bib2json . Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. input_format should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. View Source def load_bibliography ( path = None , text = None , input_format = None ) : \"\"\" Convert a bibliography to CSL JSON using ` pandoc - citeproc -- bib2json `. Accepts either a bibliography path or text ( string ) . If supplying text , pandoc - citeproc will likely require input_format be specified . The CSL JSON is returned as Python objects . Parameters ---------- path : str , pathlike , or None Path to a bibliography file . Extension is used by pandoc - citeproc to infer the format of the input . text : str or None Text representation of the bibligriophy , such as a JSON - formatted string . ` input_format ` should be specified if providing text input . input_format : str or None Manually specified input formatted that is supported by pandoc - citeproc : https : // github . com / jgm / pandoc - citeproc / blob / master / man / pandoc - citeproc . 1 . md # options Returns ------- csl_json : JSON - like object CSL JSON Data for the references encoded by the input bibliography . \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ) : raise ValueError ( ' load_bibliography: specify either path or text but not both. ' ) if not get_pandoc_info () [ ' pandoc-citeproc ' ]: logging . error ( ' pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON ' ) return [] args = [ ' pandoc-citeproc ' , ' --bib2json ' , ] if input_format : args . extend ( [ ' --format ' , input_format ] ) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ ' input ' ] = text logging . info ( ' call_pandoc subprocess args: \\n >>> ' + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = ' utf-8 ' , ** run_kwargs , ) logging . info ( f ' captured stderr: \\n {process.stderr} ' ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f ' Error parsing bib2json output as JSON: \\n {process.stdout} ' ) csl_json = [] return csl_json","title":"Bibliography"},{"location":"reference/manubot/pandoc/bibliography/#module-manubotpandocbibliography","text":"View Source import json import logging import subprocess from manubot.pandoc.util import get_pandoc_info from manubot.util import shlex_join def load_bibliography ( path = None , text = None , input_format = None ): \"\"\" Convert a bibliography to CSL JSON using `pandoc-citeproc --bib2json`. Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects. Parameters ---------- path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. `input_format` should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options Returns ------- csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ): raise ValueError ( 'load_bibliography: specify either path or text but not both.' ) if not get_pandoc_info ()[ 'pandoc-citeproc' ]: logging . error ( 'pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON' ) return [] args = [ 'pandoc-citeproc' , '--bib2json' , ] if input_format : args . extend ([ '--format' , input_format ]) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ 'input' ] = text logging . info ( 'call_pandoc subprocess args: \\n >>> ' + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = 'utf-8' , ** run_kwargs , ) logging . info ( f 'captured stderr: \\n {process.stderr}' ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f 'Error parsing bib2json output as JSON: \\n {process.stdout}' ) csl_json = [] return csl_json","title":"Module manubot.pandoc.bibliography"},{"location":"reference/manubot/pandoc/bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/bibliography/#load_bibliography","text":"def load_bibliography ( path = None , text = None , input_format = None ) Convert a bibliography to CSL JSON using pandoc-citeproc --bib2json . Accepts either a bibliography path or text (string). If supplying text, pandoc-citeproc will likely require input_format be specified. The CSL JSON is returned as Python objects.","title":"load_bibliography"},{"location":"reference/manubot/pandoc/bibliography/#parameters","text":"path : str, pathlike, or None Path to a bibliography file. Extension is used by pandoc-citeproc to infer the format of the input. text : str or None Text representation of the bibligriophy, such as a JSON-formatted string. input_format should be specified if providing text input. input_format : str or None Manually specified input formatted that is supported by pandoc-citeproc: https://github.com/jgm/pandoc-citeproc/blob/master/man/pandoc-citeproc.1.md#options","title":"Parameters"},{"location":"reference/manubot/pandoc/bibliography/#returns","text":"csl_json : JSON-like object CSL JSON Data for the references encoded by the input bibliography. View Source def load_bibliography ( path = None , text = None , input_format = None ) : \"\"\" Convert a bibliography to CSL JSON using ` pandoc - citeproc -- bib2json `. Accepts either a bibliography path or text ( string ) . If supplying text , pandoc - citeproc will likely require input_format be specified . The CSL JSON is returned as Python objects . Parameters ---------- path : str , pathlike , or None Path to a bibliography file . Extension is used by pandoc - citeproc to infer the format of the input . text : str or None Text representation of the bibligriophy , such as a JSON - formatted string . ` input_format ` should be specified if providing text input . input_format : str or None Manually specified input formatted that is supported by pandoc - citeproc : https : // github . com / jgm / pandoc - citeproc / blob / master / man / pandoc - citeproc . 1 . md # options Returns ------- csl_json : JSON - like object CSL JSON Data for the references encoded by the input bibliography . \"\"\" use_text = path is None use_path = text is None if not ( use_text ^ use_path ) : raise ValueError ( ' load_bibliography: specify either path or text but not both. ' ) if not get_pandoc_info () [ ' pandoc-citeproc ' ]: logging . error ( ' pandoc-citeproc not found on system: manubot.pandoc.bibliography.load_bibliography returning empty CSL JSON ' ) return [] args = [ ' pandoc-citeproc ' , ' --bib2json ' , ] if input_format : args . extend ( [ ' --format ' , input_format ] ) run_kwargs = {} if use_path : args . append ( str ( path )) if use_text : run_kwargs [ ' input ' ] = text logging . info ( ' call_pandoc subprocess args: \\n >>> ' + shlex_join ( args )) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , encoding = ' utf-8 ' , ** run_kwargs , ) logging . info ( f ' captured stderr: \\n {process.stderr} ' ) process . check_returncode () try : csl_json = json . loads ( process . stdout ) except Exception : logging . exception ( f ' Error parsing bib2json output as JSON: \\n {process.stdout} ' ) csl_json = [] return csl_json","title":"Returns"},{"location":"reference/manubot/pandoc/util/","text":"Module manubot.pandoc.util View Source import logging import shutil import subprocess import functools @functools.lru_cache () def get_pandoc_info (): \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } If the executables are missing, the output will be like: { 'pandoc': False, 'pandoc-citeproc': False, } \"\"\" stats = dict () for command in 'pandoc' , 'pandoc-citeproc' : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args = [ command , '--version' ], universal_newlines = True , ) logging . debug ( version ) version , * discard = version . splitlines () discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release stats [ f '{command} version' ] = version stats [ f '{command} path' ] = path logging . info ( ' \\n ' . join ( f '{k}: {v}' for k , v in stats . items ())) return stats Functions get_pandoc_info def get_pandoc_info ( ) Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } If the executables are missing, the output will be like: { 'pandoc': False, 'pandoc-citeproc': False, } View Source @ functools . lru_cache () def get_pandoc_info () : \"\"\" Return path and version information for the system ' s pandoc and pandoc - citeproc commands . When Pandoc is installed , the output will look like : { ' pandoc ' : True , ' pandoc path ' : ' /PATH_TO_EXECUTABLES/pandoc ' , ' pandoc version ' : ( 2 , 5 ) , ' pandoc-citeproc ' : True , ' pandoc-citeproc path ' : ' /PATH_TO_EXECUTABLES/pandoc-citeproc ' , ' pandoc-citeproc version ' : ( 0 , 15 ) , } If the executables are missing , the output will be like : { ' pandoc ' : False , ' pandoc-citeproc ' : False , } \"\"\" stats = dict () for command in ' pandoc ' , ' pandoc-citeproc ' : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args = [ command , ' --version ' ], universal_newlines = True , ) logging . debug ( version ) version , * discard = version . splitlines () discard , version = version . strip () . split () from packaging . version import parse as parse_version version = parse_version ( version ) . release stats [ f ' {command} version ' ] = version stats [ f ' {command} path ' ] = path logging . info ( ' \\n ' . join ( f ' {k}: {v} ' for k , v in stats . items ())) return stats","title":"Util"},{"location":"reference/manubot/pandoc/util/#module-manubotpandocutil","text":"View Source import logging import shutil import subprocess import functools @functools.lru_cache () def get_pandoc_info (): \"\"\" Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } If the executables are missing, the output will be like: { 'pandoc': False, 'pandoc-citeproc': False, } \"\"\" stats = dict () for command in 'pandoc' , 'pandoc-citeproc' : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args = [ command , '--version' ], universal_newlines = True , ) logging . debug ( version ) version , * discard = version . splitlines () discard , version = version . strip () . split () from packaging.version import parse as parse_version version = parse_version ( version ) . release stats [ f '{command} version' ] = version stats [ f '{command} path' ] = path logging . info ( ' \\n ' . join ( f '{k}: {v}' for k , v in stats . items ())) return stats","title":"Module manubot.pandoc.util"},{"location":"reference/manubot/pandoc/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/pandoc/util/#get_pandoc_info","text":"def get_pandoc_info ( ) Return path and version information for the system's pandoc and pandoc-citeproc commands. When Pandoc is installed, the output will look like: { 'pandoc': True, 'pandoc path': '/PATH_TO_EXECUTABLES/pandoc', 'pandoc version': (2, 5), 'pandoc-citeproc': True, 'pandoc-citeproc path': '/PATH_TO_EXECUTABLES/pandoc-citeproc', 'pandoc-citeproc version': (0, 15), } If the executables are missing, the output will be like: { 'pandoc': False, 'pandoc-citeproc': False, } View Source @ functools . lru_cache () def get_pandoc_info () : \"\"\" Return path and version information for the system ' s pandoc and pandoc - citeproc commands . When Pandoc is installed , the output will look like : { ' pandoc ' : True , ' pandoc path ' : ' /PATH_TO_EXECUTABLES/pandoc ' , ' pandoc version ' : ( 2 , 5 ) , ' pandoc-citeproc ' : True , ' pandoc-citeproc path ' : ' /PATH_TO_EXECUTABLES/pandoc-citeproc ' , ' pandoc-citeproc version ' : ( 0 , 15 ) , } If the executables are missing , the output will be like : { ' pandoc ' : False , ' pandoc-citeproc ' : False , } \"\"\" stats = dict () for command in ' pandoc ' , ' pandoc-citeproc ' : path = shutil . which ( command ) stats [ command ] = bool ( path ) if not path : continue version = subprocess . check_output ( args = [ command , ' --version ' ], universal_newlines = True , ) logging . debug ( version ) version , * discard = version . splitlines () discard , version = version . strip () . split () from packaging . version import parse as parse_version version = parse_version ( version ) . release stats [ f ' {command} version ' ] = version stats [ f ' {command} path ' ] = path logging . info ( ' \\n ' . join ( f ' {k}: {v} ' for k , v in stats . items ())) return stats","title":"get_pandoc_info"},{"location":"reference/manubot/process/","text":"Module manubot.process Sub-modules manubot.process.bibliography manubot.process.ci manubot.process.manuscript manubot.process.process_command manubot.process.tests manubot.process.util","title":"Index"},{"location":"reference/manubot/process/#module-manubotprocess","text":"","title":"Module manubot.process"},{"location":"reference/manubot/process/#sub-modules","text":"manubot.process.bibliography manubot.process.ci manubot.process.manuscript manubot.process.process_command manubot.process.tests manubot.process.util","title":"Sub-modules"},{"location":"reference/manubot/process/bibliography/","text":"Module manubot.process.bibliography View Source import json import logging import pathlib from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( append_to_csl_item_note , csl_item_passthrough , ) from manubot.cite.csl_item import csl_item_set_standard_id from manubot.cite.citekey import shorten_citekey def load_bibliography ( path ): \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) use_pandoc_citeproc = True try : if path . suffix == '.json' : use_pandoc_citeproc = False with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = json . load ( read_file ) if path . suffix == '.yaml' : use_pandoc_citeproc = False import yaml with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = yaml . safe_load ( read_file ) except Exception : logging . exception ( f 'process.load_bibliography: error parsing {path}. \\n ' ) csl_items = [] if use_pandoc_citeproc : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f 'process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. ' 'Setting csl_items to an empty list.' ) csl_items = [] return csl_items def load_manual_references ( paths = [], extra_csl_items = []): \"\"\" Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. \"\"\" csl_items = [] for path in paths : path = pathlib . Path ( path ) if not path . is_file (): logging . warning ( f 'process.load_bibliographies is skipping a non-existent path: {path}' ) continue for csl_item in load_bibliography ( path ): append_to_csl_item_note ( csl_item , text = f 'This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file.' , dictionary = { 'manual_reference_filename' : path . name }, ) csl_items . append ( csl_item ) csl_items . extend ( extra_csl_items ) manual_refs = dict () for csl_item in csl_items : try : csl_item_set_standard_id ( csl_item ) except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f 'Skipping csl_item where setting standard_id failed: \\n {csl_item_str}' , exc_info = True ) continue standard_id = csl_item [ 'id' ] csl_item = csl_item_passthrough ( csl_item , set_id = shorten_citekey ( standard_id )) manual_refs [ standard_id ] = csl_item return manual_refs Variables manubot_version Functions load_bibliography def load_bibliography ( path ) Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. View Source def load_bibliography ( path ): \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) use_pandoc_citeproc = True try : if path . suffix == '.json' : use_pandoc_citeproc = False with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = json . load ( read_file ) if path . suffix == '.yaml' : use_pandoc_citeproc = False import yaml with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = yaml . safe_load ( read_file ) except Exception : logging . exception ( f 'process.load_bibliography: error parsing {path}. \\n ' ) csl_items = [] if use_pandoc_citeproc : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f 'process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. ' 'Setting csl_items to an empty list.' ) csl_items = [] return csl_items load_manual_references def load_manual_references ( paths = [], extra_csl_items = [] ) Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. View Source def load_manual_references ( paths = [], extra_csl_items = [] ) : \"\"\" Read manual references ( overrides ) from files specified by a list of paths . Returns a standard_citation to CSL Item dictionary . extra_csl_items specifies JSON CSL stored as a Python object , to be used in addition to the CSL JSON stored as text in the file specified by path . Set paths = [] to only use extra_csl_items . \"\"\" csl_items = [] for path in paths : path = pathlib . Path ( path ) if not path . is_file () : logging . warning ( f ' process.load_bibliographies is skipping a non-existent path: {path} ' ) continue for csl_item in load_bibliography ( path ) : append_to_csl_item_note ( csl_item , text = f ' This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file. ' , dictionary = { ' manual_reference_filename ' : path . name }, ) csl_items . append ( csl_item ) csl_items . extend ( extra_csl_items ) manual_refs = dict () for csl_item in csl_items : try : csl_item_set_standard_id ( csl_item ) except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f ' Skipping csl_item where setting standard_id failed: \\n {csl_item_str} ' , exc_info = True ) continue standard_id = csl_item [ ' id ' ] csl_item = csl_item_passthrough ( csl_item , set_id = shorten_citekey ( standard_id )) manual_refs [ standard_id ] = csl_item return manual_refs","title":"Bibliography"},{"location":"reference/manubot/process/bibliography/#module-manubotprocessbibliography","text":"View Source import json import logging import pathlib from manubot import __version__ as manubot_version from manubot.cite.citeproc import ( append_to_csl_item_note , csl_item_passthrough , ) from manubot.cite.csl_item import csl_item_set_standard_id from manubot.cite.citekey import shorten_citekey def load_bibliography ( path ): \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) use_pandoc_citeproc = True try : if path . suffix == '.json' : use_pandoc_citeproc = False with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = json . load ( read_file ) if path . suffix == '.yaml' : use_pandoc_citeproc = False import yaml with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = yaml . safe_load ( read_file ) except Exception : logging . exception ( f 'process.load_bibliography: error parsing {path}. \\n ' ) csl_items = [] if use_pandoc_citeproc : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f 'process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. ' 'Setting csl_items to an empty list.' ) csl_items = [] return csl_items def load_manual_references ( paths = [], extra_csl_items = []): \"\"\" Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. \"\"\" csl_items = [] for path in paths : path = pathlib . Path ( path ) if not path . is_file (): logging . warning ( f 'process.load_bibliographies is skipping a non-existent path: {path}' ) continue for csl_item in load_bibliography ( path ): append_to_csl_item_note ( csl_item , text = f 'This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file.' , dictionary = { 'manual_reference_filename' : path . name }, ) csl_items . append ( csl_item ) csl_items . extend ( extra_csl_items ) manual_refs = dict () for csl_item in csl_items : try : csl_item_set_standard_id ( csl_item ) except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f 'Skipping csl_item where setting standard_id failed: \\n {csl_item_str}' , exc_info = True ) continue standard_id = csl_item [ 'id' ] csl_item = csl_item_passthrough ( csl_item , set_id = shorten_citekey ( standard_id )) manual_refs [ standard_id ] = csl_item return manual_refs","title":"Module manubot.process.bibliography"},{"location":"reference/manubot/process/bibliography/#variables","text":"manubot_version","title":"Variables"},{"location":"reference/manubot/process/bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/bibliography/#load_bibliography","text":"def load_bibliography ( path ) Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. View Source def load_bibliography ( path ): \"\"\" Load a bibliography as CSL Items (a CSL JSON Python object). For paths that already contain CSL Items (inferred from a .json or .yaml extension), parse these files directly. Otherwise, delegate conversion to CSL Items to pandoc-citeproc. \"\"\" path = pathlib . Path ( path ) use_pandoc_citeproc = True try : if path . suffix == '.json' : use_pandoc_citeproc = False with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = json . load ( read_file ) if path . suffix == '.yaml' : use_pandoc_citeproc = False import yaml with path . open ( encoding = 'utf-8-sig' ) as read_file : csl_items = yaml . safe_load ( read_file ) except Exception : logging . exception ( f 'process.load_bibliography: error parsing {path}. \\n ' ) csl_items = [] if use_pandoc_citeproc : from manubot.pandoc.bibliography import ( load_bibliography as load_bibliography_pandoc , ) csl_items = load_bibliography_pandoc ( path ) if not isinstance ( csl_items , list ): logging . error ( f 'process.load_bibliography: csl_items read from {path} are of type {type(csl_items)}. ' 'Setting csl_items to an empty list.' ) csl_items = [] return csl_items","title":"load_bibliography"},{"location":"reference/manubot/process/bibliography/#load_manual_references","text":"def load_manual_references ( paths = [], extra_csl_items = [] ) Read manual references (overrides) from files specified by a list of paths. Returns a standard_citation to CSL Item dictionary. extra_csl_items specifies JSON CSL stored as a Python object, to be used in addition to the CSL JSON stored as text in the file specified by path. Set paths=[] to only use extra_csl_items. View Source def load_manual_references ( paths = [], extra_csl_items = [] ) : \"\"\" Read manual references ( overrides ) from files specified by a list of paths . Returns a standard_citation to CSL Item dictionary . extra_csl_items specifies JSON CSL stored as a Python object , to be used in addition to the CSL JSON stored as text in the file specified by path . Set paths = [] to only use extra_csl_items . \"\"\" csl_items = [] for path in paths : path = pathlib . Path ( path ) if not path . is_file () : logging . warning ( f ' process.load_bibliographies is skipping a non-existent path: {path} ' ) continue for csl_item in load_bibliography ( path ) : append_to_csl_item_note ( csl_item , text = f ' This CSL JSON Item was loaded by Manubot v{manubot_version} from a manual reference file. ' , dictionary = { ' manual_reference_filename ' : path . name }, ) csl_items . append ( csl_item ) csl_items . extend ( extra_csl_items ) manual_refs = dict () for csl_item in csl_items : try : csl_item_set_standard_id ( csl_item ) except Exception : csl_item_str = json . dumps ( csl_item , indent = 2 ) logging . info ( f ' Skipping csl_item where setting standard_id failed: \\n {csl_item_str} ' , exc_info = True ) continue standard_id = csl_item [ ' id ' ] csl_item = csl_item_passthrough ( csl_item , set_id = shorten_citekey ( standard_id )) manual_refs [ standard_id ] = csl_item return manual_refs","title":"load_manual_references"},{"location":"reference/manubot/process/ci/","text":"Module manubot.process.ci View Source import os import logging supported_providers = [ 'travis' , 'appveyor' , ] def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build - triggering_commit: git commit that triggered the CI build. For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details \"\"\" if os . getenv ( 'CI' , 'false' ) . lower () != 'true' : # No continuous integration environment detected return None if os . getenv ( 'TRAVIS' , 'false' ) == 'true' : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ 'TRAVIS_REPO_SLUG' ] repo_owner , repo_name = repo_slug . split ( '/' ) return { 'provider' : 'travis' , 'repo_slug' : repo_slug , 'repo_owner' : repo_owner , 'repo_name' : repo_name , 'commit' : os . environ [ 'TRAVIS_COMMIT' ], 'triggering_commit' : os . getenv ( 'TRAVIS_PULL_REQUEST_SHA' ) or os . environ [ 'TRAVIS_COMMIT' ], 'build_url' : os . environ [ 'TRAVIS_BUILD_WEB_URL' ], 'job_url' : os . environ [ 'TRAVIS_JOB_WEB_URL' ], } if os . getenv ( 'APPVEYOR' , 'false' ) . lower () == 'true' : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ 'APPVEYOR_REPO_NAME' ] repo_owner , repo_name = repo_slug . split ( '/' ) provider_url = '{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}' . format ( ** os . environ ) return { 'provider' : 'appveyor' , 'provider_account' : os . environ [ 'APPVEYOR_ACCOUNT_NAME' ], 'repo_slug' : repo_slug , 'repo_owner' : repo_owner , 'repo_name' : repo_name , 'commit' : os . environ [ 'APPVEYOR_REPO_COMMIT' ], 'triggering_commit' : os . getenv ( 'APPVEYOR_PULL_REQUEST_HEAD_COMMIT' ) or os . environ [ 'APPVEYOR_REPO_COMMIT' ], 'build_url' : f \"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" , 'job_url' : f \"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\" , } logging . warning ( 'Detected CI environment variable, but get_continuous_integration_parameters ' 'did not detect environment variables for a supported CI provider. ' 'Supported providers are: {}' . format ( ', ' . join ( supported_providers )) ) return None def add_manuscript_urls_to_ci_params ( ci_params ): \"\"\" Return and edit in-place the ci_params dictionary to include 'manuscript_url'. This function assumes Travis CI is used to deploy to GitHub Pages, while AppVeyor is used for storing manuscript artifacts for pull request builds. \"\"\" if not ci_params : return ci_params assert isinstance ( ci_params , dict ) provider = ci_params . get ( 'provider' ) if provider == 'travis' : ci_params [ 'manuscript_url' ] = ( \"https://{repo_owner}.github.io/{repo_name}/v/{commit}/\" . format ( ** ci_params ) ) if provider == 'appveyor' : ci_params [ 'manuscript_url' ] = f \"{ci_params['build_url']}/artifacts\" return ci_params Variables supported_providers Functions add_manuscript_urls_to_ci_params def add_manuscript_urls_to_ci_params ( ci_params ) Return and edit in-place the ci_params dictionary to include 'manuscript_url'. This function assumes Travis CI is used to deploy to GitHub Pages, while AppVeyor is used for storing manuscript artifacts for pull request builds. View Source def add_manuscript_urls_to_ci_params ( ci_params ) : \"\"\" Return and edit in - place the ci_params dictionary to include ' manuscript_url ' . This function assumes Travis CI is used to deploy to GitHub Pages , while AppVeyor is used for storing manuscript artifacts for pull request builds . \"\"\" if not ci_params : return ci_params assert isinstance ( ci_params , dict ) provider = ci_params . get ( ' provider ' ) if provider == ' travis ' : ci_params [ ' manuscript_url ' ] = ( \" https://{repo_owner}.github.io/{repo_name}/v/{commit}/ \" . format ( ** ci_params ) ) if provider == ' appveyor ' : ci_params [ ' manuscript_url ' ] = f \" {ci_params['build_url']}/artifacts \" return ci_params get_continuous_integration_parameters def get_continuous_integration_parameters ( ) Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build - triggering_commit: git commit that triggered the CI build. For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details View Source def get_continuous_integration_parameters () : \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables . The following dictionary keys are set ( all values are also strings ) : - provider : name of CI provider , such as \" travis \" or \" appveyor \" . - repo_slug : owner / name of the source code repository , i . e . \" manubot/rootstock \" - repo_owner : owner from repo_slug , i . e . \" manubot \" - repo_name : name from repo_slug , i . e . \" rootstock \" - commit : git commit being evaluated by the CI build - triggering_commit : git commit that triggered the CI build . For pull requests , CI services often build a merge commit with the default branch rather than the commit that was added to the pull request . - build_url : URL for the webpage with build details - job_url : URL for the webpage with job details \"\"\" if os . getenv ( ' CI ' , ' false ' ) . lower () != ' true ' : # No continuous integration environment detected return None if os . getenv ( ' TRAVIS ' , ' false ' ) == ' true ' : # https : // docs . travis - ci . com / user / environment - variables / repo_slug = os . environ [ ' TRAVIS_REPO_SLUG ' ] repo_owner , repo_name = repo_slug . split ( ' / ' ) return { ' provider ' : ' travis ' , ' repo_slug ' : repo_slug , ' repo_owner ' : repo_owner , ' repo_name ' : repo_name , ' commit ' : os . environ [ ' TRAVIS_COMMIT ' ], ' triggering_commit ' : os . getenv ( ' TRAVIS_PULL_REQUEST_SHA ' ) or os . environ [ ' TRAVIS_COMMIT ' ], ' build_url ' : os . environ [ ' TRAVIS_BUILD_WEB_URL ' ], ' job_url ' : os . environ [ ' TRAVIS_JOB_WEB_URL ' ], } if os . getenv ( ' APPVEYOR ' , ' false ' ) . lower () == ' true ' : # https : // www . appveyor . com / docs / environment - variables / repo_slug = os . environ [ ' APPVEYOR_REPO_NAME ' ] repo_owner , repo_name = repo_slug . split ( ' / ' ) provider_url = ' {APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG} ' . format ( ** os . environ ) return { ' provider ' : ' appveyor ' , ' provider_account ' : os . environ [ ' APPVEYOR_ACCOUNT_NAME ' ], ' repo_slug ' : repo_slug , ' repo_owner ' : repo_owner , ' repo_name ' : repo_name , ' commit ' : os . environ [ ' APPVEYOR_REPO_COMMIT ' ], ' triggering_commit ' : os . getenv ( ' APPVEYOR_PULL_REQUEST_HEAD_COMMIT ' ) or os . environ [ ' APPVEYOR_REPO_COMMIT ' ], ' build_url ' : f \" {provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']} \" , ' job_url ' : f \" {provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']} \" , } logging . warning ( ' Detected CI environment variable, but get_continuous_integration_parameters ' ' did not detect environment variables for a supported CI provider. ' ' Supported providers are: {} ' . format ( ' , ' . join ( supported_providers )) ) return None","title":"Ci"},{"location":"reference/manubot/process/ci/#module-manubotprocessci","text":"View Source import os import logging supported_providers = [ 'travis' , 'appveyor' , ] def get_continuous_integration_parameters (): \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build - triggering_commit: git commit that triggered the CI build. For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details \"\"\" if os . getenv ( 'CI' , 'false' ) . lower () != 'true' : # No continuous integration environment detected return None if os . getenv ( 'TRAVIS' , 'false' ) == 'true' : # https://docs.travis-ci.com/user/environment-variables/ repo_slug = os . environ [ 'TRAVIS_REPO_SLUG' ] repo_owner , repo_name = repo_slug . split ( '/' ) return { 'provider' : 'travis' , 'repo_slug' : repo_slug , 'repo_owner' : repo_owner , 'repo_name' : repo_name , 'commit' : os . environ [ 'TRAVIS_COMMIT' ], 'triggering_commit' : os . getenv ( 'TRAVIS_PULL_REQUEST_SHA' ) or os . environ [ 'TRAVIS_COMMIT' ], 'build_url' : os . environ [ 'TRAVIS_BUILD_WEB_URL' ], 'job_url' : os . environ [ 'TRAVIS_JOB_WEB_URL' ], } if os . getenv ( 'APPVEYOR' , 'false' ) . lower () == 'true' : # https://www.appveyor.com/docs/environment-variables/ repo_slug = os . environ [ 'APPVEYOR_REPO_NAME' ] repo_owner , repo_name = repo_slug . split ( '/' ) provider_url = '{APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG}' . format ( ** os . environ ) return { 'provider' : 'appveyor' , 'provider_account' : os . environ [ 'APPVEYOR_ACCOUNT_NAME' ], 'repo_slug' : repo_slug , 'repo_owner' : repo_owner , 'repo_name' : repo_name , 'commit' : os . environ [ 'APPVEYOR_REPO_COMMIT' ], 'triggering_commit' : os . getenv ( 'APPVEYOR_PULL_REQUEST_HEAD_COMMIT' ) or os . environ [ 'APPVEYOR_REPO_COMMIT' ], 'build_url' : f \"{provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']}\" , 'job_url' : f \"{provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']}\" , } logging . warning ( 'Detected CI environment variable, but get_continuous_integration_parameters ' 'did not detect environment variables for a supported CI provider. ' 'Supported providers are: {}' . format ( ', ' . join ( supported_providers )) ) return None def add_manuscript_urls_to_ci_params ( ci_params ): \"\"\" Return and edit in-place the ci_params dictionary to include 'manuscript_url'. This function assumes Travis CI is used to deploy to GitHub Pages, while AppVeyor is used for storing manuscript artifacts for pull request builds. \"\"\" if not ci_params : return ci_params assert isinstance ( ci_params , dict ) provider = ci_params . get ( 'provider' ) if provider == 'travis' : ci_params [ 'manuscript_url' ] = ( \"https://{repo_owner}.github.io/{repo_name}/v/{commit}/\" . format ( ** ci_params ) ) if provider == 'appveyor' : ci_params [ 'manuscript_url' ] = f \"{ci_params['build_url']}/artifacts\" return ci_params","title":"Module manubot.process.ci"},{"location":"reference/manubot/process/ci/#variables","text":"supported_providers","title":"Variables"},{"location":"reference/manubot/process/ci/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/ci/#add_manuscript_urls_to_ci_params","text":"def add_manuscript_urls_to_ci_params ( ci_params ) Return and edit in-place the ci_params dictionary to include 'manuscript_url'. This function assumes Travis CI is used to deploy to GitHub Pages, while AppVeyor is used for storing manuscript artifacts for pull request builds. View Source def add_manuscript_urls_to_ci_params ( ci_params ) : \"\"\" Return and edit in - place the ci_params dictionary to include ' manuscript_url ' . This function assumes Travis CI is used to deploy to GitHub Pages , while AppVeyor is used for storing manuscript artifacts for pull request builds . \"\"\" if not ci_params : return ci_params assert isinstance ( ci_params , dict ) provider = ci_params . get ( ' provider ' ) if provider == ' travis ' : ci_params [ ' manuscript_url ' ] = ( \" https://{repo_owner}.github.io/{repo_name}/v/{commit}/ \" . format ( ** ci_params ) ) if provider == ' appveyor ' : ci_params [ ' manuscript_url ' ] = f \" {ci_params['build_url']}/artifacts \" return ci_params","title":"add_manuscript_urls_to_ci_params"},{"location":"reference/manubot/process/ci/#get_continuous_integration_parameters","text":"def get_continuous_integration_parameters ( ) Return a dictionary with information on a continuous integration build inferred from environment variables. The following dictionary keys are set (all values are also strings): - provider: name of CI provider, such as \"travis\" or \"appveyor\". - repo_slug: owner/name of the source code repository, i.e. \"manubot/rootstock\" - repo_owner: owner from repo_slug, i.e. \"manubot\" - repo_name: name from repo_slug, i.e. \"rootstock\" - commit: git commit being evaluated by the CI build - triggering_commit: git commit that triggered the CI build. For pull requests, CI services often build a merge commit with the default branch rather than the commit that was added to the pull request. - build_url: URL for the webpage with build details - job_url: URL for the webpage with job details View Source def get_continuous_integration_parameters () : \"\"\" Return a dictionary with information on a continuous integration build inferred from environment variables . The following dictionary keys are set ( all values are also strings ) : - provider : name of CI provider , such as \" travis \" or \" appveyor \" . - repo_slug : owner / name of the source code repository , i . e . \" manubot/rootstock \" - repo_owner : owner from repo_slug , i . e . \" manubot \" - repo_name : name from repo_slug , i . e . \" rootstock \" - commit : git commit being evaluated by the CI build - triggering_commit : git commit that triggered the CI build . For pull requests , CI services often build a merge commit with the default branch rather than the commit that was added to the pull request . - build_url : URL for the webpage with build details - job_url : URL for the webpage with job details \"\"\" if os . getenv ( ' CI ' , ' false ' ) . lower () != ' true ' : # No continuous integration environment detected return None if os . getenv ( ' TRAVIS ' , ' false ' ) == ' true ' : # https : // docs . travis - ci . com / user / environment - variables / repo_slug = os . environ [ ' TRAVIS_REPO_SLUG ' ] repo_owner , repo_name = repo_slug . split ( ' / ' ) return { ' provider ' : ' travis ' , ' repo_slug ' : repo_slug , ' repo_owner ' : repo_owner , ' repo_name ' : repo_name , ' commit ' : os . environ [ ' TRAVIS_COMMIT ' ], ' triggering_commit ' : os . getenv ( ' TRAVIS_PULL_REQUEST_SHA ' ) or os . environ [ ' TRAVIS_COMMIT ' ], ' build_url ' : os . environ [ ' TRAVIS_BUILD_WEB_URL ' ], ' job_url ' : os . environ [ ' TRAVIS_JOB_WEB_URL ' ], } if os . getenv ( ' APPVEYOR ' , ' false ' ) . lower () == ' true ' : # https : // www . appveyor . com / docs / environment - variables / repo_slug = os . environ [ ' APPVEYOR_REPO_NAME ' ] repo_owner , repo_name = repo_slug . split ( ' / ' ) provider_url = ' {APPVEYOR_URL}/project/{APPVEYOR_ACCOUNT_NAME}/{APPVEYOR_PROJECT_SLUG} ' . format ( ** os . environ ) return { ' provider ' : ' appveyor ' , ' provider_account ' : os . environ [ ' APPVEYOR_ACCOUNT_NAME ' ], ' repo_slug ' : repo_slug , ' repo_owner ' : repo_owner , ' repo_name ' : repo_name , ' commit ' : os . environ [ ' APPVEYOR_REPO_COMMIT ' ], ' triggering_commit ' : os . getenv ( ' APPVEYOR_PULL_REQUEST_HEAD_COMMIT ' ) or os . environ [ ' APPVEYOR_REPO_COMMIT ' ], ' build_url ' : f \" {provider_url}/builds/{os.environ['APPVEYOR_BUILD_ID']} \" , ' job_url ' : f \" {provider_url}/build/job/{os.environ['APPVEYOR_JOB_ID']} \" , } logging . warning ( ' Detected CI environment variable, but get_continuous_integration_parameters ' ' did not detect environment variables for a supported CI provider. ' ' Supported providers are: {} ' . format ( ' , ' . join ( supported_providers )) ) return None","title":"get_continuous_integration_parameters"},{"location":"reference/manubot/process/manuscript/","text":"Module manubot.process.manuscript View Source import collections import datetime import json import logging import pathlib import re from manubot.cite.citekey import citekey_pattern , is_valid_citekey def get_citekeys ( text ): \"\"\" Extract the deduplicated list of citations in a text. Citations that are clearly invalid such as `doi:/453` are not returned. \"\"\" citekeys = set ( citekey_pattern . findall ( text )) citekeys = filter ( lambda x : is_valid_citekey ( x , allow_tag = True , allow_raw = True , allow_pandoc_xnos = True ), citekeys , ) return sorted ( citekeys ) def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( '[0-9]*.md' )) name_to_text = collections . OrderedDict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = 'utf-8-sig' ) logging . info ( 'Manuscript content parts: \\n ' + ' \\n ' . join ( name_to_text )) return ' \\n\\n ' . join ( name_to_text . values ()) + ' \\n ' def update_manuscript_citekeys ( text , old_to_new ): \"\"\" Replace citation keys according to the old_to_new dictionary. Useful for converting citation keys to shortened versions that are appropriate for pandoc. `text` is markdown source text `old_to_new` is a dictionary like: doi:10.7287/peerj.preprints.3100v1 \u2192 11cb5HXoY \"\"\" for old , new in old_to_new . items (): text = re . sub ( pattern = re . escape ( '@' + old ) + r '(?![\\w:.#$%&\\-+?<>~/]*[a-zA-Z0-9/])' , repl = '@' + new , string = text , ) return text def get_manuscript_stats ( text , citekeys_df ): \"\"\" Compute manuscript statistics. \"\"\" stats = collections . OrderedDict () # Number of distinct references by type ref_counts = ( citekeys_df . standard_citekey . drop_duplicates () . map ( lambda x : x . split ( ':' )[ 0 ]) . pipe ( collections . Counter ) ) ref_counts [ 'total' ] = sum ( ref_counts . values ()) stats [ 'reference_counts' ] = ref_counts stats [ 'word_count' ] = len ( text . split ()) logging . info ( f \"Generated manscript stats: \\n {json.dumps(stats, indent=2)}\" ) return stats def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo ) Variables citekey_pattern Functions datetime_now def datetime_now ( ) Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 View Source def datetime_now () : \"\"\" Return the current datetime , with timezone awareness https : // stackoverflow . com / a / 39079819 / 4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo ) get_citekeys def get_citekeys ( text ) Extract the deduplicated list of citations in a text. Citations that are clearly invalid such as doi:/453 are not returned. View Source def get_citekeys ( text ) : \"\"\" Extract the deduplicated list of citations in a text . Citations that are clearly invalid such as ` doi : / 453 ` are not returned . \"\"\" citekeys = set ( citekey_pattern . findall ( text )) citekeys = filter ( lambda x : is_valid_citekey ( x , allow_tag = True , allow_raw = True , allow_pandoc_xnos = True ) , citekeys , ) return sorted ( citekeys ) get_manuscript_stats def get_manuscript_stats ( text , citekeys_df ) Compute manuscript statistics. View Source def get_manuscript_stats ( text , citekeys_df ) : \"\"\" Compute manuscript statistics . \"\"\" stats = collections . OrderedDict () # Number of distinct references by type ref_counts = ( citekeys_df . standard_citekey . drop_duplicates () . map ( lambda x : x . split ( ' : ' ) [ 0 ] ) . pipe ( collections . Counter ) ) ref_counts [ ' total ' ] = sum ( ref_counts . values ()) stats [ ' reference_counts ' ] = ref_counts stats [ ' word_count ' ] = len ( text . split ()) logging . info ( f \" Generated manscript stats: \\n {json.dumps(stats, indent=2)} \" ) return stats get_text def get_text ( directory ) Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. View Source def get_text ( directory ) : \"\"\" Return a concatenated string of section texts from the specified directory . Text files should be UTF - 8 encoded . \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( ' [0-9]*.md ' )) name_to_text = collections . OrderedDict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = ' utf-8-sig ' ) logging . info ( ' Manuscript content parts: \\n ' + ' \\n ' . join ( name_to_text )) return ' \\n\\n ' . join ( name_to_text . values ()) + ' \\n ' update_manuscript_citekeys def update_manuscript_citekeys ( text , old_to_new ) Replace citation keys according to the old_to_new dictionary. Useful for converting citation keys to shortened versions that are appropriate for pandoc. text is markdown source text old_to_new is a dictionary like: doi:10.7287/peerj.preprints.3100v1 \u2192 11cb5HXoY View Source def update_manuscript_citekeys ( text , old_to_new ) : \"\"\" Replace citation keys according to the old_to_new dictionary . Useful for converting citation keys to shortened versions that are appropriate for pandoc . ` text ` is markdown source text ` old_to_new ` is a dictionary like : doi : 10 . 7287 / peerj . preprints . 3100 v1 \u2192 11 cb5HXoY \"\"\" for old , new in old_to_new . items () : text = re . sub ( pattern = re . escape ( ' @ ' + old ) + r ' (?![\\w:.#$%&\\-+?<>~/]*[a-zA-Z0-9/]) ' , repl = ' @ ' + new , string = text , ) return text","title":"Manuscript"},{"location":"reference/manubot/process/manuscript/#module-manubotprocessmanuscript","text":"View Source import collections import datetime import json import logging import pathlib import re from manubot.cite.citekey import citekey_pattern , is_valid_citekey def get_citekeys ( text ): \"\"\" Extract the deduplicated list of citations in a text. Citations that are clearly invalid such as `doi:/453` are not returned. \"\"\" citekeys = set ( citekey_pattern . findall ( text )) citekeys = filter ( lambda x : is_valid_citekey ( x , allow_tag = True , allow_raw = True , allow_pandoc_xnos = True ), citekeys , ) return sorted ( citekeys ) def get_text ( directory ): \"\"\" Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( '[0-9]*.md' )) name_to_text = collections . OrderedDict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = 'utf-8-sig' ) logging . info ( 'Manuscript content parts: \\n ' + ' \\n ' . join ( name_to_text )) return ' \\n\\n ' . join ( name_to_text . values ()) + ' \\n ' def update_manuscript_citekeys ( text , old_to_new ): \"\"\" Replace citation keys according to the old_to_new dictionary. Useful for converting citation keys to shortened versions that are appropriate for pandoc. `text` is markdown source text `old_to_new` is a dictionary like: doi:10.7287/peerj.preprints.3100v1 \u2192 11cb5HXoY \"\"\" for old , new in old_to_new . items (): text = re . sub ( pattern = re . escape ( '@' + old ) + r '(?![\\w:.#$%&\\-+?<>~/]*[a-zA-Z0-9/])' , repl = '@' + new , string = text , ) return text def get_manuscript_stats ( text , citekeys_df ): \"\"\" Compute manuscript statistics. \"\"\" stats = collections . OrderedDict () # Number of distinct references by type ref_counts = ( citekeys_df . standard_citekey . drop_duplicates () . map ( lambda x : x . split ( ':' )[ 0 ]) . pipe ( collections . Counter ) ) ref_counts [ 'total' ] = sum ( ref_counts . values ()) stats [ 'reference_counts' ] = ref_counts stats [ 'word_count' ] = len ( text . split ()) logging . info ( f \"Generated manscript stats: \\n {json.dumps(stats, indent=2)}\" ) return stats def datetime_now (): \"\"\" Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo )","title":"Module manubot.process.manuscript"},{"location":"reference/manubot/process/manuscript/#variables","text":"citekey_pattern","title":"Variables"},{"location":"reference/manubot/process/manuscript/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/manuscript/#datetime_now","text":"def datetime_now ( ) Return the current datetime, with timezone awareness https://stackoverflow.com/a/39079819/4651668 View Source def datetime_now () : \"\"\" Return the current datetime , with timezone awareness https : // stackoverflow . com / a / 39079819 / 4651668 \"\"\" tzinfo = datetime . datetime . now ( datetime . timezone . utc ) . astimezone () . tzinfo return datetime . datetime . now ( tzinfo )","title":"datetime_now"},{"location":"reference/manubot/process/manuscript/#get_citekeys","text":"def get_citekeys ( text ) Extract the deduplicated list of citations in a text. Citations that are clearly invalid such as doi:/453 are not returned. View Source def get_citekeys ( text ) : \"\"\" Extract the deduplicated list of citations in a text . Citations that are clearly invalid such as ` doi : / 453 ` are not returned . \"\"\" citekeys = set ( citekey_pattern . findall ( text )) citekeys = filter ( lambda x : is_valid_citekey ( x , allow_tag = True , allow_raw = True , allow_pandoc_xnos = True ) , citekeys , ) return sorted ( citekeys )","title":"get_citekeys"},{"location":"reference/manubot/process/manuscript/#get_manuscript_stats","text":"def get_manuscript_stats ( text , citekeys_df ) Compute manuscript statistics. View Source def get_manuscript_stats ( text , citekeys_df ) : \"\"\" Compute manuscript statistics . \"\"\" stats = collections . OrderedDict () # Number of distinct references by type ref_counts = ( citekeys_df . standard_citekey . drop_duplicates () . map ( lambda x : x . split ( ' : ' ) [ 0 ] ) . pipe ( collections . Counter ) ) ref_counts [ ' total ' ] = sum ( ref_counts . values ()) stats [ ' reference_counts ' ] = ref_counts stats [ ' word_count ' ] = len ( text . split ()) logging . info ( f \" Generated manscript stats: \\n {json.dumps(stats, indent=2)} \" ) return stats","title":"get_manuscript_stats"},{"location":"reference/manubot/process/manuscript/#get_text","text":"def get_text ( directory ) Return a concatenated string of section texts from the specified directory. Text files should be UTF-8 encoded. View Source def get_text ( directory ) : \"\"\" Return a concatenated string of section texts from the specified directory . Text files should be UTF - 8 encoded . \"\"\" section_dir = pathlib . Path ( directory ) paths = sorted ( section_dir . glob ( ' [0-9]*.md ' )) name_to_text = collections . OrderedDict () for path in paths : name_to_text [ path . stem ] = path . read_text ( encoding = ' utf-8-sig ' ) logging . info ( ' Manuscript content parts: \\n ' + ' \\n ' . join ( name_to_text )) return ' \\n\\n ' . join ( name_to_text . values ()) + ' \\n '","title":"get_text"},{"location":"reference/manubot/process/manuscript/#update_manuscript_citekeys","text":"def update_manuscript_citekeys ( text , old_to_new ) Replace citation keys according to the old_to_new dictionary. Useful for converting citation keys to shortened versions that are appropriate for pandoc. text is markdown source text old_to_new is a dictionary like: doi:10.7287/peerj.preprints.3100v1 \u2192 11cb5HXoY View Source def update_manuscript_citekeys ( text , old_to_new ) : \"\"\" Replace citation keys according to the old_to_new dictionary . Useful for converting citation keys to shortened versions that are appropriate for pandoc . ` text ` is markdown source text ` old_to_new ` is a dictionary like : doi : 10 . 7287 / peerj . preprints . 3100 v1 \u2192 11 cb5HXoY \"\"\" for old , new in old_to_new . items () : text = re . sub ( pattern = re . escape ( ' @ ' + old ) + r ' (?![\\w:.#$%&\\-+?<>~/]*[a-zA-Z0-9/]) ' , repl = ' @ ' + new , string = text , ) return text","title":"update_manuscript_citekeys"},{"location":"reference/manubot/process/process_command/","text":"Module manubot.process.process_command View Source import logging def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f 'content directory does not exist: {content_dir}' ) args_dict [ 'citation_tags_path' ] = content_dir . joinpath ( 'citation-tags.tsv' ) args_dict [ 'meta_yaml_path' ] = content_dir . joinpath ( 'metadata.yaml' ) args_dict [ 'manual_references_paths' ] = sorted ( content_dir . rglob ( 'manual-references*.*' )) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ 'manuscript_path' ] = output_dir . joinpath ( 'manuscript.md' ) args_dict [ 'citations_path' ] = output_dir . joinpath ( 'citations.tsv' ) args_dict [ 'references_path' ] = output_dir . joinpath ( 'references.json' ) args_dict [ 'variables_path' ] = output_dir . joinpath ( 'variables.json' ) # Set paths for caching args_dict [ 'cache_directory' ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ 'requests_cache_path' ] = str ( args . cache_directory . joinpath ( 'requests-cache' )) from manubot.process.util import prepare_manuscript prepare_manuscript ( args ) Functions cli_process def cli_process ( args ) View Source def cli_process ( args ) : args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir () : logging . warning ( f ' content directory does not exist: {content_dir} ' ) args_dict [ ' citation_tags_path ' ] = content_dir . joinpath ( ' citation-tags.tsv ' ) args_dict [ ' meta_yaml_path ' ] = content_dir . joinpath ( ' metadata.yaml ' ) args_dict [ ' manual_references_paths ' ] = sorted ( content_dir . rglob ( ' manual-references*.* ' )) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ ' manuscript_path ' ] = output_dir . joinpath ( ' manuscript.md ' ) args_dict [ ' citations_path ' ] = output_dir . joinpath ( ' citations.tsv ' ) args_dict [ ' references_path ' ] = output_dir . joinpath ( ' references.json ' ) args_dict [ ' variables_path ' ] = output_dir . joinpath ( ' variables.json ' ) # Set paths for caching args_dict [ ' cache_directory ' ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ ' requests_cache_path ' ] = str ( args . cache_directory . joinpath ( ' requests-cache ' )) from manubot . process . util import prepare_manuscript prepare_manuscript ( args )","title":"Process Command"},{"location":"reference/manubot/process/process_command/#module-manubotprocessprocess_command","text":"View Source import logging def cli_process ( args ): args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir (): logging . warning ( f 'content directory does not exist: {content_dir}' ) args_dict [ 'citation_tags_path' ] = content_dir . joinpath ( 'citation-tags.tsv' ) args_dict [ 'meta_yaml_path' ] = content_dir . joinpath ( 'metadata.yaml' ) args_dict [ 'manual_references_paths' ] = sorted ( content_dir . rglob ( 'manual-references*.*' )) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ 'manuscript_path' ] = output_dir . joinpath ( 'manuscript.md' ) args_dict [ 'citations_path' ] = output_dir . joinpath ( 'citations.tsv' ) args_dict [ 'references_path' ] = output_dir . joinpath ( 'references.json' ) args_dict [ 'variables_path' ] = output_dir . joinpath ( 'variables.json' ) # Set paths for caching args_dict [ 'cache_directory' ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ 'requests_cache_path' ] = str ( args . cache_directory . joinpath ( 'requests-cache' )) from manubot.process.util import prepare_manuscript prepare_manuscript ( args )","title":"Module manubot.process.process_command"},{"location":"reference/manubot/process/process_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/process_command/#cli_process","text":"def cli_process ( args ) View Source def cli_process ( args ) : args_dict = vars ( args ) # Set paths for content content_dir = args . content_directory if not content_dir . is_dir () : logging . warning ( f ' content directory does not exist: {content_dir} ' ) args_dict [ ' citation_tags_path ' ] = content_dir . joinpath ( ' citation-tags.tsv ' ) args_dict [ ' meta_yaml_path ' ] = content_dir . joinpath ( ' metadata.yaml ' ) args_dict [ ' manual_references_paths ' ] = sorted ( content_dir . rglob ( ' manual-references*.* ' )) # Set paths for output output_dir = args . output_directory output_dir . mkdir ( parents = True , exist_ok = True ) args_dict [ ' manuscript_path ' ] = output_dir . joinpath ( ' manuscript.md ' ) args_dict [ ' citations_path ' ] = output_dir . joinpath ( ' citations.tsv ' ) args_dict [ ' references_path ' ] = output_dir . joinpath ( ' references.json ' ) args_dict [ ' variables_path ' ] = output_dir . joinpath ( ' variables.json ' ) # Set paths for caching args_dict [ ' cache_directory ' ] = args . cache_directory or output_dir args . cache_directory . mkdir ( parents = True , exist_ok = True ) args_dict [ ' requests_cache_path ' ] = str ( args . cache_directory . joinpath ( ' requests-cache ' )) from manubot . process . util import prepare_manuscript prepare_manuscript ( args )","title":"cli_process"},{"location":"reference/manubot/process/util/","text":"Module manubot.process.util View Source import collections import json import logging import pathlib import re import textwrap import warnings import jinja2 import pandas import requests import requests_cache import yaml from manubot.process.bibliography import ( load_manual_references , ) from manubot.process.ci import ( add_manuscript_urls_to_ci_params , get_continuous_integration_parameters , ) from manubot.process.manuscript import ( datetime_now , get_citekeys , get_manuscript_stats , get_text , update_manuscript_citekeys , ) from manubot.cite.citekey import ( citekey_to_csl_item , shorten_citekey , is_valid_citekey , standardize_citekey , ) def check_collisions ( citekeys_df ): \"\"\" Check for short_citekey hash collisions \"\"\" collision_df = citekeys_df [[ 'standard_citekey' , 'short_citekey' ]] . drop_duplicates () collision_df = collision_df [ collision_df . short_citekey . duplicated ( keep = False )] if not collision_df . empty : logging . error ( f 'OMF! Hash collision. Congratulations. \\n {collision_df}' ) return collision_df def check_multiple_citation_strings ( citekeys_df ): \"\"\" Identify different citation strings referring the the same reference. \"\"\" message = textwrap . dedent ( f ''' \\ {len(citekeys_df)} unique citations strings extracted from text {citekeys_df.standard_citekey.nunique()} unique standard citations \\ ''' ) logging . info ( message ) multi_df = citekeys_df [ citekeys_df . standard_citekey . duplicated ( keep = False )] if not multi_df . empty : table = multi_df . to_string ( index = False , columns = [ 'standard_citekey' , 'manuscript_citekey' ]) logging . warning ( f 'Multiple citekeys detected for the same reference: \\n {table}' ) return multi_df def read_json ( path ): \"\"\" Read json from a path or URL. \"\"\" if re . match ( '^(http|ftp)s?://' , path ): response = requests . get ( path ) obj = response . json ( object_pairs_hook = collections . OrderedDict ) else : path = pathlib . Path ( path ) with path . open ( encoding = 'utf-8-sig' ) as read_file : obj = json . load ( read_file , object_pairs_hook = collections . OrderedDict ) return obj def read_jsons ( paths ): \"\"\" Read multiple JSON files into a user_variables dictionary. Provide a list of paths (URLs or filepaths). Paths can optionally have a namespace prepended. For example: ``` paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). \"\"\" user_variables = collections . OrderedDict () for path in paths : logging . info ( f 'Read the following user-provided templating variables for {path}' ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r '([a-zA-Z_][a-zA-Z0-9_]*)=(.+)' , path ) if match : namespace , path = match . groups () logging . info ( f 'Using the \"{namespace}\" namespace for template variables from {path}' ) try : obj = read_json ( path ) except Exception : logging . exception ( f 'Error reading template variables from {path}' ) continue if match : obj = { namespace : obj } assert isinstance ( obj , dict ) conflicts = user_variables . keys () & obj . keys () if conflicts : logging . warning ( f 'Template variables in {path} overwrite existing ' 'values for the following keys: \\n ' + ' \\n ' . join ( conflicts ) ) user_variables . update ( obj ) logging . info ( f 'Reading user-provided templating variables complete: \\n ' f '{json.dumps(user_variables, indent=2, ensure_ascii=False)}' ) return user_variables def add_author_affiliations ( variables ): \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" rows = list () for author in variables [ 'authors' ]: if 'affiliations' not in author : continue if not isinstance ( author [ 'affiliations' ], list ): warnings . warn ( f \"Expected list for {author['name']}'s affiliations. \" f \"Assuming multiple affiliations are `; ` separated. \" f \"Please switch affiliations to a list.\" , category = DeprecationWarning ) author [ 'affiliations' ] = author [ 'affiliations' ] . split ( '; ' ) for affiliation in author [ 'affiliations' ]: rows . append (( author [ 'name' ], affiliation )) if not rows : return variables affil_map_df = pandas . DataFrame ( rows , columns = [ 'name' , 'affiliation' ]) affiliation_df = affil_map_df [[ 'affiliation' ]] . drop_duplicates () affiliation_df [ 'affiliation_number' ] = range ( 1 , 1 + len ( affiliation_df )) affil_map_df = affil_map_df . merge ( affiliation_df ) name_to_numbers = { name : sorted ( df . affiliation_number ) for name , df in affil_map_df . groupby ( 'name' )} for author in variables [ 'authors' ]: author [ 'affiliation_numbers' ] = name_to_numbers . get ( author [ 'name' ], []) variables [ 'affiliations' ] = affiliation_df . to_dict ( orient = 'records' ) return variables def get_metadata_and_variables ( args ): \"\"\" Process metadata.yaml and create variables available for jinja2 templating. \"\"\" # Generated manuscript variables variables = collections . OrderedDict () # Read metadata which contains pandoc_yaml_metadata # as well as author_info. if args . meta_yaml_path . is_file (): with args . meta_yaml_path . open ( encoding = 'utf-8-sig' ) as read_file : metadata = yaml . safe_load ( read_file ) assert isinstance ( metadata , dict ) else : metadata = {} logging . warning ( f 'missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc' ) # Add date to metadata now = datetime_now () logging . info ( f 'Using {now:%Z} timezone. \\n ' f 'Dating manuscript with the current datetime: {now.isoformat()}' ) metadata [ 'date-meta' ] = now . date () . isoformat () variables [ 'date' ] = f '{now:%B} {now.day}, {now.year}' # Process authors metadata authors = metadata . pop ( 'author_info' , []) if authors is None : authors = [] metadata [ 'author-meta' ] = [ author [ 'name' ] for author in authors ] variables [ 'authors' ] = authors variables = add_author_affiliations ( variables ) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ 'ci_source' ] = add_manuscript_urls_to_ci_params ( ci_params ) # Update variables with user-provided variables here user_variables = read_jsons ( args . template_variables_path ) variables . update ( user_variables ) return metadata , variables def get_citekeys_df ( args , text ): \"\"\" Generate citekeys_df and save it to 'citations.tsv'. citekeys_df is a pandas.DataFrame with the following columns: - manuscript_citekey: citation keys extracted from the manuscript content files. - detagged_citekey: manuscript_citekey but with tag citekeys dereferenced - standard_citekey: detagged_citekey standardized - short_citekey: standard_citekey hashed to create a shortened citekey \"\"\" citekeys_df = pandas . DataFrame ( { 'manuscript_citekey' : get_citekeys ( text )} ) if args . citation_tags_path . is_file (): tag_df = pandas . read_csv ( args . citation_tags_path , sep = ' \\t ' ) na_rows_df = tag_df [ tag_df . isnull () . any ( axis = 'columns' )] if not na_rows_df . empty : logging . error ( f '{args.citation_tags_path} contains rows with missing values: \\n ' f '{na_rows_df} \\n ' 'This error can be caused by using spaces rather than tabs to delimit fields. \\n ' 'Proceeding to reread TSV with delim_whitespace=True.' ) tag_df = pandas . read_csv ( args . citation_tags_path , delim_whitespace = True ) tag_df [ 'manuscript_citekey' ] = 'tag:' + tag_df . tag tag_df = tag_df . rename ( columns = { 'citation' : 'detagged_citekey' }) for detagged_citekey in tag_df . detagged_citekey : is_valid_citekey ( detagged_citekey , allow_raw = True ) citekeys_df = citekeys_df . merge ( tag_df [[ 'manuscript_citekey' , 'detagged_citekey' ]], how = 'left' ) else : citekeys_df [ 'detagged_citekey' ] = None logging . info ( f 'missing {args.citation_tags_path} file: no citation tags (citekey aliases) set' ) citekeys_df . detagged_citekey . fillna ( citekeys_df . manuscript_citekey . astype ( str ), inplace = True ) citekeys_df [ 'standard_citekey' ] = citekeys_df . detagged_citekey . map ( standardize_citekey ) citekeys_df [ 'short_citekey' ] = citekeys_df . standard_citekey . map ( shorten_citekey ) citekeys_df = citekeys_df . sort_values ([ 'standard_citekey' , 'detagged_citekey' ]) citekeys_df . to_csv ( args . citations_path , sep = ' \\t ' , index = False ) check_collisions ( citekeys_df ) check_multiple_citation_strings ( citekeys_df ) return citekeys_df def generate_csl_items ( args , citekeys_df ): \"\"\" General CSL (citeproc) items for standard_citekeys in citekeys_df. Writes references.json to disk and logs warnings for potential problems. \"\"\" # Read manual references (overrides) in JSON CSL manual_refs = load_manual_references ( args . manual_references_paths ) requests_cache . install_cache ( args . requests_cache_path , include_get_headers = True ) cache = requests_cache . get_cache () if args . clear_requests_cache : logging . info ( 'Clearing requests-cache' ) requests_cache . clear () logging . info ( f 'requests-cache starting with {len(cache.responses)} cached responses' ) csl_items = list () failures = list () for standard_citekey in citekeys_df . standard_citekey . unique (): if standard_citekey in manual_refs : csl_items . append ( manual_refs [ standard_citekey ]) continue elif standard_citekey . startswith ( 'raw:' ): logging . error ( f 'CSL JSON Data with a standard_citekey of {standard_citekey!r} not found in manual-references.json. ' 'Metadata must be provided for raw citekeys.' ) failures . append ( standard_citekey ) try : csl_item = citekey_to_csl_item ( standard_citekey ) csl_items . append ( csl_item ) except Exception : logging . exception ( f 'Citeproc retrieval failure for {standard_citekey!r}' ) failures . append ( standard_citekey ) logging . info ( f 'requests-cache finished with {len(cache.responses)} cached responses' ) requests_cache . uninstall_cache () if failures : message = 'CSL JSON Data retrieval failed for the following standardized citation keys: \\n {}' . format ( ' \\n ' . join ( failures )) logging . error ( message ) # Write JSON CSL bibliography for Pandoc. with args . references_path . open ( 'w' , encoding = 'utf-8' ) as write_file : json . dump ( csl_items , write_file , indent = 2 , ensure_ascii = False ) write_file . write ( ' \\n ' ) return csl_items def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), comment_start_string = '{##' , comment_end_string = '##}' , ) template = jinja_environment . from_string ( text ) return template . render ( ** variables ) def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) citekeys_df = get_citekeys_df ( args , text ) generate_csl_items ( args , citekeys_df ) citekey_mapping = collections . OrderedDict ( zip ( citekeys_df . manuscript_citekey , citekeys_df . short_citekey )) text = update_manuscript_citekeys ( text , citekey_mapping ) metadata , variables = get_metadata_and_variables ( args ) variables [ 'manuscript_stats' ] = get_manuscript_stats ( text , citekeys_df ) with args . variables_path . open ( 'w' , encoding = 'utf-8' ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( 'w' , encoding = 'utf-8' ) as write_file : yaml . dump ( metadata , write_file , default_flow_style = False , explicit_start = True , explicit_end = True ) write_file . write ( ' \\n ' ) write_file . write ( text ) Functions add_author_affiliations def add_author_affiliations ( variables ) Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. View Source def add_author_affiliations ( variables ) : \"\"\" Edit variables to contain numbered author affiliations . Specifically , add a list of affiliation_numbers for each author and add a list of affiliations to the top - level of variables . If no authors have any affiliations , variables is left unmodified . \"\"\" rows = list () for author in variables [ ' authors ' ]: if ' affiliations ' not in author : continue if not isinstance ( author [ ' affiliations ' ], list ) : warnings . warn ( f \" Expected list for {author['name']}'s affiliations. \" f \" Assuming multiple affiliations are `; ` separated. \" f \" Please switch affiliations to a list. \" , category = DeprecationWarning ) author [ ' affiliations ' ] = author [ ' affiliations ' ]. split ( ' ; ' ) for affiliation in author [ ' affiliations ' ]: rows . append (( author [ ' name ' ], affiliation )) if not rows : return variables affil_map_df = pandas . DataFrame ( rows , columns = [ ' name ' , ' affiliation ' ] ) affiliation_df = affil_map_df [[ ' affiliation ' ]]. drop_duplicates () affiliation_df [ ' affiliation_number ' ] = range ( 1 , 1 + len ( affiliation_df )) affil_map_df = affil_map_df . merge ( affiliation_df ) name_to_numbers = { name : sorted ( df . affiliation_number ) for name , df in affil_map_df . groupby ( ' name ' ) } for author in variables [ ' authors ' ]: author [ ' affiliation_numbers ' ] = name_to_numbers . get ( author [ ' name ' ], [] ) variables [ ' affiliations ' ] = affiliation_df . to_dict ( orient = ' records ' ) return variables check_collisions def check_collisions ( citekeys_df ) Check for short_citekey hash collisions View Source def check_collisions ( citekeys_df ) : \"\"\" Check for short_citekey hash collisions \"\"\" collision_df = citekeys_df [[ ' standard_citekey ' , ' short_citekey ' ]]. drop_duplicates () collision_df = collision_df [ collision_df . short_citekey . duplicated ( keep = False ) ] if not collision_df . empty : logging . error ( f ' OMF! Hash collision. Congratulations. \\n {collision_df} ' ) return collision_df check_multiple_citation_strings def check_multiple_citation_strings ( citekeys_df ) Identify different citation strings referring the the same reference. View Source def check_multiple_citation_strings ( citekeys_df ) : \"\"\" Identify different citation strings referring the the same reference . \"\"\" message = textwrap . dedent ( f ''' \\ { len ( citekeys_df ) } unique citations strings extracted from text { citekeys_df . standard_citekey . nunique () } unique standard citations \\ ''' ) logging . info ( message ) multi_df = citekeys_df [ citekeys_df . standard_citekey . duplicated ( keep = False ) ] if not multi_df . empty : table = multi_df . to_string ( index = False , columns = [ ' standard_citekey ' , ' manuscript_citekey ' ] ) logging . warning ( f ' Multiple citekeys detected for the same reference: \\n {table} ' ) return multi_df generate_csl_items def generate_csl_items ( args , citekeys_df ) General CSL (citeproc) items for standard_citekeys in citekeys_df. Writes references.json to disk and logs warnings for potential problems. View Source def generate_csl_items ( args , citekeys_df ) : \"\"\" General CSL ( citeproc ) items for standard_citekeys in citekeys_df . Writes references . json to disk and logs warnings for potential problems . \"\"\" # Read manual references ( overrides ) in JSON CSL manual_refs = load_manual_references ( args . manual_references_paths ) requests_cache . install_cache ( args . requests_cache_path , include_get_headers = True ) cache = requests_cache . get_cache () if args . clear_requests_cache : logging . info ( ' Clearing requests-cache ' ) requests_cache . clear () logging . info ( f ' requests-cache starting with {len(cache.responses)} cached responses ' ) csl_items = list () failures = list () for standard_citekey in citekeys_df . standard_citekey . unique () : if standard_citekey in manual_refs : csl_items . append ( manual_refs [ standard_citekey ] ) continue elif standard_citekey . startswith ( ' raw: ' ) : logging . error ( f ' CSL JSON Data with a standard_citekey of {standard_citekey!r} not found in manual-references.json. ' ' Metadata must be provided for raw citekeys. ' ) failures . append ( standard_citekey ) try : csl_item = citekey_to_csl_item ( standard_citekey ) csl_items . append ( csl_item ) except Exception : logging . exception ( f ' Citeproc retrieval failure for {standard_citekey!r} ' ) failures . append ( standard_citekey ) logging . info ( f ' requests-cache finished with {len(cache.responses)} cached responses ' ) requests_cache . uninstall_cache () if failures : message = ' CSL JSON Data retrieval failed for the following standardized citation keys: \\n {} ' . format ( ' \\n ' . join ( failures )) logging . error ( message ) # Write JSON CSL bibliography for Pandoc . with args . references_path . open ( ' w ' , encoding = ' utf-8 ' ) as write_file : json . dump ( csl_items , write_file , indent = 2 , ensure_ascii = False ) write_file . write ( ' \\n ' ) return csl_items get_citekeys_df def get_citekeys_df ( args , text ) Generate citekeys_df and save it to 'citations.tsv'. citekeys_df is a pandas.DataFrame with the following columns: - manuscript_citekey: citation keys extracted from the manuscript content files. - detagged_citekey: manuscript_citekey but with tag citekeys dereferenced - standard_citekey: detagged_citekey standardized - short_citekey: standard_citekey hashed to create a shortened citekey View Source def get_citekeys_df ( args , text ) : \"\"\" Generate citekeys_df and save it to ' citations.tsv ' . citekeys_df is a pandas . DataFrame with the following columns : - manuscript_citekey : citation keys extracted from the manuscript content files . - detagged_citekey : manuscript_citekey but with tag citekeys dereferenced - standard_citekey : detagged_citekey standardized - short_citekey : standard_citekey hashed to create a shortened citekey \"\"\" citekeys_df = pandas . DataFrame ( { ' manuscript_citekey ' : get_citekeys ( text ) } ) if args . citation_tags_path . is_file () : tag_df = pandas . read_csv ( args . citation_tags_path , sep = ' \\t ' ) na_rows_df = tag_df [ tag_df . isnull () . any ( axis = ' columns ' ) ] if not na_rows_df . empty : logging . error ( f ' {args.citation_tags_path} contains rows with missing values: \\n ' f ' {na_rows_df} \\n ' ' This error can be caused by using spaces rather than tabs to delimit fields. \\n ' ' Proceeding to reread TSV with delim_whitespace=True. ' ) tag_df = pandas . read_csv ( args . citation_tags_path , delim_whitespace = True ) tag_df [ ' manuscript_citekey ' ] = ' tag: ' + tag_df . tag tag_df = tag_df . rename ( columns = { ' citation ' : ' detagged_citekey ' } ) for detagged_citekey in tag_df . detagged_citekey : is_valid_citekey ( detagged_citekey , allow_raw = True ) citekeys_df = citekeys_df . merge ( tag_df [[ ' manuscript_citekey ' , ' detagged_citekey ' ]], how = ' left ' ) else : citekeys_df [ ' detagged_citekey ' ] = None logging . info ( f ' missing {args.citation_tags_path} file: no citation tags (citekey aliases) set ' ) citekeys_df . detagged_citekey . fillna ( citekeys_df . manuscript_citekey . astype ( str ) , inplace = True ) citekeys_df [ ' standard_citekey ' ] = citekeys_df . detagged_citekey . map ( standardize_citekey ) citekeys_df [ ' short_citekey ' ] = citekeys_df . standard_citekey . map ( shorten_citekey ) citekeys_df = citekeys_df . sort_values ( [ ' standard_citekey ' , ' detagged_citekey ' ] ) citekeys_df . to_csv ( args . citations_path , sep = ' \\t ' , index = False ) check_collisions ( citekeys_df ) check_multiple_citation_strings ( citekeys_df ) return citekeys_df get_metadata_and_variables def get_metadata_and_variables ( args ) Process metadata.yaml and create variables available for jinja2 templating. View Source def get_metadata_and_variables ( args ) : \"\"\" Process metadata . yaml and create variables available for jinja2 templating . \"\"\" # Generated manuscript variables variables = collections . OrderedDict () # Read metadata which contains pandoc_yaml_metadata # as well as author_info . if args . meta_yaml_path . is_file () : with args . meta_yaml_path . open ( encoding = ' utf-8-sig ' ) as read_file : metadata = yaml . safe_load ( read_file ) assert isinstance ( metadata , dict ) else : metadata = {} logging . warning ( f ' missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc ' ) # Add date to metadata now = datetime_now () logging . info ( f ' Using {now:%Z} timezone. \\n ' f ' Dating manuscript with the current datetime: {now.isoformat()} ' ) metadata [ ' date-meta ' ] = now . date () . isoformat () variables [ ' date ' ] = f ' {now:%B} {now.day}, {now.year} ' # Process authors metadata authors = metadata . pop ( ' author_info ' , [] ) if authors is None : authors = [] metadata [ ' author-meta ' ] = [ author [ ' name ' ] for author in authors ] variables [ ' authors ' ] = authors variables = add_author_affiliations ( variables ) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ ' ci_source ' ] = add_manuscript_urls_to_ci_params ( ci_params ) # Update variables with user - provided variables here user_variables = read_jsons ( args . template_variables_path ) variables . update ( user_variables ) return metadata , variables prepare_manuscript def prepare_manuscript ( args ) Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. View Source def prepare_manuscript ( args ) : \"\"\" Compile manuscript , creating manuscript . md and references . json as inputs for pandoc . \"\"\" text = get_text ( args . content_directory ) citekeys_df = get_citekeys_df ( args , text ) generate_csl_items ( args , citekeys_df ) citekey_mapping = collections . OrderedDict ( zip ( citekeys_df . manuscript_citekey , citekeys_df . short_citekey )) text = update_manuscript_citekeys ( text , citekey_mapping ) metadata , variables = get_metadata_and_variables ( args ) variables [ ' manuscript_stats ' ] = get_manuscript_stats ( text , citekeys_df ) with args . variables_path . open ( ' w ' , encoding = ' utf-8 ' ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( ' w ' , encoding = ' utf-8 ' ) as write_file : yaml . dump ( metadata , write_file , default_flow_style = False , explicit_start = True , explicit_end = True ) write_file . write ( ' \\n ' ) write_file . write ( text ) read_json def read_json ( path ) Read json from a path or URL. View Source def read_json ( path ) : \"\"\" Read json from a path or URL . \"\"\" if re . match ( ' ^(http|ftp)s?:// ' , path ) : response = requests . get ( path ) obj = response . json ( object_pairs_hook = collections . OrderedDict ) else : path = pathlib . Path ( path ) with path . open ( encoding = ' utf-8-sig ' ) as read_file : obj = json . load ( read_file , object_pairs_hook = collections . OrderedDict ) return obj read_jsons def read_jsons ( paths ) Read multiple JSON files into a user_variables dictionary. Provide a list of paths (URLs or filepaths). Paths can optionally have a namespace prepended. For example: paths = [ 'https://git.io/vbkqm' , # update the dictionary 's top-level ' namespace_1 = https : // git . io / vbkqm ', # store under ' namespace_1 ' key ' namespace_2 = some_local_path . json ', # store under ' namespace_2 ' key ] If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). View Source def read_jsons ( paths ) : \"\"\" Read multiple JSON files into a user_variables dictionary . Provide a list of paths ( URLs or filepaths ) . Paths can optionally have a namespace prepended . For example : ``` paths = [ ' https://git.io/vbkqm ' , # update the dictionary ' s top-level ' namespace_1=https://git.io/vbkqm ' , # store under ' namespace_1 ' key ' namespace_2=some_local_path.json ' , # store under ' namespace_2 ' key ] ``` If a namespace is not provided , the JSON must contain a dictionary as its top level . Namespaces should consist only of ASCII alphanumeric characters ( includes underscores , first character cannot be numeric ) . \"\"\" user_variables = collections . OrderedDict () for path in paths : logging . info ( f ' Read the following user-provided templating variables for {path} ' ) # Match only namespaces that are valid jinja2 variable names # http : // jinja . pocoo . org / docs / 2 . 10 / api / # identifier - naming match = re . match ( r ' ([a-zA-Z_][a-zA-Z0-9_]*)=(.+) ' , path ) if match : namespace , path = match . groups () logging . info ( f ' Using the \"{namespace}\" namespace for template variables from {path} ' ) try : obj = read_json ( path ) except Exception : logging . exception ( f ' Error reading template variables from {path} ' ) continue if match : obj = { namespace : obj } assert isinstance ( obj , dict ) conflicts = user_variables . keys () & obj . keys () if conflicts : logging . warning ( f ' Template variables in {path} overwrite existing ' ' values for the following keys: \\n ' + ' \\n ' . join ( conflicts ) ) user_variables . update ( obj ) logging . info ( f ' Reading user-provided templating variables complete: \\n ' f ' {json.dumps(user_variables, indent=2, ensure_ascii=False)} ' ) return user_variables template_with_jinja2 def template_with_jinja2 ( text , variables ) Template using jinja2 with the variables dictionary unpacked as keyword arguments. View Source def template_with_jinja2 ( text , variables ) : \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments . \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader () , undefined = jinja2 . make_logging_undefined ( logging . getLogger ()) , comment_start_string = ' {## ' , comment_end_string = ' ##} ' , ) template = jinja_environment . from_string ( text ) return template . render ( ** variables )","title":"Util"},{"location":"reference/manubot/process/util/#module-manubotprocessutil","text":"View Source import collections import json import logging import pathlib import re import textwrap import warnings import jinja2 import pandas import requests import requests_cache import yaml from manubot.process.bibliography import ( load_manual_references , ) from manubot.process.ci import ( add_manuscript_urls_to_ci_params , get_continuous_integration_parameters , ) from manubot.process.manuscript import ( datetime_now , get_citekeys , get_manuscript_stats , get_text , update_manuscript_citekeys , ) from manubot.cite.citekey import ( citekey_to_csl_item , shorten_citekey , is_valid_citekey , standardize_citekey , ) def check_collisions ( citekeys_df ): \"\"\" Check for short_citekey hash collisions \"\"\" collision_df = citekeys_df [[ 'standard_citekey' , 'short_citekey' ]] . drop_duplicates () collision_df = collision_df [ collision_df . short_citekey . duplicated ( keep = False )] if not collision_df . empty : logging . error ( f 'OMF! Hash collision. Congratulations. \\n {collision_df}' ) return collision_df def check_multiple_citation_strings ( citekeys_df ): \"\"\" Identify different citation strings referring the the same reference. \"\"\" message = textwrap . dedent ( f ''' \\ {len(citekeys_df)} unique citations strings extracted from text {citekeys_df.standard_citekey.nunique()} unique standard citations \\ ''' ) logging . info ( message ) multi_df = citekeys_df [ citekeys_df . standard_citekey . duplicated ( keep = False )] if not multi_df . empty : table = multi_df . to_string ( index = False , columns = [ 'standard_citekey' , 'manuscript_citekey' ]) logging . warning ( f 'Multiple citekeys detected for the same reference: \\n {table}' ) return multi_df def read_json ( path ): \"\"\" Read json from a path or URL. \"\"\" if re . match ( '^(http|ftp)s?://' , path ): response = requests . get ( path ) obj = response . json ( object_pairs_hook = collections . OrderedDict ) else : path = pathlib . Path ( path ) with path . open ( encoding = 'utf-8-sig' ) as read_file : obj = json . load ( read_file , object_pairs_hook = collections . OrderedDict ) return obj def read_jsons ( paths ): \"\"\" Read multiple JSON files into a user_variables dictionary. Provide a list of paths (URLs or filepaths). Paths can optionally have a namespace prepended. For example: ``` paths = [ 'https://git.io/vbkqm', # update the dictionary's top-level 'namespace_1=https://git.io/vbkqm', # store under 'namespace_1' key 'namespace_2=some_local_path.json', # store under 'namespace_2' key ] ``` If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). \"\"\" user_variables = collections . OrderedDict () for path in paths : logging . info ( f 'Read the following user-provided templating variables for {path}' ) # Match only namespaces that are valid jinja2 variable names # http://jinja.pocoo.org/docs/2.10/api/#identifier-naming match = re . match ( r '([a-zA-Z_][a-zA-Z0-9_]*)=(.+)' , path ) if match : namespace , path = match . groups () logging . info ( f 'Using the \"{namespace}\" namespace for template variables from {path}' ) try : obj = read_json ( path ) except Exception : logging . exception ( f 'Error reading template variables from {path}' ) continue if match : obj = { namespace : obj } assert isinstance ( obj , dict ) conflicts = user_variables . keys () & obj . keys () if conflicts : logging . warning ( f 'Template variables in {path} overwrite existing ' 'values for the following keys: \\n ' + ' \\n ' . join ( conflicts ) ) user_variables . update ( obj ) logging . info ( f 'Reading user-provided templating variables complete: \\n ' f '{json.dumps(user_variables, indent=2, ensure_ascii=False)}' ) return user_variables def add_author_affiliations ( variables ): \"\"\" Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. \"\"\" rows = list () for author in variables [ 'authors' ]: if 'affiliations' not in author : continue if not isinstance ( author [ 'affiliations' ], list ): warnings . warn ( f \"Expected list for {author['name']}'s affiliations. \" f \"Assuming multiple affiliations are `; ` separated. \" f \"Please switch affiliations to a list.\" , category = DeprecationWarning ) author [ 'affiliations' ] = author [ 'affiliations' ] . split ( '; ' ) for affiliation in author [ 'affiliations' ]: rows . append (( author [ 'name' ], affiliation )) if not rows : return variables affil_map_df = pandas . DataFrame ( rows , columns = [ 'name' , 'affiliation' ]) affiliation_df = affil_map_df [[ 'affiliation' ]] . drop_duplicates () affiliation_df [ 'affiliation_number' ] = range ( 1 , 1 + len ( affiliation_df )) affil_map_df = affil_map_df . merge ( affiliation_df ) name_to_numbers = { name : sorted ( df . affiliation_number ) for name , df in affil_map_df . groupby ( 'name' )} for author in variables [ 'authors' ]: author [ 'affiliation_numbers' ] = name_to_numbers . get ( author [ 'name' ], []) variables [ 'affiliations' ] = affiliation_df . to_dict ( orient = 'records' ) return variables def get_metadata_and_variables ( args ): \"\"\" Process metadata.yaml and create variables available for jinja2 templating. \"\"\" # Generated manuscript variables variables = collections . OrderedDict () # Read metadata which contains pandoc_yaml_metadata # as well as author_info. if args . meta_yaml_path . is_file (): with args . meta_yaml_path . open ( encoding = 'utf-8-sig' ) as read_file : metadata = yaml . safe_load ( read_file ) assert isinstance ( metadata , dict ) else : metadata = {} logging . warning ( f 'missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc' ) # Add date to metadata now = datetime_now () logging . info ( f 'Using {now:%Z} timezone. \\n ' f 'Dating manuscript with the current datetime: {now.isoformat()}' ) metadata [ 'date-meta' ] = now . date () . isoformat () variables [ 'date' ] = f '{now:%B} {now.day}, {now.year}' # Process authors metadata authors = metadata . pop ( 'author_info' , []) if authors is None : authors = [] metadata [ 'author-meta' ] = [ author [ 'name' ] for author in authors ] variables [ 'authors' ] = authors variables = add_author_affiliations ( variables ) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ 'ci_source' ] = add_manuscript_urls_to_ci_params ( ci_params ) # Update variables with user-provided variables here user_variables = read_jsons ( args . template_variables_path ) variables . update ( user_variables ) return metadata , variables def get_citekeys_df ( args , text ): \"\"\" Generate citekeys_df and save it to 'citations.tsv'. citekeys_df is a pandas.DataFrame with the following columns: - manuscript_citekey: citation keys extracted from the manuscript content files. - detagged_citekey: manuscript_citekey but with tag citekeys dereferenced - standard_citekey: detagged_citekey standardized - short_citekey: standard_citekey hashed to create a shortened citekey \"\"\" citekeys_df = pandas . DataFrame ( { 'manuscript_citekey' : get_citekeys ( text )} ) if args . citation_tags_path . is_file (): tag_df = pandas . read_csv ( args . citation_tags_path , sep = ' \\t ' ) na_rows_df = tag_df [ tag_df . isnull () . any ( axis = 'columns' )] if not na_rows_df . empty : logging . error ( f '{args.citation_tags_path} contains rows with missing values: \\n ' f '{na_rows_df} \\n ' 'This error can be caused by using spaces rather than tabs to delimit fields. \\n ' 'Proceeding to reread TSV with delim_whitespace=True.' ) tag_df = pandas . read_csv ( args . citation_tags_path , delim_whitespace = True ) tag_df [ 'manuscript_citekey' ] = 'tag:' + tag_df . tag tag_df = tag_df . rename ( columns = { 'citation' : 'detagged_citekey' }) for detagged_citekey in tag_df . detagged_citekey : is_valid_citekey ( detagged_citekey , allow_raw = True ) citekeys_df = citekeys_df . merge ( tag_df [[ 'manuscript_citekey' , 'detagged_citekey' ]], how = 'left' ) else : citekeys_df [ 'detagged_citekey' ] = None logging . info ( f 'missing {args.citation_tags_path} file: no citation tags (citekey aliases) set' ) citekeys_df . detagged_citekey . fillna ( citekeys_df . manuscript_citekey . astype ( str ), inplace = True ) citekeys_df [ 'standard_citekey' ] = citekeys_df . detagged_citekey . map ( standardize_citekey ) citekeys_df [ 'short_citekey' ] = citekeys_df . standard_citekey . map ( shorten_citekey ) citekeys_df = citekeys_df . sort_values ([ 'standard_citekey' , 'detagged_citekey' ]) citekeys_df . to_csv ( args . citations_path , sep = ' \\t ' , index = False ) check_collisions ( citekeys_df ) check_multiple_citation_strings ( citekeys_df ) return citekeys_df def generate_csl_items ( args , citekeys_df ): \"\"\" General CSL (citeproc) items for standard_citekeys in citekeys_df. Writes references.json to disk and logs warnings for potential problems. \"\"\" # Read manual references (overrides) in JSON CSL manual_refs = load_manual_references ( args . manual_references_paths ) requests_cache . install_cache ( args . requests_cache_path , include_get_headers = True ) cache = requests_cache . get_cache () if args . clear_requests_cache : logging . info ( 'Clearing requests-cache' ) requests_cache . clear () logging . info ( f 'requests-cache starting with {len(cache.responses)} cached responses' ) csl_items = list () failures = list () for standard_citekey in citekeys_df . standard_citekey . unique (): if standard_citekey in manual_refs : csl_items . append ( manual_refs [ standard_citekey ]) continue elif standard_citekey . startswith ( 'raw:' ): logging . error ( f 'CSL JSON Data with a standard_citekey of {standard_citekey!r} not found in manual-references.json. ' 'Metadata must be provided for raw citekeys.' ) failures . append ( standard_citekey ) try : csl_item = citekey_to_csl_item ( standard_citekey ) csl_items . append ( csl_item ) except Exception : logging . exception ( f 'Citeproc retrieval failure for {standard_citekey!r}' ) failures . append ( standard_citekey ) logging . info ( f 'requests-cache finished with {len(cache.responses)} cached responses' ) requests_cache . uninstall_cache () if failures : message = 'CSL JSON Data retrieval failed for the following standardized citation keys: \\n {}' . format ( ' \\n ' . join ( failures )) logging . error ( message ) # Write JSON CSL bibliography for Pandoc. with args . references_path . open ( 'w' , encoding = 'utf-8' ) as write_file : json . dump ( csl_items , write_file , indent = 2 , ensure_ascii = False ) write_file . write ( ' \\n ' ) return csl_items def template_with_jinja2 ( text , variables ): \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments. \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader (), undefined = jinja2 . make_logging_undefined ( logging . getLogger ()), comment_start_string = '{##' , comment_end_string = '##}' , ) template = jinja_environment . from_string ( text ) return template . render ( ** variables ) def prepare_manuscript ( args ): \"\"\" Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. \"\"\" text = get_text ( args . content_directory ) citekeys_df = get_citekeys_df ( args , text ) generate_csl_items ( args , citekeys_df ) citekey_mapping = collections . OrderedDict ( zip ( citekeys_df . manuscript_citekey , citekeys_df . short_citekey )) text = update_manuscript_citekeys ( text , citekey_mapping ) metadata , variables = get_metadata_and_variables ( args ) variables [ 'manuscript_stats' ] = get_manuscript_stats ( text , citekeys_df ) with args . variables_path . open ( 'w' , encoding = 'utf-8' ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( 'w' , encoding = 'utf-8' ) as write_file : yaml . dump ( metadata , write_file , default_flow_style = False , explicit_start = True , explicit_end = True ) write_file . write ( ' \\n ' ) write_file . write ( text )","title":"Module manubot.process.util"},{"location":"reference/manubot/process/util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/util/#add_author_affiliations","text":"def add_author_affiliations ( variables ) Edit variables to contain numbered author affiliations. Specifically, add a list of affiliation_numbers for each author and add a list of affiliations to the top-level of variables. If no authors have any affiliations, variables is left unmodified. View Source def add_author_affiliations ( variables ) : \"\"\" Edit variables to contain numbered author affiliations . Specifically , add a list of affiliation_numbers for each author and add a list of affiliations to the top - level of variables . If no authors have any affiliations , variables is left unmodified . \"\"\" rows = list () for author in variables [ ' authors ' ]: if ' affiliations ' not in author : continue if not isinstance ( author [ ' affiliations ' ], list ) : warnings . warn ( f \" Expected list for {author['name']}'s affiliations. \" f \" Assuming multiple affiliations are `; ` separated. \" f \" Please switch affiliations to a list. \" , category = DeprecationWarning ) author [ ' affiliations ' ] = author [ ' affiliations ' ]. split ( ' ; ' ) for affiliation in author [ ' affiliations ' ]: rows . append (( author [ ' name ' ], affiliation )) if not rows : return variables affil_map_df = pandas . DataFrame ( rows , columns = [ ' name ' , ' affiliation ' ] ) affiliation_df = affil_map_df [[ ' affiliation ' ]]. drop_duplicates () affiliation_df [ ' affiliation_number ' ] = range ( 1 , 1 + len ( affiliation_df )) affil_map_df = affil_map_df . merge ( affiliation_df ) name_to_numbers = { name : sorted ( df . affiliation_number ) for name , df in affil_map_df . groupby ( ' name ' ) } for author in variables [ ' authors ' ]: author [ ' affiliation_numbers ' ] = name_to_numbers . get ( author [ ' name ' ], [] ) variables [ ' affiliations ' ] = affiliation_df . to_dict ( orient = ' records ' ) return variables","title":"add_author_affiliations"},{"location":"reference/manubot/process/util/#check_collisions","text":"def check_collisions ( citekeys_df ) Check for short_citekey hash collisions View Source def check_collisions ( citekeys_df ) : \"\"\" Check for short_citekey hash collisions \"\"\" collision_df = citekeys_df [[ ' standard_citekey ' , ' short_citekey ' ]]. drop_duplicates () collision_df = collision_df [ collision_df . short_citekey . duplicated ( keep = False ) ] if not collision_df . empty : logging . error ( f ' OMF! Hash collision. Congratulations. \\n {collision_df} ' ) return collision_df","title":"check_collisions"},{"location":"reference/manubot/process/util/#check_multiple_citation_strings","text":"def check_multiple_citation_strings ( citekeys_df ) Identify different citation strings referring the the same reference. View Source def check_multiple_citation_strings ( citekeys_df ) : \"\"\" Identify different citation strings referring the the same reference . \"\"\" message = textwrap . dedent ( f ''' \\ { len ( citekeys_df ) } unique citations strings extracted from text { citekeys_df . standard_citekey . nunique () } unique standard citations \\ ''' ) logging . info ( message ) multi_df = citekeys_df [ citekeys_df . standard_citekey . duplicated ( keep = False ) ] if not multi_df . empty : table = multi_df . to_string ( index = False , columns = [ ' standard_citekey ' , ' manuscript_citekey ' ] ) logging . warning ( f ' Multiple citekeys detected for the same reference: \\n {table} ' ) return multi_df","title":"check_multiple_citation_strings"},{"location":"reference/manubot/process/util/#generate_csl_items","text":"def generate_csl_items ( args , citekeys_df ) General CSL (citeproc) items for standard_citekeys in citekeys_df. Writes references.json to disk and logs warnings for potential problems. View Source def generate_csl_items ( args , citekeys_df ) : \"\"\" General CSL ( citeproc ) items for standard_citekeys in citekeys_df . Writes references . json to disk and logs warnings for potential problems . \"\"\" # Read manual references ( overrides ) in JSON CSL manual_refs = load_manual_references ( args . manual_references_paths ) requests_cache . install_cache ( args . requests_cache_path , include_get_headers = True ) cache = requests_cache . get_cache () if args . clear_requests_cache : logging . info ( ' Clearing requests-cache ' ) requests_cache . clear () logging . info ( f ' requests-cache starting with {len(cache.responses)} cached responses ' ) csl_items = list () failures = list () for standard_citekey in citekeys_df . standard_citekey . unique () : if standard_citekey in manual_refs : csl_items . append ( manual_refs [ standard_citekey ] ) continue elif standard_citekey . startswith ( ' raw: ' ) : logging . error ( f ' CSL JSON Data with a standard_citekey of {standard_citekey!r} not found in manual-references.json. ' ' Metadata must be provided for raw citekeys. ' ) failures . append ( standard_citekey ) try : csl_item = citekey_to_csl_item ( standard_citekey ) csl_items . append ( csl_item ) except Exception : logging . exception ( f ' Citeproc retrieval failure for {standard_citekey!r} ' ) failures . append ( standard_citekey ) logging . info ( f ' requests-cache finished with {len(cache.responses)} cached responses ' ) requests_cache . uninstall_cache () if failures : message = ' CSL JSON Data retrieval failed for the following standardized citation keys: \\n {} ' . format ( ' \\n ' . join ( failures )) logging . error ( message ) # Write JSON CSL bibliography for Pandoc . with args . references_path . open ( ' w ' , encoding = ' utf-8 ' ) as write_file : json . dump ( csl_items , write_file , indent = 2 , ensure_ascii = False ) write_file . write ( ' \\n ' ) return csl_items","title":"generate_csl_items"},{"location":"reference/manubot/process/util/#get_citekeys_df","text":"def get_citekeys_df ( args , text ) Generate citekeys_df and save it to 'citations.tsv'. citekeys_df is a pandas.DataFrame with the following columns: - manuscript_citekey: citation keys extracted from the manuscript content files. - detagged_citekey: manuscript_citekey but with tag citekeys dereferenced - standard_citekey: detagged_citekey standardized - short_citekey: standard_citekey hashed to create a shortened citekey View Source def get_citekeys_df ( args , text ) : \"\"\" Generate citekeys_df and save it to ' citations.tsv ' . citekeys_df is a pandas . DataFrame with the following columns : - manuscript_citekey : citation keys extracted from the manuscript content files . - detagged_citekey : manuscript_citekey but with tag citekeys dereferenced - standard_citekey : detagged_citekey standardized - short_citekey : standard_citekey hashed to create a shortened citekey \"\"\" citekeys_df = pandas . DataFrame ( { ' manuscript_citekey ' : get_citekeys ( text ) } ) if args . citation_tags_path . is_file () : tag_df = pandas . read_csv ( args . citation_tags_path , sep = ' \\t ' ) na_rows_df = tag_df [ tag_df . isnull () . any ( axis = ' columns ' ) ] if not na_rows_df . empty : logging . error ( f ' {args.citation_tags_path} contains rows with missing values: \\n ' f ' {na_rows_df} \\n ' ' This error can be caused by using spaces rather than tabs to delimit fields. \\n ' ' Proceeding to reread TSV with delim_whitespace=True. ' ) tag_df = pandas . read_csv ( args . citation_tags_path , delim_whitespace = True ) tag_df [ ' manuscript_citekey ' ] = ' tag: ' + tag_df . tag tag_df = tag_df . rename ( columns = { ' citation ' : ' detagged_citekey ' } ) for detagged_citekey in tag_df . detagged_citekey : is_valid_citekey ( detagged_citekey , allow_raw = True ) citekeys_df = citekeys_df . merge ( tag_df [[ ' manuscript_citekey ' , ' detagged_citekey ' ]], how = ' left ' ) else : citekeys_df [ ' detagged_citekey ' ] = None logging . info ( f ' missing {args.citation_tags_path} file: no citation tags (citekey aliases) set ' ) citekeys_df . detagged_citekey . fillna ( citekeys_df . manuscript_citekey . astype ( str ) , inplace = True ) citekeys_df [ ' standard_citekey ' ] = citekeys_df . detagged_citekey . map ( standardize_citekey ) citekeys_df [ ' short_citekey ' ] = citekeys_df . standard_citekey . map ( shorten_citekey ) citekeys_df = citekeys_df . sort_values ( [ ' standard_citekey ' , ' detagged_citekey ' ] ) citekeys_df . to_csv ( args . citations_path , sep = ' \\t ' , index = False ) check_collisions ( citekeys_df ) check_multiple_citation_strings ( citekeys_df ) return citekeys_df","title":"get_citekeys_df"},{"location":"reference/manubot/process/util/#get_metadata_and_variables","text":"def get_metadata_and_variables ( args ) Process metadata.yaml and create variables available for jinja2 templating. View Source def get_metadata_and_variables ( args ) : \"\"\" Process metadata . yaml and create variables available for jinja2 templating . \"\"\" # Generated manuscript variables variables = collections . OrderedDict () # Read metadata which contains pandoc_yaml_metadata # as well as author_info . if args . meta_yaml_path . is_file () : with args . meta_yaml_path . open ( encoding = ' utf-8-sig ' ) as read_file : metadata = yaml . safe_load ( read_file ) assert isinstance ( metadata , dict ) else : metadata = {} logging . warning ( f ' missing {args.meta_yaml_path} file with yaml_metadata_block for pandoc ' ) # Add date to metadata now = datetime_now () logging . info ( f ' Using {now:%Z} timezone. \\n ' f ' Dating manuscript with the current datetime: {now.isoformat()} ' ) metadata [ ' date-meta ' ] = now . date () . isoformat () variables [ ' date ' ] = f ' {now:%B} {now.day}, {now.year} ' # Process authors metadata authors = metadata . pop ( ' author_info ' , [] ) if authors is None : authors = [] metadata [ ' author-meta ' ] = [ author [ ' name ' ] for author in authors ] variables [ ' authors ' ] = authors variables = add_author_affiliations ( variables ) # Set repository version metadata for CI builds ci_params = get_continuous_integration_parameters () if ci_params : variables [ ' ci_source ' ] = add_manuscript_urls_to_ci_params ( ci_params ) # Update variables with user - provided variables here user_variables = read_jsons ( args . template_variables_path ) variables . update ( user_variables ) return metadata , variables","title":"get_metadata_and_variables"},{"location":"reference/manubot/process/util/#prepare_manuscript","text":"def prepare_manuscript ( args ) Compile manuscript, creating manuscript.md and references.json as inputs for pandoc. View Source def prepare_manuscript ( args ) : \"\"\" Compile manuscript , creating manuscript . md and references . json as inputs for pandoc . \"\"\" text = get_text ( args . content_directory ) citekeys_df = get_citekeys_df ( args , text ) generate_csl_items ( args , citekeys_df ) citekey_mapping = collections . OrderedDict ( zip ( citekeys_df . manuscript_citekey , citekeys_df . short_citekey )) text = update_manuscript_citekeys ( text , citekey_mapping ) metadata , variables = get_metadata_and_variables ( args ) variables [ ' manuscript_stats ' ] = get_manuscript_stats ( text , citekeys_df ) with args . variables_path . open ( ' w ' , encoding = ' utf-8 ' ) as write_file : json . dump ( variables , write_file , ensure_ascii = False , indent = 2 ) write_file . write ( ' \\n ' ) text = template_with_jinja2 ( text , variables ) # Write manuscript for pandoc with args . manuscript_path . open ( ' w ' , encoding = ' utf-8 ' ) as write_file : yaml . dump ( metadata , write_file , default_flow_style = False , explicit_start = True , explicit_end = True ) write_file . write ( ' \\n ' ) write_file . write ( text )","title":"prepare_manuscript"},{"location":"reference/manubot/process/util/#read_json","text":"def read_json ( path ) Read json from a path or URL. View Source def read_json ( path ) : \"\"\" Read json from a path or URL . \"\"\" if re . match ( ' ^(http|ftp)s?:// ' , path ) : response = requests . get ( path ) obj = response . json ( object_pairs_hook = collections . OrderedDict ) else : path = pathlib . Path ( path ) with path . open ( encoding = ' utf-8-sig ' ) as read_file : obj = json . load ( read_file , object_pairs_hook = collections . OrderedDict ) return obj","title":"read_json"},{"location":"reference/manubot/process/util/#read_jsons","text":"def read_jsons ( paths ) Read multiple JSON files into a user_variables dictionary. Provide a list of paths (URLs or filepaths). Paths can optionally have a namespace prepended. For example: paths = [ 'https://git.io/vbkqm' , # update the dictionary 's top-level ' namespace_1 = https : // git . io / vbkqm ', # store under ' namespace_1 ' key ' namespace_2 = some_local_path . json ', # store under ' namespace_2 ' key ] If a namespace is not provided, the JSON must contain a dictionary as its top level. Namespaces should consist only of ASCII alphanumeric characters (includes underscores, first character cannot be numeric). View Source def read_jsons ( paths ) : \"\"\" Read multiple JSON files into a user_variables dictionary . Provide a list of paths ( URLs or filepaths ) . Paths can optionally have a namespace prepended . For example : ``` paths = [ ' https://git.io/vbkqm ' , # update the dictionary ' s top-level ' namespace_1=https://git.io/vbkqm ' , # store under ' namespace_1 ' key ' namespace_2=some_local_path.json ' , # store under ' namespace_2 ' key ] ``` If a namespace is not provided , the JSON must contain a dictionary as its top level . Namespaces should consist only of ASCII alphanumeric characters ( includes underscores , first character cannot be numeric ) . \"\"\" user_variables = collections . OrderedDict () for path in paths : logging . info ( f ' Read the following user-provided templating variables for {path} ' ) # Match only namespaces that are valid jinja2 variable names # http : // jinja . pocoo . org / docs / 2 . 10 / api / # identifier - naming match = re . match ( r ' ([a-zA-Z_][a-zA-Z0-9_]*)=(.+) ' , path ) if match : namespace , path = match . groups () logging . info ( f ' Using the \"{namespace}\" namespace for template variables from {path} ' ) try : obj = read_json ( path ) except Exception : logging . exception ( f ' Error reading template variables from {path} ' ) continue if match : obj = { namespace : obj } assert isinstance ( obj , dict ) conflicts = user_variables . keys () & obj . keys () if conflicts : logging . warning ( f ' Template variables in {path} overwrite existing ' ' values for the following keys: \\n ' + ' \\n ' . join ( conflicts ) ) user_variables . update ( obj ) logging . info ( f ' Reading user-provided templating variables complete: \\n ' f ' {json.dumps(user_variables, indent=2, ensure_ascii=False)} ' ) return user_variables","title":"read_jsons"},{"location":"reference/manubot/process/util/#template_with_jinja2","text":"def template_with_jinja2 ( text , variables ) Template using jinja2 with the variables dictionary unpacked as keyword arguments. View Source def template_with_jinja2 ( text , variables ) : \"\"\" Template using jinja2 with the variables dictionary unpacked as keyword arguments . \"\"\" jinja_environment = jinja2 . Environment ( loader = jinja2 . BaseLoader () , undefined = jinja2 . make_logging_undefined ( logging . getLogger ()) , comment_start_string = ' {## ' , comment_end_string = ' ##} ' , ) template = jinja_environment . from_string ( text ) return template . render ( ** variables )","title":"template_with_jinja2"},{"location":"reference/manubot/process/tests/","text":"Module manubot.process.tests Sub-modules manubot.process.tests.test_bibliography manubot.process.tests.test_ci manubot.process.tests.test_manuscript manubot.process.tests.test_process_command manubot.process.tests.test_util","title":"Index"},{"location":"reference/manubot/process/tests/#module-manubotprocesstests","text":"","title":"Module manubot.process.tests"},{"location":"reference/manubot/process/tests/#sub-modules","text":"manubot.process.tests.test_bibliography manubot.process.tests.test_ci manubot.process.tests.test_manuscript manubot.process.tests.test_process_command manubot.process.tests.test_util","title":"Sub-modules"},{"location":"reference/manubot/process/tests/test_bibliography/","text":"Module manubot.process.tests.test_bibliography View Source import shutil import pytest from manubot.pandoc.tests.test_bibliography import ( bibliography_paths ) from manubot.process.bibliography import ( load_manual_references , ) @pytest.mark.skipif ( not shutil . which ( 'pandoc-citeproc' ), reason = 'pandoc-citeproc installation not found on system' , ) def test_load_multiple_bibliography_paths (): citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( citation_to_csl_item )) assert 'doi:10.7554/elife.32822' in citation_to_csl_item csl_item_1 = citation_to_csl_item [ 'doi:10.7554/elife.32822' ] assert csl_item_1 [ 'title' ] . startswith ( 'Sci-Hub' ) assert 'CSL JSON Item was loaded by Manubot' in csl_item_1 [ 'note' ] assert 'manual_reference_filename: bibliography.json' in csl_item_1 [ 'note' ] assert 'standard_id: doi:10.7554/elife.32822' in csl_item_1 [ 'note' ] # raw id corresponding to bibliography.bib assert 'raw:noauthor_techblog:_nodate' in citation_to_csl_item csl_item_2 = citation_to_csl_item [ 'raw:noauthor_techblog:_nodate' ] assert csl_item_2 [ 'title' ] . startswith ( 'TechBlog' ) assert 'manual_reference_filename: bibliography.bib' in csl_item_2 [ 'note' ] assert 'original_id: noauthor_techblog:_nodate' in csl_item_2 [ 'note' ] # id inferred by pandoc-citeproc during bib2json conversion of .nbib file assert 'raw:Beaulieu-Jones2017' in citation_to_csl_item csl_item_3 = citation_to_csl_item [ 'raw:Beaulieu-Jones2017' ] assert csl_item_3 [ 'author' ][ 0 ][ 'family' ] == 'Beaulieu-Jones' assert 'manual_reference_filename: bibliography.nbib' in csl_item_3 [ 'note' ] assert 'original_id: Beaulieu-Jones2017' in csl_item_3 [ 'note' ] Variables bibliography_paths Functions test_load_multiple_bibliography_paths def test_load_multiple_bibliography_paths ( ) View Source @pytest . mark . skipif ( not shutil . which ( 'pandoc-citeproc' ), reason = 'pandoc-citeproc installation not found on system' , ) def test_load_multiple_bibliography_paths () : citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( citation_to_csl_item )) assert 'doi:10.7554/elife.32822' in citation_to_csl_item csl_item_1 = citation_to_csl_item [ 'doi:10.7554/elife.32822' ] assert csl_item_1 [ 'title' ] . startswith ( 'Sci-Hub' ) assert 'CSL JSON Item was loaded by Manubot' in csl_item_1 [ 'note' ] assert 'manual_reference_filename: bibliography.json' in csl_item_1 [ 'note' ] assert 'standard_id: doi:10.7554/elife.32822' in csl_item_1 [ 'note' ] # raw id corresponding to bibliography . bib assert 'raw:noauthor_techblog:_nodate' in citation_to_csl_item csl_item_2 = citation_to_csl_item [ 'raw:noauthor_techblog:_nodate' ] assert csl_item_2 [ 'title' ] . startswith ( 'TechBlog' ) assert 'manual_reference_filename: bibliography.bib' in csl_item_2 [ 'note' ] assert 'original_id: noauthor_techblog:_nodate' in csl_item_2 [ 'note' ] # id inferred by pandoc - citeproc during bib2json conversion of . nbib file assert 'raw:Beaulieu-Jones2017' in citation_to_csl_item csl_item_3 = citation_to_csl_item [ 'raw:Beaulieu-Jones2017' ] assert csl_item_3 [ 'author' ][ 0 ][ 'family' ] == 'Beaulieu-Jones' assert 'manual_reference_filename: bibliography.nbib' in csl_item_3 [ 'note' ] assert 'original_id: Beaulieu-Jones2017' in csl_item_3 [ 'note' ]","title":"Test Bibliography"},{"location":"reference/manubot/process/tests/test_bibliography/#module-manubotprocessteststest_bibliography","text":"View Source import shutil import pytest from manubot.pandoc.tests.test_bibliography import ( bibliography_paths ) from manubot.process.bibliography import ( load_manual_references , ) @pytest.mark.skipif ( not shutil . which ( 'pandoc-citeproc' ), reason = 'pandoc-citeproc installation not found on system' , ) def test_load_multiple_bibliography_paths (): citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( citation_to_csl_item )) assert 'doi:10.7554/elife.32822' in citation_to_csl_item csl_item_1 = citation_to_csl_item [ 'doi:10.7554/elife.32822' ] assert csl_item_1 [ 'title' ] . startswith ( 'Sci-Hub' ) assert 'CSL JSON Item was loaded by Manubot' in csl_item_1 [ 'note' ] assert 'manual_reference_filename: bibliography.json' in csl_item_1 [ 'note' ] assert 'standard_id: doi:10.7554/elife.32822' in csl_item_1 [ 'note' ] # raw id corresponding to bibliography.bib assert 'raw:noauthor_techblog:_nodate' in citation_to_csl_item csl_item_2 = citation_to_csl_item [ 'raw:noauthor_techblog:_nodate' ] assert csl_item_2 [ 'title' ] . startswith ( 'TechBlog' ) assert 'manual_reference_filename: bibliography.bib' in csl_item_2 [ 'note' ] assert 'original_id: noauthor_techblog:_nodate' in csl_item_2 [ 'note' ] # id inferred by pandoc-citeproc during bib2json conversion of .nbib file assert 'raw:Beaulieu-Jones2017' in citation_to_csl_item csl_item_3 = citation_to_csl_item [ 'raw:Beaulieu-Jones2017' ] assert csl_item_3 [ 'author' ][ 0 ][ 'family' ] == 'Beaulieu-Jones' assert 'manual_reference_filename: bibliography.nbib' in csl_item_3 [ 'note' ] assert 'original_id: Beaulieu-Jones2017' in csl_item_3 [ 'note' ]","title":"Module manubot.process.tests.test_bibliography"},{"location":"reference/manubot/process/tests/test_bibliography/#variables","text":"bibliography_paths","title":"Variables"},{"location":"reference/manubot/process/tests/test_bibliography/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_bibliography/#test_load_multiple_bibliography_paths","text":"def test_load_multiple_bibliography_paths ( ) View Source @pytest . mark . skipif ( not shutil . which ( 'pandoc-citeproc' ), reason = 'pandoc-citeproc installation not found on system' , ) def test_load_multiple_bibliography_paths () : citation_to_csl_item = load_manual_references ( bibliography_paths ) print ( list ( citation_to_csl_item )) assert 'doi:10.7554/elife.32822' in citation_to_csl_item csl_item_1 = citation_to_csl_item [ 'doi:10.7554/elife.32822' ] assert csl_item_1 [ 'title' ] . startswith ( 'Sci-Hub' ) assert 'CSL JSON Item was loaded by Manubot' in csl_item_1 [ 'note' ] assert 'manual_reference_filename: bibliography.json' in csl_item_1 [ 'note' ] assert 'standard_id: doi:10.7554/elife.32822' in csl_item_1 [ 'note' ] # raw id corresponding to bibliography . bib assert 'raw:noauthor_techblog:_nodate' in citation_to_csl_item csl_item_2 = citation_to_csl_item [ 'raw:noauthor_techblog:_nodate' ] assert csl_item_2 [ 'title' ] . startswith ( 'TechBlog' ) assert 'manual_reference_filename: bibliography.bib' in csl_item_2 [ 'note' ] assert 'original_id: noauthor_techblog:_nodate' in csl_item_2 [ 'note' ] # id inferred by pandoc - citeproc during bib2json conversion of . nbib file assert 'raw:Beaulieu-Jones2017' in citation_to_csl_item csl_item_3 = citation_to_csl_item [ 'raw:Beaulieu-Jones2017' ] assert csl_item_3 [ 'author' ][ 0 ][ 'family' ] == 'Beaulieu-Jones' assert 'manual_reference_filename: bibliography.nbib' in csl_item_3 [ 'note' ] assert 'original_id: Beaulieu-Jones2017' in csl_item_3 [ 'note' ]","title":"test_load_multiple_bibliography_paths"},{"location":"reference/manubot/process/tests/test_ci/","text":"Module manubot.process.tests.test_ci View Source import os import re import pytest from ..ci import ( add_manuscript_urls_to_ci_params , get_continuous_integration_parameters , ) @pytest.mark.skipif ( 'TRAVIS' not in os . environ , reason = 'tests environment variables set by Travis builds only' ) def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'travis' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/jobs/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://manubot\\.github\\.io/manubot/v/[0-9a-f]{40}/\" , string = info [ 'manuscript_url' ], ) @pytest.mark.skipif ( 'APPVEYOR' not in os . environ , reason = 'tests environment variables set by AppVeyor builds only' ) def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'appveyor' assert info [ 'provider_account' ] == 'manubot' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/build/job/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ 'manuscript_url' ], ) @pytest.mark.skipif ( 'CI' in os . environ , reason = 'tests function when run outside of a CI build' ) def test_get_continuous_integration_parameters_no_ci (): info = get_continuous_integration_parameters () assert info is None # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info_updated is None Functions test_get_continuous_integration_parameters_appveyor def test_get_continuous_integration_parameters_appveyor ( ) View Source @pytest . mark . skipif ( 'APPVEYOR' not in os . environ , reason = 'tests environment variables set by AppVeyor builds only' ) def test_get_continuous_integration_parameters_appveyor () : info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'appveyor' assert info [ 'provider_account' ] == 'manubot' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/build/job/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ 'manuscript_url' ] , ) test_get_continuous_integration_parameters_no_ci def test_get_continuous_integration_parameters_no_ci ( ) View Source @pytest . mark . skipif ( 'CI' in os . environ , reason = 'tests function when run outside of a CI build' ) def test_get_continuous_integration_parameters_no_ci () : info = get_continuous_integration_parameters () assert info is None # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info_updated is None test_get_continuous_integration_parameters_travis def test_get_continuous_integration_parameters_travis ( ) View Source @pytest . mark . skipif ( 'TRAVIS' not in os . environ , reason = 'tests environment variables set by Travis builds only' ) def test_get_continuous_integration_parameters_travis () : info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'travis' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/jobs/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://manubot\\.github\\.io/manubot/v/[0-9a-f]{40}/\" , string = info [ 'manuscript_url' ] , )","title":"Test Ci"},{"location":"reference/manubot/process/tests/test_ci/#module-manubotprocessteststest_ci","text":"View Source import os import re import pytest from ..ci import ( add_manuscript_urls_to_ci_params , get_continuous_integration_parameters , ) @pytest.mark.skipif ( 'TRAVIS' not in os . environ , reason = 'tests environment variables set by Travis builds only' ) def test_get_continuous_integration_parameters_travis (): info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'travis' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/jobs/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://manubot\\.github\\.io/manubot/v/[0-9a-f]{40}/\" , string = info [ 'manuscript_url' ], ) @pytest.mark.skipif ( 'APPVEYOR' not in os . environ , reason = 'tests environment variables set by AppVeyor builds only' ) def test_get_continuous_integration_parameters_appveyor (): info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'appveyor' assert info [ 'provider_account' ] == 'manubot' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/build/job/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ 'manuscript_url' ], ) @pytest.mark.skipif ( 'CI' in os . environ , reason = 'tests function when run outside of a CI build' ) def test_get_continuous_integration_parameters_no_ci (): info = get_continuous_integration_parameters () assert info is None # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info_updated is None","title":"Module manubot.process.tests.test_ci"},{"location":"reference/manubot/process/tests/test_ci/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_appveyor","text":"def test_get_continuous_integration_parameters_appveyor ( ) View Source @pytest . mark . skipif ( 'APPVEYOR' not in os . environ , reason = 'tests environment variables set by AppVeyor builds only' ) def test_get_continuous_integration_parameters_appveyor () : info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'appveyor' assert info [ 'provider_account' ] == 'manubot' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://ci.appveyor.com/project/manubot/manubot/build/job/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://ci\\.appveyor\\.com/project/manubot/manubot/builds/[0-9]+/artifacts\" , string = info [ 'manuscript_url' ] , )","title":"test_get_continuous_integration_parameters_appveyor"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_no_ci","text":"def test_get_continuous_integration_parameters_no_ci ( ) View Source @pytest . mark . skipif ( 'CI' in os . environ , reason = 'tests function when run outside of a CI build' ) def test_get_continuous_integration_parameters_no_ci () : info = get_continuous_integration_parameters () assert info is None # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info_updated is None","title":"test_get_continuous_integration_parameters_no_ci"},{"location":"reference/manubot/process/tests/test_ci/#test_get_continuous_integration_parameters_travis","text":"def test_get_continuous_integration_parameters_travis ( ) View Source @pytest . mark . skipif ( 'TRAVIS' not in os . environ , reason = 'tests environment variables set by Travis builds only' ) def test_get_continuous_integration_parameters_travis () : info = get_continuous_integration_parameters () assert info is not None assert info [ 'provider' ] == 'travis' assert info [ 'repo_slug' ] == 'manubot/manubot' assert info [ 'repo_owner' ] == 'manubot' assert info [ 'repo_name' ] == 'manubot' assert info [ 'commit' ] assert info [ 'triggering_commit' ] assert info [ 'build_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/builds/' ) assert info [ 'job_url' ] . startswith ( 'https://travis-ci.com/manubot/manubot/jobs/' ) # test add_manuscript_urls_to_ci_params info_updated = add_manuscript_urls_to_ci_params ( info ) assert info is info_updated assert re . fullmatch ( pattern = r \"https://manubot\\.github\\.io/manubot/v/[0-9a-f]{40}/\" , string = info [ 'manuscript_url' ] , )","title":"test_get_continuous_integration_parameters_travis"},{"location":"reference/manubot/process/tests/test_manuscript/","text":"Module manubot.process.tests.test_manuscript View Source from manubot.process.manuscript import ( get_citekeys , update_manuscript_citekeys ) def test_get_citekeys_1 (): text = ''' Sci-Hub has released article request records from its server logs, covering 165 days from September 2015 through February 2016 [@doi:10.1126/science.352.6285.508; @doi:10.1126/science.aaf5664; @doi:10.5061/dryad.q447c/1]. We filtered for valid requests by excluding DOIs not included in our literature catalog and omitting requests that occurred before an article's publication date. Figure {@fig:citescore}B shows that articles from highly cited journals were visited much more frequently on average. @10.5061/bad_doi says blah but @url:https://www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/ disagrees. ''' citekeys = get_citekeys ( text ) expected = sorted ([ 'doi:10.1126/science.352.6285.508' , 'doi:10.1126/science.aaf5664' , 'doi:10.5061/dryad.q447c/1' , 'url:https://www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/' , ]) assert citekeys == expected def test_update_manuscript_citekeys (): \"\"\" Test that text does not get converted to: > our new Manubot tool [@cTN2TQIL-rootstock; @cTN2TQIL] for automating manuscript generation. See https://github.com/manubot/manubot/issues/9 \"\"\" string_to_id = { 'url:https://github.com/manubot/manubot' : 'mNMayr3f' , 'url:https://github.com/greenelab/manubot-rootstock' : '1B7Y2HVtw' , } text = 'our new Manubot tool [@url:https://github.com/greenelab/manubot-rootstock; @url:https://github.com/manubot/manubot] for automating manuscript generation.' text = update_manuscript_citekeys ( text , string_to_id ) assert '[@1B7Y2HVtw; @mNMayr3f]' in text Functions test_get_citekeys_1 def test_get_citekeys_1 ( ) View Source def test_get_citekeys_1 () : text = ''' Sci - Hub has released article request records from its server logs , covering 165 days from September 2015 through February 2016 [@ doi : 10.1126 / science .352.6285.508 ; @ doi : 10.1126 / science . aaf5664 ; @ doi : 10.5061 / dryad . q447c / 1 ]. We filtered for valid requests by excluding DOIs not included in our literature catalog and omitting requests that occurred before an article ' s publication date . Figure {@ fig : citescore } B shows that articles from highly cited journals were visited much more frequently on average . @10.5061 / bad_doi says blah but @ url : https : //www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/ disagrees. ''' citekeys = get_citekeys ( text ) expected = sorted ([ ' doi : 10.1126 / science .352.6285.508 ' , ' doi : 10.1126 / science . aaf5664 ' , ' doi : 10.5061 / dryad . q447c / 1 ' , ' url : https : //www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/', ]) assert citekeys == expected test_update_manuscript_citekeys def test_update_manuscript_citekeys ( ) Test that text does not get converted to: our new Manubot tool [@cTN2TQIL-rootstock; @cTN2TQIL] for automating manuscript generation. See https://github.com/manubot/manubot/issues/9 View Source def test_update_manuscript_citekeys () : \"\"\" Test that text does not get converted to : > our new Manubot tool [@ cTN2TQIL - rootstock ; @ cTN2TQIL ] for automating manuscript generation . See https : //github.com/manubot/manubot/issues/9 \"\"\" string_to_id = { ' url : https : //github.com/manubot/manubot': 'mNMayr3f', ' url : https : //github.com/greenelab/manubot-rootstock': '1B7Y2HVtw', } text = ' our new Manubot tool [@ url : https : //github.com/greenelab/manubot-rootstock; @url:https://github.com/manubot/manubot] for automating manuscript generation.' text = update_manuscript_citekeys ( text , string_to_id ) assert ' [ @1 B7Y2HVtw ; @ mNMayr3f ] ' in text","title":"Test Manuscript"},{"location":"reference/manubot/process/tests/test_manuscript/#module-manubotprocessteststest_manuscript","text":"View Source from manubot.process.manuscript import ( get_citekeys , update_manuscript_citekeys ) def test_get_citekeys_1 (): text = ''' Sci-Hub has released article request records from its server logs, covering 165 days from September 2015 through February 2016 [@doi:10.1126/science.352.6285.508; @doi:10.1126/science.aaf5664; @doi:10.5061/dryad.q447c/1]. We filtered for valid requests by excluding DOIs not included in our literature catalog and omitting requests that occurred before an article's publication date. Figure {@fig:citescore}B shows that articles from highly cited journals were visited much more frequently on average. @10.5061/bad_doi says blah but @url:https://www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/ disagrees. ''' citekeys = get_citekeys ( text ) expected = sorted ([ 'doi:10.1126/science.352.6285.508' , 'doi:10.1126/science.aaf5664' , 'doi:10.5061/dryad.q447c/1' , 'url:https://www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/' , ]) assert citekeys == expected def test_update_manuscript_citekeys (): \"\"\" Test that text does not get converted to: > our new Manubot tool [@cTN2TQIL-rootstock; @cTN2TQIL] for automating manuscript generation. See https://github.com/manubot/manubot/issues/9 \"\"\" string_to_id = { 'url:https://github.com/manubot/manubot' : 'mNMayr3f' , 'url:https://github.com/greenelab/manubot-rootstock' : '1B7Y2HVtw' , } text = 'our new Manubot tool [@url:https://github.com/greenelab/manubot-rootstock; @url:https://github.com/manubot/manubot] for automating manuscript generation.' text = update_manuscript_citekeys ( text , string_to_id ) assert '[@1B7Y2HVtw; @mNMayr3f]' in text","title":"Module manubot.process.tests.test_manuscript"},{"location":"reference/manubot/process/tests/test_manuscript/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_manuscript/#test_get_citekeys_1","text":"def test_get_citekeys_1 ( ) View Source def test_get_citekeys_1 () : text = ''' Sci - Hub has released article request records from its server logs , covering 165 days from September 2015 through February 2016 [@ doi : 10.1126 / science .352.6285.508 ; @ doi : 10.1126 / science . aaf5664 ; @ doi : 10.5061 / dryad . q447c / 1 ]. We filtered for valid requests by excluding DOIs not included in our literature catalog and omitting requests that occurred before an article ' s publication date . Figure {@ fig : citescore } B shows that articles from highly cited journals were visited much more frequently on average . @10.5061 / bad_doi says blah but @ url : https : //www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/ disagrees. ''' citekeys = get_citekeys ( text ) expected = sorted ([ ' doi : 10.1126 / science .352.6285.508 ' , ' doi : 10.1126 / science . aaf5664 ' , ' doi : 10.5061 / dryad . q447c / 1 ' , ' url : https : //www.courtlistener.com/docket/4355308/1/elsevier-inc-v-sci-hub/', ]) assert citekeys == expected","title":"test_get_citekeys_1"},{"location":"reference/manubot/process/tests/test_manuscript/#test_update_manuscript_citekeys","text":"def test_update_manuscript_citekeys ( ) Test that text does not get converted to: our new Manubot tool [@cTN2TQIL-rootstock; @cTN2TQIL] for automating manuscript generation. See https://github.com/manubot/manubot/issues/9 View Source def test_update_manuscript_citekeys () : \"\"\" Test that text does not get converted to : > our new Manubot tool [@ cTN2TQIL - rootstock ; @ cTN2TQIL ] for automating manuscript generation . See https : //github.com/manubot/manubot/issues/9 \"\"\" string_to_id = { ' url : https : //github.com/manubot/manubot': 'mNMayr3f', ' url : https : //github.com/greenelab/manubot-rootstock': '1B7Y2HVtw', } text = ' our new Manubot tool [@ url : https : //github.com/greenelab/manubot-rootstock; @url:https://github.com/manubot/manubot] for automating manuscript generation.' text = update_manuscript_citekeys ( text , string_to_id ) assert ' [ @1 B7Y2HVtw ; @ mNMayr3f ] ' in text","title":"test_update_manuscript_citekeys"},{"location":"reference/manubot/process/tests/test_process_command/","text":"Module manubot.process.tests.test_process_command View Source import pathlib import subprocess import pytest directory = pathlib . Path ( __file__ ) . parent . resolve () # List of manuscripts for testing. All subdirectories of ./manuscripts manuscripts = [ path . name for path in directory . joinpath ( 'manuscripts' ) . iterdir () if path . is_dir ()] @pytest.mark.parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( 'manuscripts' , manuscript ) args = [ 'manubot' , 'process' , '--log-level' , 'INFO' , '--content-directory' , str ( manuscript_dir . joinpath ( 'content' )), '--output-directory' , str ( manuscript_dir . joinpath ( 'output' )), ] if manuscript == 'variables' : args . extend ([ '--template-variables-path' , str ( manuscript_dir . joinpath ( 'content/template-variables.json' )), ]) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . args ) print ( process . stderr . decode ()) assert process . returncode == 0 Variables directory manuscripts Functions test_example_manuscript def test_example_manuscript ( manuscript ) Test command line execution of manubot to build an example manuscript. View Source @ pytest . mark . parametrize ( \" manuscript \" , manuscripts ) def test_example_manuscript ( manuscript ) : \"\"\" Test command line execution of manubot to build an example manuscript . \"\"\" manuscript_dir = directory . joinpath ( ' manuscripts ' , manuscript ) args = [ ' manubot ' , ' process ' , ' --log-level ' , ' INFO ' , ' --content-directory ' , str ( manuscript_dir . joinpath ( ' content ' )) , ' --output-directory ' , str ( manuscript_dir . joinpath ( ' output ' )) , ] if manuscript == ' variables ' : args . extend ( [ ' --template-variables-path ' , str ( manuscript_dir . joinpath ( ' content/template-variables.json ' )) , ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . args ) print ( process . stderr . decode ()) assert process . returncode == 0","title":"Test Process Command"},{"location":"reference/manubot/process/tests/test_process_command/#module-manubotprocessteststest_process_command","text":"View Source import pathlib import subprocess import pytest directory = pathlib . Path ( __file__ ) . parent . resolve () # List of manuscripts for testing. All subdirectories of ./manuscripts manuscripts = [ path . name for path in directory . joinpath ( 'manuscripts' ) . iterdir () if path . is_dir ()] @pytest.mark.parametrize ( \"manuscript\" , manuscripts ) def test_example_manuscript ( manuscript ): \"\"\" Test command line execution of manubot to build an example manuscript. \"\"\" manuscript_dir = directory . joinpath ( 'manuscripts' , manuscript ) args = [ 'manubot' , 'process' , '--log-level' , 'INFO' , '--content-directory' , str ( manuscript_dir . joinpath ( 'content' )), '--output-directory' , str ( manuscript_dir . joinpath ( 'output' )), ] if manuscript == 'variables' : args . extend ([ '--template-variables-path' , str ( manuscript_dir . joinpath ( 'content/template-variables.json' )), ]) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . args ) print ( process . stderr . decode ()) assert process . returncode == 0","title":"Module manubot.process.tests.test_process_command"},{"location":"reference/manubot/process/tests/test_process_command/#variables","text":"directory manuscripts","title":"Variables"},{"location":"reference/manubot/process/tests/test_process_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_process_command/#test_example_manuscript","text":"def test_example_manuscript ( manuscript ) Test command line execution of manubot to build an example manuscript. View Source @ pytest . mark . parametrize ( \" manuscript \" , manuscripts ) def test_example_manuscript ( manuscript ) : \"\"\" Test command line execution of manubot to build an example manuscript . \"\"\" manuscript_dir = directory . joinpath ( ' manuscripts ' , manuscript ) args = [ ' manubot ' , ' process ' , ' --log-level ' , ' INFO ' , ' --content-directory ' , str ( manuscript_dir . joinpath ( ' content ' )) , ' --output-directory ' , str ( manuscript_dir . joinpath ( ' output ' )) , ] if manuscript == ' variables ' : args . extend ( [ ' --template-variables-path ' , str ( manuscript_dir . joinpath ( ' content/template-variables.json ' )) , ] ) process = subprocess . run ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ) print ( process . args ) print ( process . stderr . decode ()) assert process . returncode == 0","title":"test_example_manuscript"},{"location":"reference/manubot/process/tests/test_util/","text":"Module manubot.process.tests.test_util View Source import pathlib import pytest from manubot.process.util import ( add_author_affiliations , read_jsons , ) directory = pathlib . Path ( __file__ ) . parent . resolve () def test_read_jsons_empty (): paths = [] user_variables = read_jsons ( paths ) assert isinstance ( user_variables , dict ) and not user_variables def test_read_jsons (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = 'manuscripts/variables/content/template-variables.json' local_path = directory . joinpath ( local_path ) paths = [ 'https://git.io/vbkqm' , 'https://git.io/vbkqm' , 'namespace_1=https://git.io/vbkqm' , 'namespace_2=https://git.io/vbkqm' , f 'namespace_2={local_path}' , f 'namespace_3={local_path}' , ] user_variables = read_jsons ( paths ) assert 'namespace_1' in user_variables assert 'namespace_2' in user_variables assert 'namespace_3' in user_variables assert user_variables [ 'generated_by' ] == 'Manubot' assert 'violet' in user_variables [ 'namespace_1' ][ 'rainbow' ] assert 'yellow' in user_variables [ 'namespace_2' ][ 'rainbow' ] assert 'orange' in user_variables [ 'namespace_3' ][ 'rainbow' ] def test_add_author_affiliations_empty (): variables = {} variables [ 'authors' ] = [ { 'name' : 'Jane Roe' }, { 'name' : 'John Doe' }, ] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert 'affiliations' not in variables for author in variables [ 'authors' ]: assert 'affiliation_numbers' not in author def test_add_author_affiliations (): variables = {} variables [ 'authors' ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { 'name' : 'Jane Roe' , 'affiliations' : 'Department of Doe, University of Roe; Peppertea University' }, # Prefered affiliations format as a list { 'name' : 'John Doe' , 'affiliations' : [ 'Unique University' , 'Peppertea University' ]}, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ 'affiliations' ] == [ { 'affiliation' : 'Department of Doe, University of Roe' , 'affiliation_number' : 1 }, { 'affiliation' : 'Peppertea University' , 'affiliation_number' : 2 }, { 'affiliation' : 'Unique University' , 'affiliation_number' : 3 }, ] authors = variables [ 'authors' ] assert authors [ 0 ][ 'affiliations' ] == [ 'Department of Doe, University of Roe' , 'Peppertea University' , ] assert authors [ 0 ][ 'affiliation_numbers' ] == [ 1 , 2 ] assert authors [ 1 ][ 'affiliation_numbers' ] == [ 2 , 3 ] Variables directory Functions test_add_author_affiliations def test_add_author_affiliations ( ) View Source def test_add_author_affiliations (): variables = {} variables [ 'authors' ] = [ # Deprecated affiliations format ( as a string that 's `; ` separated) {' name ': ' Jane Roe ', ' affiliations ': ' Department of Doe , University of Roe ; Peppertea University '}, # Prefered affiliations format as a list {' name ': ' John Doe ', ' affiliations ': [' Unique University ', ' Peppertea University ']}, ] with pytest.warns(DeprecationWarning): returned_variables = add_author_affiliations(variables) assert variables is returned_variables assert variables[' affiliations '] == [ {' affiliation ': ' Department of Doe , University of Roe ', ' affiliation_number ': 1}, {' affiliation ': ' Peppertea University ', ' affiliation_number ': 2}, {' affiliation ': ' Unique University ', ' affiliation_number ': 3}, ] authors = variables[' authors '] assert authors[0][' affiliations '] == [ ' Department of Doe , University of Roe ', ' Peppertea University ', ] assert authors[0][' affiliation_numbers '] == [1, 2] assert authors[1][' affiliation_numbers ' ] == [ 2 , 3 ] test_add_author_affiliations_empty def test_add_author_affiliations_empty ( ) View Source def test_add_author_affiliations_empty () : variables = {} variables [ ' authors ' ] = [ { ' name ' : ' Jane Roe ' }, { ' name ' : ' John Doe ' }, ] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert ' affiliations ' not in variables for author in variables [ ' authors ' ]: assert ' affiliation_numbers ' not in author test_read_jsons def test_read_jsons ( ) Test reading multiple JSON files, from both local paths and URLs. View Source def test_read_jsons (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = 'manuscripts/variables/content/template-variables.json' local_path = directory . joinpath ( local_path ) paths = [ 'https://git.io/vbkqm' , 'https://git.io/vbkqm' , 'namespace_1=https://git.io/vbkqm' , 'namespace_2=https://git.io/vbkqm' , f 'namespace_2={local_path}' , f 'namespace_3={local_path}' , ] user_variables = read_jsons ( paths ) assert 'namespace_1' in user_variables assert 'namespace_2' in user_variables assert 'namespace_3' in user_variables assert user_variables [ 'generated_by' ] == 'Manubot' assert 'violet' in user_variables [ 'namespace_1' ][ 'rainbow' ] assert 'yellow' in user_variables [ 'namespace_2' ][ 'rainbow' ] assert 'orange' in user_variables [ 'namespace_3' ][ 'rainbow' ] test_read_jsons_empty def test_read_jsons_empty ( ) View Source def test_read_jsons_empty (): paths = [] user_variables = read_jsons ( paths ) assert isinstance ( user_variables , dict ) and not user_variables","title":"Test Util"},{"location":"reference/manubot/process/tests/test_util/#module-manubotprocessteststest_util","text":"View Source import pathlib import pytest from manubot.process.util import ( add_author_affiliations , read_jsons , ) directory = pathlib . Path ( __file__ ) . parent . resolve () def test_read_jsons_empty (): paths = [] user_variables = read_jsons ( paths ) assert isinstance ( user_variables , dict ) and not user_variables def test_read_jsons (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = 'manuscripts/variables/content/template-variables.json' local_path = directory . joinpath ( local_path ) paths = [ 'https://git.io/vbkqm' , 'https://git.io/vbkqm' , 'namespace_1=https://git.io/vbkqm' , 'namespace_2=https://git.io/vbkqm' , f 'namespace_2={local_path}' , f 'namespace_3={local_path}' , ] user_variables = read_jsons ( paths ) assert 'namespace_1' in user_variables assert 'namespace_2' in user_variables assert 'namespace_3' in user_variables assert user_variables [ 'generated_by' ] == 'Manubot' assert 'violet' in user_variables [ 'namespace_1' ][ 'rainbow' ] assert 'yellow' in user_variables [ 'namespace_2' ][ 'rainbow' ] assert 'orange' in user_variables [ 'namespace_3' ][ 'rainbow' ] def test_add_author_affiliations_empty (): variables = {} variables [ 'authors' ] = [ { 'name' : 'Jane Roe' }, { 'name' : 'John Doe' }, ] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert 'affiliations' not in variables for author in variables [ 'authors' ]: assert 'affiliation_numbers' not in author def test_add_author_affiliations (): variables = {} variables [ 'authors' ] = [ # Deprecated affiliations format (as a string that's `; ` separated) { 'name' : 'Jane Roe' , 'affiliations' : 'Department of Doe, University of Roe; Peppertea University' }, # Prefered affiliations format as a list { 'name' : 'John Doe' , 'affiliations' : [ 'Unique University' , 'Peppertea University' ]}, ] with pytest . warns ( DeprecationWarning ): returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert variables [ 'affiliations' ] == [ { 'affiliation' : 'Department of Doe, University of Roe' , 'affiliation_number' : 1 }, { 'affiliation' : 'Peppertea University' , 'affiliation_number' : 2 }, { 'affiliation' : 'Unique University' , 'affiliation_number' : 3 }, ] authors = variables [ 'authors' ] assert authors [ 0 ][ 'affiliations' ] == [ 'Department of Doe, University of Roe' , 'Peppertea University' , ] assert authors [ 0 ][ 'affiliation_numbers' ] == [ 1 , 2 ] assert authors [ 1 ][ 'affiliation_numbers' ] == [ 2 , 3 ]","title":"Module manubot.process.tests.test_util"},{"location":"reference/manubot/process/tests/test_util/#variables","text":"directory","title":"Variables"},{"location":"reference/manubot/process/tests/test_util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/process/tests/test_util/#test_add_author_affiliations","text":"def test_add_author_affiliations ( ) View Source def test_add_author_affiliations (): variables = {} variables [ 'authors' ] = [ # Deprecated affiliations format ( as a string that 's `; ` separated) {' name ': ' Jane Roe ', ' affiliations ': ' Department of Doe , University of Roe ; Peppertea University '}, # Prefered affiliations format as a list {' name ': ' John Doe ', ' affiliations ': [' Unique University ', ' Peppertea University ']}, ] with pytest.warns(DeprecationWarning): returned_variables = add_author_affiliations(variables) assert variables is returned_variables assert variables[' affiliations '] == [ {' affiliation ': ' Department of Doe , University of Roe ', ' affiliation_number ': 1}, {' affiliation ': ' Peppertea University ', ' affiliation_number ': 2}, {' affiliation ': ' Unique University ', ' affiliation_number ': 3}, ] authors = variables[' authors '] assert authors[0][' affiliations '] == [ ' Department of Doe , University of Roe ', ' Peppertea University ', ] assert authors[0][' affiliation_numbers '] == [1, 2] assert authors[1][' affiliation_numbers ' ] == [ 2 , 3 ]","title":"test_add_author_affiliations"},{"location":"reference/manubot/process/tests/test_util/#test_add_author_affiliations_empty","text":"def test_add_author_affiliations_empty ( ) View Source def test_add_author_affiliations_empty () : variables = {} variables [ ' authors ' ] = [ { ' name ' : ' Jane Roe ' }, { ' name ' : ' John Doe ' }, ] returned_variables = add_author_affiliations ( variables ) assert variables is returned_variables assert ' affiliations ' not in variables for author in variables [ ' authors ' ]: assert ' affiliation_numbers ' not in author","title":"test_add_author_affiliations_empty"},{"location":"reference/manubot/process/tests/test_util/#test_read_jsons","text":"def test_read_jsons ( ) Test reading multiple JSON files, from both local paths and URLs. View Source def test_read_jsons (): \"\"\" Test reading multiple JSON files, from both local paths and URLs. \"\"\" local_path = 'manuscripts/variables/content/template-variables.json' local_path = directory . joinpath ( local_path ) paths = [ 'https://git.io/vbkqm' , 'https://git.io/vbkqm' , 'namespace_1=https://git.io/vbkqm' , 'namespace_2=https://git.io/vbkqm' , f 'namespace_2={local_path}' , f 'namespace_3={local_path}' , ] user_variables = read_jsons ( paths ) assert 'namespace_1' in user_variables assert 'namespace_2' in user_variables assert 'namespace_3' in user_variables assert user_variables [ 'generated_by' ] == 'Manubot' assert 'violet' in user_variables [ 'namespace_1' ][ 'rainbow' ] assert 'yellow' in user_variables [ 'namespace_2' ][ 'rainbow' ] assert 'orange' in user_variables [ 'namespace_3' ][ 'rainbow' ]","title":"test_read_jsons"},{"location":"reference/manubot/process/tests/test_util/#test_read_jsons_empty","text":"def test_read_jsons_empty ( ) View Source def test_read_jsons_empty (): paths = [] user_variables = read_jsons ( paths ) assert isinstance ( user_variables , dict ) and not user_variables","title":"test_read_jsons_empty"},{"location":"reference/manubot/tests/","text":"Module manubot.tests Sub-modules manubot.tests.test_command manubot.tests.test_imports manubot.tests.test_readme manubot.tests.test_util","title":"Index"},{"location":"reference/manubot/tests/#module-manubottests","text":"","title":"Module manubot.tests"},{"location":"reference/manubot/tests/#sub-modules","text":"manubot.tests.test_command manubot.tests.test_imports manubot.tests.test_readme manubot.tests.test_util","title":"Sub-modules"},{"location":"reference/manubot/tests/test_command/","text":"Module manubot.tests.test_command View Source import subprocess import manubot def test_version (): stdout = subprocess . check_output ( [ 'manubot' , '--version' ], universal_newlines = True , ) version_str = f 'v{manubot.__version__}' assert version_str == stdout . rstrip () def test_missing_subcommand (): process = subprocess . run ( [ 'manubot' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'error: the following arguments are required: subcommand' in process . stderr Functions test_missing_subcommand def test_missing_subcommand ( ) View Source def test_missing_subcommand (): process = subprocess . run ( [ 'manubot' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'error: the following arguments are required: subcommand' in process . stderr test_version def test_version ( ) View Source def test_version (): stdout = subprocess . check_output ( [ 'manubot' , '--version' ], universal_newlines = True , ) version_str = f 'v{manubot.__version__}' assert version_str == stdout . rstrip ()","title":"Test Command"},{"location":"reference/manubot/tests/test_command/#module-manubotteststest_command","text":"View Source import subprocess import manubot def test_version (): stdout = subprocess . check_output ( [ 'manubot' , '--version' ], universal_newlines = True , ) version_str = f 'v{manubot.__version__}' assert version_str == stdout . rstrip () def test_missing_subcommand (): process = subprocess . run ( [ 'manubot' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'error: the following arguments are required: subcommand' in process . stderr","title":"Module manubot.tests.test_command"},{"location":"reference/manubot/tests/test_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_command/#test_missing_subcommand","text":"def test_missing_subcommand ( ) View Source def test_missing_subcommand (): process = subprocess . run ( [ 'manubot' ], stdout = subprocess . PIPE , stderr = subprocess . PIPE , universal_newlines = True , ) print ( process . stderr ) assert process . returncode == 2 assert 'error: the following arguments are required: subcommand' in process . stderr","title":"test_missing_subcommand"},{"location":"reference/manubot/tests/test_command/#test_version","text":"def test_version ( ) View Source def test_version (): stdout = subprocess . check_output ( [ 'manubot' , '--version' ], universal_newlines = True , ) version_str = f 'v{manubot.__version__}' assert version_str == stdout . rstrip ()","title":"test_version"},{"location":"reference/manubot/tests/test_imports/","text":"Module manubot.tests.test_imports View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript def assert_instance_type (): from manubot.cite.citekey import citeproc_retrievers assert isinstance ( citeproc_retrievers , dict ) Functions assert_instance_type def assert_instance_type ( ) View Source def assert_instance_type (): from manubot.cite.citekey import citeproc_retrievers assert isinstance ( citeproc_retrievers , dict ) test_imports def test_imports ( ) View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript","title":"Test Imports"},{"location":"reference/manubot/tests/test_imports/#module-manubotteststest_imports","text":"View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript def assert_instance_type (): from manubot.cite.citekey import citeproc_retrievers assert isinstance ( citeproc_retrievers , dict )","title":"Module manubot.tests.test_imports"},{"location":"reference/manubot/tests/test_imports/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_imports/#assert_instance_type","text":"def assert_instance_type ( ) View Source def assert_instance_type (): from manubot.cite.citekey import citeproc_retrievers assert isinstance ( citeproc_retrievers , dict )","title":"assert_instance_type"},{"location":"reference/manubot/tests/test_imports/#test_imports","text":"def test_imports ( ) View Source def test_imports (): import manubot.cite import manubot.cite.arxiv import manubot.cite.doi import manubot.cite.pubmed import manubot.cite.url import manubot.process.manuscript","title":"test_imports"},{"location":"reference/manubot/tests/test_readme/","text":"Module manubot.tests.test_readme View Source import pathlib import re import shlex import subprocess import pytest readme_path = pathlib . Path ( __file__ ) . parent . parent . parent / 'README.md' readme = readme_path . read_text ( encoding = 'utf-8-sig' ) template = r ''' <!-- test codeblock contains output of `{command}` --> ``` {output}``` ''' pattern = template . format ( command = r \"(?P<command>.+?)\" , output = r \"(?P<output>.+?)\" , ) pattern = re . compile ( pattern , re . DOTALL ) matches = list ( pattern . finditer ( readme )) @pytest.mark.parametrize ( argnames = [ 'command' , 'expected' ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( 'command' ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected def _get_output_from ( command ): return subprocess . check_output ( shlex . split ( command ), universal_newlines = True ) def _match_to_repl ( match ): template_dict = match . groupdict () template_dict [ 'output' ] = _get_output_from ( template_dict [ 'command' ]) return template . format ( ** template_dict ) if __name__ == '__main__' : \"\"\" Run `python manubot/tests/test_readme.py` to populate README codeblocks with output from the specified commands. \"\"\" repl_readme = pattern . sub ( repl = _match_to_repl , string = readme ) readme_path . write_text ( repl_readme , encoding = 'utf-8' ) Variables matches pattern readme readme_path template Functions test_readme_codeblock_contains_output_from def test_readme_codeblock_contains_output_from ( command , expected ) If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: { expected } View Source @ pytest . mark . parametrize ( argnames = [ ' command ' , ' expected ' ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( ' command ' ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ) : \"\"\" If this test fails , ensure that codeblocks in README . md have the correct output . To enable this check for output in a codeblock , use the following construct : <!-- test codeblock contains output of `{ command }` --> ``` { expected } ``` \"\"\" output = _get_output_from ( command ) assert output == expected","title":"Test Readme"},{"location":"reference/manubot/tests/test_readme/#module-manubotteststest_readme","text":"View Source import pathlib import re import shlex import subprocess import pytest readme_path = pathlib . Path ( __file__ ) . parent . parent . parent / 'README.md' readme = readme_path . read_text ( encoding = 'utf-8-sig' ) template = r ''' <!-- test codeblock contains output of `{command}` --> ``` {output}``` ''' pattern = template . format ( command = r \"(?P<command>.+?)\" , output = r \"(?P<output>.+?)\" , ) pattern = re . compile ( pattern , re . DOTALL ) matches = list ( pattern . finditer ( readme )) @pytest.mark.parametrize ( argnames = [ 'command' , 'expected' ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( 'command' ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ): \"\"\" If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: <!-- test codeblock contains output of `{command}` --> ``` {expected} ``` \"\"\" output = _get_output_from ( command ) assert output == expected def _get_output_from ( command ): return subprocess . check_output ( shlex . split ( command ), universal_newlines = True ) def _match_to_repl ( match ): template_dict = match . groupdict () template_dict [ 'output' ] = _get_output_from ( template_dict [ 'command' ]) return template . format ( ** template_dict ) if __name__ == '__main__' : \"\"\" Run `python manubot/tests/test_readme.py` to populate README codeblocks with output from the specified commands. \"\"\" repl_readme = pattern . sub ( repl = _match_to_repl , string = readme ) readme_path . write_text ( repl_readme , encoding = 'utf-8' )","title":"Module manubot.tests.test_readme"},{"location":"reference/manubot/tests/test_readme/#variables","text":"matches pattern readme readme_path template","title":"Variables"},{"location":"reference/manubot/tests/test_readme/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_readme/#test_readme_codeblock_contains_output_from","text":"def test_readme_codeblock_contains_output_from ( command , expected ) If this test fails, ensure that codeblocks in README.md have the correct output. To enable this check for output in a codeblock, use the following construct: { expected } View Source @ pytest . mark . parametrize ( argnames = [ ' command ' , ' expected ' ], argvalues = [ match . groups () for match in matches ], ids = [ match . group ( ' command ' ) for match in matches ], ) def test_readme_codeblock_contains_output_from ( command , expected ) : \"\"\" If this test fails , ensure that codeblocks in README . md have the correct output . To enable this check for output in a codeblock , use the following construct : <!-- test codeblock contains output of `{ command }` --> ``` { expected } ``` \"\"\" output = _get_output_from ( command ) assert output == expected","title":"test_readme_codeblock_contains_output_from"},{"location":"reference/manubot/tests/test_util/","text":"Module manubot.tests.test_util View Source import manubot.util def test_shlex_join (): import pathlib args = [ 'command' , 'positional arg' , 'path_arg' , pathlib . Path ( 'path' ), ] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\" Functions test_shlex_join def test_shlex_join ( ) View Source def test_shlex_join (): import pathlib args = [ 'command' , 'positional arg' , 'path_arg' , pathlib . Path ( 'path' ), ] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"Test Util"},{"location":"reference/manubot/tests/test_util/#module-manubotteststest_util","text":"View Source import manubot.util def test_shlex_join (): import pathlib args = [ 'command' , 'positional arg' , 'path_arg' , pathlib . Path ( 'path' ), ] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"Module manubot.tests.test_util"},{"location":"reference/manubot/tests/test_util/#functions","text":"","title":"Functions"},{"location":"reference/manubot/tests/test_util/#test_shlex_join","text":"def test_shlex_join ( ) View Source def test_shlex_join (): import pathlib args = [ 'command' , 'positional arg' , 'path_arg' , pathlib . Path ( 'path' ), ] output = manubot . util . shlex_join ( args ) assert output == \"command 'positional arg' path_arg path\"","title":"test_shlex_join"},{"location":"reference/manubot/webpage/","text":"Module manubot.webpage Sub-modules manubot.webpage.webpage_command","title":"Index"},{"location":"reference/manubot/webpage/#module-manubotwebpage","text":"","title":"Module manubot.webpage"},{"location":"reference/manubot/webpage/#sub-modules","text":"manubot.webpage.webpage_command","title":"Sub-modules"},{"location":"reference/manubot/webpage/webpage_command/","text":"Module manubot.webpage.webpage_command View Source import logging import pathlib import shutil import subprocess from manubot.util import shlex_join def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f 'Running `manubot webpage` with the following args: \\n {args}' ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ 'timestamp' ] = False # Directory where Manubot outputs reside args_dict [ 'output_directory' ] = pathlib . Path ( 'output' ) # Set webpage directory args_dict [ 'webpage_directory' ] = pathlib . Path ( 'webpage' ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ 'versions_directory' ] = args . webpage_directory . joinpath ( 'v' ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot.process.ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ 'version' ] = ci_params . get ( 'commit' , 'local' ) else : args_dict [ 'version' ] = 'local' # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f '{version_directory} exists: replacing it with an empty directory' ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ 'version_directory' ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( 'latest' ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ 'latest_directory' ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( 'freeze' ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ 'freeze_directory' ] = freeze_directory return args def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ 'git' , f '--work-tree={args.webpage_directory}' , 'checkout' , args . checkout , '--' , 'v' , ] logging . info ( f \"Attempting checkout with the following command: \\n {shlex_join(process.args)}\" ) process = subprocess . run ( command , stderr = subprocess . PIPE ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v/* files that don't actually exist subprocess . run ([ 'git' , 'add' , 'v' ]) else : stderr = process . stderr . decode () message = f 'Checkout returned a nonzero exit status. See stderr: \\n {stderr.rstrip()}' if 'pathspec' in stderr : message += ( ' \\n Manubot note: if there are no preexisting webpage versions (like for a newly created manuscript), ' 'the pathspec error above is expected and can be safely ignored.' ) # see https://github.com/manubot/rootstock/issues/183 logging . warning ( message ) def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content/images to webpage/v/commit/images images_src = pathlib . Path ( 'content/images' ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( 'images' ), ) # Copy output files to to webpage/v/version/ renamer = { 'manuscript.html' : 'index.html' , 'manuscript.pdf' : 'manuscript.pdf' , } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path , ) if args . timestamp : ots_stamp ( dst_path ) # Create v/freeze to redirect to v/commit path = pathlib . Path ( __file__ ) . with_name ( 'redirect-template.html' ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f '../{args.version}/' ) args . freeze_directory . joinpath ( 'index.html' ) . write_text ( redirect_html ) def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir ()} versions -= { 'freeze' , 'latest' } versions = sorted ( versions ) return versions def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( '**/*.ots' )) ots_paths . sort () for ots_path in ots_paths : process_args = [ 'ots' ] if args . no_ots_cache : process_args . append ( '--no-cache' ) else : process_args . extend ([ '--cache' , str ( args . ots_cache )]) process_args . extend ([ 'upgrade' , str ( ots_path ), ]) process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) message = f \">>> {shlex_join(process.args)} \\n {process.stderr}\" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message}\" ) elif not process . stderr . strip () == 'Success! Timestamp complete' : logging . info ( message ) backup_path = ots_path . with_suffix ( '.ots.bak' ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path ) def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ 'ots' , 'stamp' , str ( path )] process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \">>> {shlex_join(process.args)} \\n \" f \"{process.stderr}\" ) Functions checkout_existing_versions def checkout_existing_versions ( args ) Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v View Source def checkout_existing_versions ( args ) : \"\"\" Must populate webpage / v from the gh - pages branch to get history References : http : // clubmate . fi / git - checkout - file - or - directories - from - another - branch / https : // stackoverflow . com / a / 2668947 / 4651668 https : // stackoverflow . com / a / 16493707 / 4651668 Command modeled after : git -- work - tree = webpage checkout upstream / gh - pages -- v \"\"\" if not args . checkout : return command = [ ' git ' , f ' --work-tree={args.webpage_directory} ' , ' checkout ' , args . checkout , ' -- ' , ' v ' , ] logging . info ( f \" Attempting checkout with the following command: \\n {shlex_join(process.args)} \" ) process = subprocess . run ( command , stderr = subprocess . PIPE ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v /* files that don't actually exist subprocess.run(['git', 'add', 'v']) else: stderr = process.stderr.decode() message = f'Checkout returned a nonzero exit status. See stderr:\\n{stderr.rstrip()}' if 'pathspec' in stderr: message += ( '\\nManubot note: if there are no preexisting webpage versions (like for a newly created manuscript), ' 'the pathspec error above is expected and can be safely ignored.' ) # see https://github.com/manubot/rootstock/issues/183 logging.warning(message) cli_webpage def cli_webpage ( args ) Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. View Source def cli_webpage ( args ) : \"\"\" Execute manubot webpage commands . args should be an argparse . Namespace object created by parser . parse_args . \"\"\" configure_args ( args ) logging . debug ( f ' Running `manubot webpage` with the following args: \\n {args} ' ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) configure_args def configure_args ( args ) Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. View Source def configure_args ( args ) : \"\"\" Perform additional processing of arguments that is not handled by argparse . Derive additional variables and add them to args . For example , add directories to args and create them if neccessary . Note that versions_directory is the parent of version_directory . \"\"\" args_dict = vars ( args ) # If -- timestamp specified , check that opentimestamps - client is installed if args . timestamp : ots_executable_path = shutil . which ( \" ots \" ) if not ots_executable_path : logging . error ( \" manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \" Setting --timestamp=False. \" \" Fix this by installing https://pypi.org/project/opentimestamps-client/ \" ) args_dict [ ' timestamp ' ] = False # Directory where Manubot outputs reside args_dict [ ' output_directory ' ] = pathlib . Path ( ' output ' ) # Set webpage directory args_dict [ ' webpage_directory ' ] = pathlib . Path ( ' webpage ' ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage / v directory ( if it doesn ' t already exist) args_dict [ ' versions_directory ' ] = args . webpage_directory . joinpath ( ' v ' ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply -- version argument defaults if args . version is None : from manubot . process . ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ ' version ' ] = ci_params . get ( ' commit ' , ' local ' ) else : args_dict [ ' version ' ] = ' local ' # Create empty webpage / v / version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir () : logging . warning ( f ' {version_directory} exists: replacing it with an empty directory ' ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ ' version_directory ' ] = version_directory # Symlink webpage / v / latest to point to webpage / v / commit latest_directory = args . versions_directory . joinpath ( ' latest ' ) if latest_directory . is_symlink () or latest_directory . is_file () : latest_directory . unlink () elif latest_directory . is_dir () : shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ ' latest_directory ' ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( ' freeze ' ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ ' freeze_directory ' ] = freeze_directory return args create_version def create_version ( args ) Populate the version directory for a new version. View Source def create_version ( args ) : \"\"\" Populate the version directory for a new version . \"\"\" # Copy content / images to webpage / v / commit / images images_src = pathlib . Path ( ' content/images ' ) if images_src . exists () : shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( ' images ' ) , ) # Copy output files to to webpage / v / version / renamer = { ' manuscript.html ' : ' index.html ' , ' manuscript.pdf ' : ' manuscript.pdf ' , } for src , dst in renamer . items () : src_path = args . output_directory . joinpath ( src ) if not src_path . exists () : continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path , ) if args . timestamp : ots_stamp ( dst_path ) # Create v / freeze to redirect to v / commit path = pathlib . Path ( __file__ ) . with_name ( ' redirect-template.html ' ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f ' ../{args.version}/ ' ) args . freeze_directory . joinpath ( ' index.html ' ) . write_text ( redirect_html ) get_versions def get_versions ( args ) Extract versions from the webpage/v directory, which should each contain a manuscript. View Source def get_versions ( args ) : \"\"\" Extract versions from the webpage / v directory , which should each contain a manuscript . \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir () } versions -= { ' freeze ' , ' latest ' } versions = sorted ( versions ) return versions ots_stamp def ots_stamp ( path ) Timestamp a file using OpenTimestamps. This function calls ots stamp path . If path does not exist, this function does nothing. View Source def ots_stamp ( path ) : \"\"\" Timestamp a file using OpenTimestamps . This function calls ` ots stamp path `. If ` path ` does not exist , this function does nothing . \"\"\" process_args = [ ' ots ' , ' stamp ' , str ( path ) ] process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \" OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \" >>> {shlex_join(process.args)} \\n \" f \" {process.stderr} \" ) ots_upgrade def ots_upgrade ( args ) Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 View Source def ots_upgrade ( args ) : \"\"\" Upgrade OpenTimestamps . ots files in versioned commit directory trees . Upgrades each . ots file with a separate ots upgrade subprocess call due to https : // github . com / opentimestamps / opentimestamps - client / issues / 71 \"\"\" ots_paths = list () for version in get_versions ( args ) : ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( ' **/*.ots ' )) ots_paths . sort () for ots_path in ots_paths : process_args = [ ' ots ' ] if args . no_ots_cache : process_args . append ( ' --no-cache ' ) else : process_args . extend ( [ ' --cache ' , str ( args . ots_cache ) ] ) process_args . extend ( [ ' upgrade ' , str ( ots_path ) , ] ) process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) message = f \" >>> {shlex_join(process.args)} \\n {process.stderr} \" if process . returncode != 0 : logging . warning ( f \" OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message} \" ) elif not process . stderr . strip () == ' Success! Timestamp complete ' : logging . info ( message ) backup_path = ots_path . with_suffix ( ' .ots.bak ' ) if backup_path . exists () : if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path )","title":"Webpage Command"},{"location":"reference/manubot/webpage/webpage_command/#module-manubotwebpagewebpage_command","text":"View Source import logging import pathlib import shutil import subprocess from manubot.util import shlex_join def cli_webpage ( args ): \"\"\" Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. \"\"\" configure_args ( args ) logging . debug ( f 'Running `manubot webpage` with the following args: \\n {args}' ) if args . timestamp : ots_upgrade ( args ) create_version ( args ) def configure_args ( args ): \"\"\" Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. \"\"\" args_dict = vars ( args ) # If --timestamp specified, check that opentimestamps-client is installed if args . timestamp : ots_executable_path = shutil . which ( \"ots\" ) if not ots_executable_path : logging . error ( \"manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \"Setting --timestamp=False. \" \"Fix this by installing https://pypi.org/project/opentimestamps-client/\" ) args_dict [ 'timestamp' ] = False # Directory where Manubot outputs reside args_dict [ 'output_directory' ] = pathlib . Path ( 'output' ) # Set webpage directory args_dict [ 'webpage_directory' ] = pathlib . Path ( 'webpage' ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage/v directory (if it doesn't already exist) args_dict [ 'versions_directory' ] = args . webpage_directory . joinpath ( 'v' ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply --version argument defaults if args . version is None : from manubot.process.ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ 'version' ] = ci_params . get ( 'commit' , 'local' ) else : args_dict [ 'version' ] = 'local' # Create empty webpage/v/version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir (): logging . warning ( f '{version_directory} exists: replacing it with an empty directory' ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ 'version_directory' ] = version_directory # Symlink webpage/v/latest to point to webpage/v/commit latest_directory = args . versions_directory . joinpath ( 'latest' ) if latest_directory . is_symlink () or latest_directory . is_file (): latest_directory . unlink () elif latest_directory . is_dir (): shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ 'latest_directory' ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( 'freeze' ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ 'freeze_directory' ] = freeze_directory return args def checkout_existing_versions ( args ): \"\"\" Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v \"\"\" if not args . checkout : return command = [ 'git' , f '--work-tree={args.webpage_directory}' , 'checkout' , args . checkout , '--' , 'v' , ] logging . info ( f \"Attempting checkout with the following command: \\n {shlex_join(process.args)}\" ) process = subprocess . run ( command , stderr = subprocess . PIPE ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v/* files that don't actually exist subprocess . run ([ 'git' , 'add' , 'v' ]) else : stderr = process . stderr . decode () message = f 'Checkout returned a nonzero exit status. See stderr: \\n {stderr.rstrip()}' if 'pathspec' in stderr : message += ( ' \\n Manubot note: if there are no preexisting webpage versions (like for a newly created manuscript), ' 'the pathspec error above is expected and can be safely ignored.' ) # see https://github.com/manubot/rootstock/issues/183 logging . warning ( message ) def create_version ( args ): \"\"\" Populate the version directory for a new version. \"\"\" # Copy content/images to webpage/v/commit/images images_src = pathlib . Path ( 'content/images' ) if images_src . exists (): shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( 'images' ), ) # Copy output files to to webpage/v/version/ renamer = { 'manuscript.html' : 'index.html' , 'manuscript.pdf' : 'manuscript.pdf' , } for src , dst in renamer . items (): src_path = args . output_directory . joinpath ( src ) if not src_path . exists (): continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path , ) if args . timestamp : ots_stamp ( dst_path ) # Create v/freeze to redirect to v/commit path = pathlib . Path ( __file__ ) . with_name ( 'redirect-template.html' ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f '../{args.version}/' ) args . freeze_directory . joinpath ( 'index.html' ) . write_text ( redirect_html ) def get_versions ( args ): \"\"\" Extract versions from the webpage/v directory, which should each contain a manuscript. \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir ()} versions -= { 'freeze' , 'latest' } versions = sorted ( versions ) return versions def ots_upgrade ( args ): \"\"\" Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 \"\"\" ots_paths = list () for version in get_versions ( args ): ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( '**/*.ots' )) ots_paths . sort () for ots_path in ots_paths : process_args = [ 'ots' ] if args . no_ots_cache : process_args . append ( '--no-cache' ) else : process_args . extend ([ '--cache' , str ( args . ots_cache )]) process_args . extend ([ 'upgrade' , str ( ots_path ), ]) process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) message = f \">>> {shlex_join(process.args)} \\n {process.stderr}\" if process . returncode != 0 : logging . warning ( f \"OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message}\" ) elif not process . stderr . strip () == 'Success! Timestamp complete' : logging . info ( message ) backup_path = ots_path . with_suffix ( '.ots.bak' ) if backup_path . exists (): if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path ) def ots_stamp ( path ): \"\"\" Timestamp a file using OpenTimestamps. This function calls `ots stamp path`. If `path` does not exist, this function does nothing. \"\"\" process_args = [ 'ots' , 'stamp' , str ( path )] process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \"OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \">>> {shlex_join(process.args)} \\n \" f \"{process.stderr}\" )","title":"Module manubot.webpage.webpage_command"},{"location":"reference/manubot/webpage/webpage_command/#functions","text":"","title":"Functions"},{"location":"reference/manubot/webpage/webpage_command/#checkout_existing_versions","text":"def checkout_existing_versions ( args ) Must populate webpage/v from the gh-pages branch to get history References: http://clubmate.fi/git-checkout-file-or-directories-from-another-branch/ https://stackoverflow.com/a/2668947/4651668 https://stackoverflow.com/a/16493707/4651668 Command modeled after: git --work-tree=webpage checkout upstream/gh-pages -- v View Source def checkout_existing_versions ( args ) : \"\"\" Must populate webpage / v from the gh - pages branch to get history References : http : // clubmate . fi / git - checkout - file - or - directories - from - another - branch / https : // stackoverflow . com / a / 2668947 / 4651668 https : // stackoverflow . com / a / 16493707 / 4651668 Command modeled after : git -- work - tree = webpage checkout upstream / gh - pages -- v \"\"\" if not args . checkout : return command = [ ' git ' , f ' --work-tree={args.webpage_directory} ' , ' checkout ' , args . checkout , ' -- ' , ' v ' , ] logging . info ( f \" Attempting checkout with the following command: \\n {shlex_join(process.args)} \" ) process = subprocess . run ( command , stderr = subprocess . PIPE ) if process . returncode == 0 : # Addresses an odd behavior where git checkout stages v /* files that don't actually exist subprocess.run(['git', 'add', 'v']) else: stderr = process.stderr.decode() message = f'Checkout returned a nonzero exit status. See stderr:\\n{stderr.rstrip()}' if 'pathspec' in stderr: message += ( '\\nManubot note: if there are no preexisting webpage versions (like for a newly created manuscript), ' 'the pathspec error above is expected and can be safely ignored.' ) # see https://github.com/manubot/rootstock/issues/183 logging.warning(message)","title":"checkout_existing_versions"},{"location":"reference/manubot/webpage/webpage_command/#cli_webpage","text":"def cli_webpage ( args ) Execute manubot webpage commands. args should be an argparse.Namespace object created by parser.parse_args. View Source def cli_webpage ( args ) : \"\"\" Execute manubot webpage commands . args should be an argparse . Namespace object created by parser . parse_args . \"\"\" configure_args ( args ) logging . debug ( f ' Running `manubot webpage` with the following args: \\n {args} ' ) if args . timestamp : ots_upgrade ( args ) create_version ( args )","title":"cli_webpage"},{"location":"reference/manubot/webpage/webpage_command/#configure_args","text":"def configure_args ( args ) Perform additional processing of arguments that is not handled by argparse. Derive additional variables and add them to args. For example, add directories to args and create them if neccessary. Note that versions_directory is the parent of version_directory. View Source def configure_args ( args ) : \"\"\" Perform additional processing of arguments that is not handled by argparse . Derive additional variables and add them to args . For example , add directories to args and create them if neccessary . Note that versions_directory is the parent of version_directory . \"\"\" args_dict = vars ( args ) # If -- timestamp specified , check that opentimestamps - client is installed if args . timestamp : ots_executable_path = shutil . which ( \" ots \" ) if not ots_executable_path : logging . error ( \" manubot webpage --timestamp was specified but opentimestamps-client not found on system. \" \" Setting --timestamp=False. \" \" Fix this by installing https://pypi.org/project/opentimestamps-client/ \" ) args_dict [ ' timestamp ' ] = False # Directory where Manubot outputs reside args_dict [ ' output_directory ' ] = pathlib . Path ( ' output ' ) # Set webpage directory args_dict [ ' webpage_directory ' ] = pathlib . Path ( ' webpage ' ) args . webpage_directory . mkdir ( exist_ok = True ) # Create webpage / v directory ( if it doesn ' t already exist) args_dict [ ' versions_directory ' ] = args . webpage_directory . joinpath ( ' v ' ) args . versions_directory . mkdir ( exist_ok = True ) # Checkout existing version directories checkout_existing_versions ( args ) # Apply -- version argument defaults if args . version is None : from manubot . process . ci import get_continuous_integration_parameters ci_params = get_continuous_integration_parameters () if ci_params : args_dict [ ' version ' ] = ci_params . get ( ' commit ' , ' local ' ) else : args_dict [ ' version ' ] = ' local ' # Create empty webpage / v / version directory version_directory = args . versions_directory . joinpath ( args . version ) if version_directory . is_dir () : logging . warning ( f ' {version_directory} exists: replacing it with an empty directory ' ) shutil . rmtree ( version_directory ) version_directory . mkdir () args_dict [ ' version_directory ' ] = version_directory # Symlink webpage / v / latest to point to webpage / v / commit latest_directory = args . versions_directory . joinpath ( ' latest ' ) if latest_directory . is_symlink () or latest_directory . is_file () : latest_directory . unlink () elif latest_directory . is_dir () : shutil . rmtree ( latest_directory ) latest_directory . symlink_to ( args . version , target_is_directory = True ) args_dict [ ' latest_directory ' ] = latest_directory # Create freeze directory freeze_directory = args . versions_directory . joinpath ( ' freeze ' ) freeze_directory . mkdir ( exist_ok = True ) args_dict [ ' freeze_directory ' ] = freeze_directory return args","title":"configure_args"},{"location":"reference/manubot/webpage/webpage_command/#create_version","text":"def create_version ( args ) Populate the version directory for a new version. View Source def create_version ( args ) : \"\"\" Populate the version directory for a new version . \"\"\" # Copy content / images to webpage / v / commit / images images_src = pathlib . Path ( ' content/images ' ) if images_src . exists () : shutil . copytree ( src = images_src , dst = args . version_directory . joinpath ( ' images ' ) , ) # Copy output files to to webpage / v / version / renamer = { ' manuscript.html ' : ' index.html ' , ' manuscript.pdf ' : ' manuscript.pdf ' , } for src , dst in renamer . items () : src_path = args . output_directory . joinpath ( src ) if not src_path . exists () : continue dst_path = args . version_directory . joinpath ( dst ) shutil . copy2 ( src = src_path , dst = dst_path , ) if args . timestamp : ots_stamp ( dst_path ) # Create v / freeze to redirect to v / commit path = pathlib . Path ( __file__ ) . with_name ( ' redirect-template.html ' ) redirect_html = path . read_text () redirect_html = redirect_html . format ( url = f ' ../{args.version}/ ' ) args . freeze_directory . joinpath ( ' index.html ' ) . write_text ( redirect_html )","title":"create_version"},{"location":"reference/manubot/webpage/webpage_command/#get_versions","text":"def get_versions ( args ) Extract versions from the webpage/v directory, which should each contain a manuscript. View Source def get_versions ( args ) : \"\"\" Extract versions from the webpage / v directory , which should each contain a manuscript . \"\"\" versions = { x . name for x in args . versions_directory . iterdir () if x . is_dir () } versions -= { ' freeze ' , ' latest ' } versions = sorted ( versions ) return versions","title":"get_versions"},{"location":"reference/manubot/webpage/webpage_command/#ots_stamp","text":"def ots_stamp ( path ) Timestamp a file using OpenTimestamps. This function calls ots stamp path . If path does not exist, this function does nothing. View Source def ots_stamp ( path ) : \"\"\" Timestamp a file using OpenTimestamps . This function calls ` ots stamp path `. If ` path ` does not exist , this function does nothing . \"\"\" process_args = [ ' ots ' , ' stamp ' , str ( path ) ] process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) if process . returncode != 0 : logging . warning ( f \" OpenTimestamp command returned nonzero code ({process.returncode}). \\n \" f \" >>> {shlex_join(process.args)} \\n \" f \" {process.stderr} \" )","title":"ots_stamp"},{"location":"reference/manubot/webpage/webpage_command/#ots_upgrade","text":"def ots_upgrade ( args ) Upgrade OpenTimestamps .ots files in versioned commit directory trees. Upgrades each .ots file with a separate ots upgrade subprocess call due to https://github.com/opentimestamps/opentimestamps-client/issues/71 View Source def ots_upgrade ( args ) : \"\"\" Upgrade OpenTimestamps . ots files in versioned commit directory trees . Upgrades each . ots file with a separate ots upgrade subprocess call due to https : // github . com / opentimestamps / opentimestamps - client / issues / 71 \"\"\" ots_paths = list () for version in get_versions ( args ) : ots_paths . extend ( args . versions_directory . joinpath ( version ) . glob ( ' **/*.ots ' )) ots_paths . sort () for ots_path in ots_paths : process_args = [ ' ots ' ] if args . no_ots_cache : process_args . append ( ' --no-cache ' ) else : process_args . extend ( [ ' --cache ' , str ( args . ots_cache ) ] ) process_args . extend ( [ ' upgrade ' , str ( ots_path ) , ] ) process = subprocess . run ( process_args , stderr = subprocess . PIPE , universal_newlines = True , ) message = f \" >>> {shlex_join(process.args)} \\n {process.stderr} \" if process . returncode != 0 : logging . warning ( f \" OpenTimestamp upgrade failed with exit code {process.returncode}. \\n {message} \" ) elif not process . stderr . strip () == ' Success! Timestamp complete ' : logging . info ( message ) backup_path = ots_path . with_suffix ( ' .ots.bak ' ) if backup_path . exists () : if process . returncode == 0 : backup_path . unlink () else : # Restore original timestamp if failure backup_path . rename ( ots_path )","title":"ots_upgrade"}]}